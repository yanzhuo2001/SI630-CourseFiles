{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44690ff2",
   "metadata": {},
   "source": [
    "# Homework 3, Part 2\n",
    "\n",
    "For this part of the homework, we'll work with the data we labeled in Part 1 on estimating human values to fine-tune a deep learning classifier based on a pre-trained language model. Pre-trained language models like BERT or GPT4 have all been trained to do the task of language modeling in some form and their parameters \"know\" about language through doing the task, much like how in Homework 2 the word vectors end up \"knowing\" about word meaning through the task of context word prediction. By starting from these parameters, we can often build a much more effective model for some NLP tasks, such as a classifier. Often, we speak of _fine-tuning_ the parameters for a specific task to distinguish from the _pre-training_ step. Both are training the model, but the former is what we'll do as practitioners to adapt the model to get it to do what we want.\n",
    "\n",
    "To fine-tune our classifier, we'll be using the Huggingface [`transformers`](https://huggingface.co/) library. This library provides many useful functions for working with pre-trained language models and its [model repository](https://huggingface.co/models) contains many pre-trained models that companies and individuals have shared for doing different tasks. With their code, we can often quickly get a basic model up and running.\n",
    "\n",
    "In particular, we'll be going through how to use the Huggingface [`Trainer`](https://huggingface.co/docs/transformers/training) class, which is a powerful, high-level abstraction for how to fine-tune models for many types of different NLP tasks. We'll look at two types of tasks:\n",
    "- Training a regular classifier\n",
    "- Training a _multilabel_ classifier (i.e., one that predicts multiple different labels at the same time)\n",
    "\n",
    "The first type is similar to what we saw in Homeworks 1 and 2. The second type is what we'll need to use with our data annotation where we have zero or more values being present in any given instance ---we want to predict which of them are present at once, not just one at a time.\n",
    "\n",
    "We'll start by going through the basics of how to load data and get it prepared for use with `Trainer` and then extend it to the multilabel case.\n",
    "\n",
    "## Summary of the Training Task and Learning Goals\n",
    "\n",
    "Homework 3 focuses on a subjective annotation/classification task: What values are signaled by an author in their writing? For this part of the homework we've provided example data from the recent SemEval shared task: [SemEval-2023 Task 4: ValueEval: Identification of Human Values Behind Arguments](https://aclanthology.org/2023.semeval-1.313/). This dataset and format will be similar to what we have to annotate in Part 1, so this is just to get you started.\n",
    "\n",
    "Learning goals\n",
    "- Gain experience in working with `torch` and `transformer` libraries \n",
    "- Learn how to tokenize text and create batches\n",
    "- Learn how to fine-tune pre-trained models for different tasks using the `Trainer` class\n",
    "- Learn how to evaluate models\n",
    "- Learn how to train models on a GPU \n",
    "- Learn how to use Great Lakes to submit jobs \n",
    "\n",
    "## How to do this part of the assignment\n",
    "\n",
    "Huggingface makes it easy to fine-tune models. We'll be working with one very small model in this case. To get things started, you can get most of the code working in this notebook, including doing some very small-scale training (e.g., a few training steps) which will verify that all the steps work. Once you get it working, you'll then convert this notebook to a script and run it on Great Lakes to train the whole model and save the model. You only need to use Great Lakes for training.\n",
    "\n",
    "Once you have the final model, you can run the experiments with it on your laptop in the next notebook (Part 3). \n",
    "\n",
    "## Important Notes on Training on Great Lakes\n",
    "\n",
    "This homework requires that you use a GPU for fine-tuning your model. You can use your own or you can use the GPUs on Great Lakes for free.  We have provided you with a course account for Great Lakes, `si630w24`, which will give you access to many hours of GPU time on the cluster using state-of-the-art GPUs. Your Great Lakes jobs are limited to 4 hours of training, which is more than enough to train your model for this assignment. \n",
    "\n",
    "We've put together a [guide](https://docs.google.com/document/d/1YtOkxSGUyX0siaOtKhry-dMgKDqPAycPvlE62wbrBsM/edit?usp=sharing) on how to access Great Lakes, set up your environment, and submit your jobs. We strongly encourage learning to use Great Lakes for this assignment as (1) you have free access to substantial computational resources and (2) you would want to use it for the next homework and the final project.\n",
    "\n",
    "Great Lakes uses a queueing system where users (like you) submit requests for their programs to run (often called \"jobs\"). Great Lakes supports two kinds of job requests: (1) interactive mode jobs that give you a Jupyter notebook and (2) running a script as a job. **We strongly encourage the scripts and discourage interactive jobs for the GPUs.** To get a GPU, you'll need to submit a job to the cluster, which uses [SLURM for scheduling](https://arc.umich.edu/greatlakes/slurm-user-guide/). If you attempt to queue for an interactive job, you will have no control over when it starts, so you may end up having your notebook run for 4 hours from 3am to 7am and then it ends, at which point you have to get back in the queue. If you submit a job as a script (i.e., a .py file that runs the code in this notebook), it will run for the specified amount of time and save your fine-tuned model without you having to interact with anything. \n",
    "\n",
    "SLURM and cluster scheduling are very common in some industries where there is a single cluster resource and people share it by submitting jobs to run so that no one can monopolize the system and that jobs can run in parallel. Given that Great Lakes will be useful to you in future assignments and projects, we strongly encourage you to learn how to use it effectively in this assignment.\n",
    "\n",
    "Depending on how you're working on this file, there are a few ways to directly convert the notebook to a file if you use [Jupyter or the command line](https://mljar.com/blog/convert-jupyter-notebook-python/) or [VSCode](https://stackoverflow.com/questions/64297272/best-way-to-convert-ipynb-to-py-in-vscode). Once you convert it, you'll modify the file some to change the epochs and text file as specified in the PDF. **We also strongly recommend having your script save the model at the end of every epoch.**  That way, if your script takes longer than 4 hours and gets killed, you still have the best-saved model you could get based on the amount of training you could do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7242b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:40:59.737112Z",
     "start_time": "2024-03-19T20:40:53.644197Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce8ee8",
   "metadata": {},
   "source": [
    "# Task 3.1.1: Load Dataset \n",
    "\n",
    "We'll start by loading the dataset for the [SemEval 2023 Values prediction task](https://aclanthology.org/2023.semeval-1.313/) and converting this into a `Dataset` that we can use to train our model. \n",
    "\n",
    "For the `Trainer` code to work, it expects a column called \"labels\" (unless you want to configure it more heavily). In our case, we've already given you one column to rename as \"labels\". \n",
    "\n",
    "- Start by [loading](https://huggingface.co/docs/datasets/v1.2.1/loading_datasets.html) the training, dev, and test data into a pandas `DataFrame`.\n",
    "- Add or name that column so each of the `DataFrame`s has a \"labels\" column corresponding to the value of the \"Security: personal\" columns\n",
    "- Then convert all of these into a `Dataset`.\n",
    "- Then wrap all of these in [`DatasetDict`]()\n",
    "\n",
    "We recommend reading the [Huggingface documentation](https://huggingface.co/docs/datasets/index) on how to load and interact with datasets, as you'll end up doing this a lot as a practitioner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdbde72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:01.225447Z",
     "start_time": "2024-03-19T20:41:01.167321Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_path = 'si630-w24-train.tsv'\n",
    "test_data_path = 'si630-w24-test.tsv'  # 假设你也有一个test.tsv文件\n",
    "dev_data_path = 'si630-w24-dev.tsv'    # 假设你也有一个dev.tsv文件\n",
    "\n",
    "train_df = pd.read_csv(train_data_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_data_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_data_path, sep='\\t')\n",
    "\n",
    "train_df = train_df.rename(columns={\"Security: personal\": \"labels\"})\n",
    "test_df = test_df.rename(columns={\"Security: personal\": \"labels\"})\n",
    "dev_df = dev_df.rename(columns={\"Security: personal\": \"labels\"})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'validation': dev_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833baac",
   "metadata": {},
   "source": [
    "# Task 3.2 Preparing the Data\n",
    "\n",
    "Once we have our Dataset created, we need to turn it into features and labels so we can train the model with it.\n",
    "\n",
    "## Task 3.2.1 Tokezing\n",
    "\n",
    "Pre-trained language models are each associated with a specific method for tokenizing data, much like how in Homework 1 you write the `tokenize` function that turns strings into discrete features. Huggingface has a fast [`tokenizers`]() package that we will use here through the [`AutoTokenizer`]() class.\n",
    "\n",
    "On Huggingface, each model is associated with a particular unique string. For example, the original BERT model is `bert-based-uncased`. You can use this string to look up the corresponding tokenizer with `AutoTokenizer`\n",
    "\n",
    "Be sure to set the `model_max_length` argument to indicate what's the maximum length sequence this model can handle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23324e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:03.121043Z",
     "start_time": "2024-03-19T20:41:02.784384Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: you can use this smaller model if you want to get started\n",
    "# model_name = 'microsoft/MiniLM-L12-H384-uncased'\n",
    "model_name = \"google-bert/bert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1c1df",
   "metadata": {},
   "source": [
    "## Task 3.2.2 Converting the labels \n",
    "\n",
    "Each label needs to also be associated with an ID (starting at 0). In 3.2.2, we'll do a _very simple_ pass and **use only one label column** (you'll use more columns later). Your tasks are\n",
    "\n",
    "- Create a list of the labels in the dataset.  For Task 3.2.2, *this list should contains only a single label* (for now)\n",
    "- Create a mapping from ID to label, and the reverse \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79a8972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:04.346073Z",
     "start_time": "2024-03-19T20:41:04.342980Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = [\"0\", \"1\"]\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3e340",
   "metadata": {},
   "source": [
    "## Task 3.2.3: Preprocessing the data\n",
    "\n",
    "Models are typically fine-tuned using _batches_ rather than single instances. For the attention-based classifier in Homework 2 Part 2, we used a batch size of 1 because of the key challenge in batching over sequences: texts have different lengths, but all items in a batch need to be the same length. \n",
    "\n",
    "Here, we'll use a function to _create_ batches of the same length by padding them with extra tokens typically labeled as `[PAD]`. The underlying model code knows to avoid doing computation with these tokens so they don't have any effect on the text, other than making the tensors all the same size; if you're curious, look around for details of the \"attention mask\" to see how the \"[PAD]\" token gets ignored (you'll also see this in the tokenized dataset object too!).\n",
    "\n",
    "One of the big reasons we use a `Dataset` is that it supports easy preprocessing to turn the text into IDs and do this truncation for us. We'll define the function below that says how to transform the instances and then call `map` on the dataset to get the preprocessed/tokenized data back out.\n",
    "\n",
    "**Important Note**: One key hyperparameter to deal with is the maximum length of the sequence. If you recall from our attention, the attention mechanism is $O(n^2)$ for the length $n$ of a sequence. This means long sequences get very expensive in the kinds of models we'll use here. Since all the items in a batch get padded to the length of either the longest sequence in a batch or the maximum length of the model, one long sequence can mean we use a lot more memory just to hold empty [PAD] tokens. As a result, often we can truncate very long sequences to make them fit in memory, under the assumption that the extra tokens aren't that informative. In the code below, we use the longest sequence in a batch which hopefully helps keep things smaller. However, in your projects, you may explore other truncation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892bb4da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:06.463123Z",
     "start_time": "2024-03-19T20:41:05.972194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/5393 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f24495edcec246db9cb3fac5960546cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1576 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5073e4ed176b46c9902b5fac0302d57e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1896 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7fb843184e34477afd43ebd8a093197"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"longest\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75e5f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:08.201776Z",
     "start_time": "2024-03-19T20:41:08.198388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['inst_id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5393\n    })\n    test: Dataset({\n        features: ['inst_id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1576\n    })\n    validation: Dataset({\n        features: ['inst_id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1896\n    })\n})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what all got added to the tokenized_datasets\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f422315",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:09.232232Z",
     "start_time": "2024-03-19T20:41:09.228561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'[CLS] We should ban human cloning because as it will only cause huge issues when you have a bunch of the same humans running around all acting the same. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at what we have\n",
    "example = tokenized_datasets['train'][0]\n",
    "\n",
    "# We can reverse the tokenization to see the original text too\n",
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3869687",
   "metadata": {},
   "source": [
    "### Decide on where the computation will be\n",
    "\n",
    "The deep learning library under all the huggingface code is `torch` which is a set of libraries for doing matrix math quickly. Different hardware have different capabilities for how to do math so torch lets you specify where you want to do the computation with the `device` argument. GPUs are designed to do fast matrix multiplication and so are ideal for our purposes. As you might have noticed in homework 2, `torch` can change the device though and will run just fine (but slower) on the GPU. The code snippet below shows how to choose the device.\n",
    "\n",
    "**Important Note:** When doing the computation both the data and the parameters need to be on the same device. This means if you move the pre-trained model to the GPU but the data is sitting on the CPU, the model can't see the data (or vice-versa). This is why you'll see a lot of `something.to(device)` to everything is on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58631a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:11.083580Z",
     "start_time": "2024-03-19T20:41:11.080484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cpu' device\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "device = 'cpu' \n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f83a3",
   "metadata": {},
   "source": [
    "## Task 3.3.1: Getting the model and `Trainer` setup\n",
    "\n",
    "Now it's time to bring in the model. Just like with the tokenizer, we can use huggingface's `AutoModelFor[TASK_TYPE]` to load the model for the appropriate task type. In this case, we want to classify a sequence of tokens, so we'll use the `AutoModelForSequenceClassification` class but there are many other options after \"For\" that you can check out (e.g., for your course project)\n",
    "\n",
    "In this case, we need to specify how many classes/labels there are in our data.\n",
    "Calculate that from the data above and provide it as an argument when loading the parameters for the pre-trained model we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3835e120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:13.529384Z",
     "start_time": "2024-03-19T20:41:12.683959Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d6448a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:15.353590Z",
     "start_time": "2024-03-19T20:41:15.349801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108311810 || all params: 108311810 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "## Let's see how many parameters we are going to be changing\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cff9e9",
   "metadata": {},
   "source": [
    "## Task 3.3.2 Setting up the training arguments\n",
    "\n",
    "There are a lot of options when you fine-tune a model! In homework 2, you saw a few of these when we set the learning rate and number of epochs. When using the huggingface `Trainer` class, we specify all of the training arguments at once in a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments) object. There are a _lot_ of arguments you can specify and for the most part, you do not need to set all of them. Reading that class's documentation can be overwhelming, so don't worry if you don't know what all of them mean.\n",
    "\n",
    "Here are a few useful arguments that you'll need to set:\n",
    "- `output_dir` - where to save the models. This directory can get very large if you save all the checkpoints!\n",
    "- `overwrite_output_dir` - whether to overwrite the previously saved models\n",
    "- `learning_rate` - use 2e-5,\n",
    "- `per_device_train_batch_size` - how many items per batch. You usually want this to be a power of 2 due to how computers work. Common sizes with GPUs are between 64 and 256, but it will depend a lot on how much memory the GPU has (and how big your sequences are). \n",
    "- `per_device_eval_batch_size` - same as above, but because you're not doing gradient descent on these (just eval), there's less \"stuff\" needed in memory and this can be a bit larger.\n",
    "- `do_eval` - whether do evaluate on the development/validation data periodically during training\n",
    "- `seed` - the random seed to use. Use 12345\n",
    "- `evaluation_strategy` - when to evaluate the model during training. \"epoch\" evaluates after the end of every epoch, while \"steps\" evaluates every `eval_steps`. If you have a very large dataset, you probably want to use \"steps\" so that you can get periodic updates on how the model is doing. Even though our dataset is relatively small, let's use \"steps\".\n",
    "- `eval_steps` - how many steps between an evaluation on the dev data. For this assignment, use 50 so we can see how our model trains. In real-world scenarios, you'll often have this larger (e.g., 1000) so that you're not spending more of your GPU time evaluating instead of training\n",
    "- `save_strategy` - how over to save your model's parameters during training. This is either \"epoch\" or \"steps\", where `save_steps` is used. The logic for setting the argument is similar to that for `evaluation_strategy`. Because our dataset is relatively small, use \"steps\".\n",
    "- `save_steps` - how many steps between saving the model's parameters. Typically you set this the same as `eval_steps` which we'll do here.\n",
    "- `num_train_epochs` - how many epochs to train for. Use 10 for now.\n",
    "- `logging_dir` - where to save the log files.\n",
    "- `load_best_model_at_end` -  Whether or not to load the best model found during training at the end of training. When this option is enabled, the best checkpoint will always be saved. This is kind of a sneakily-important argument. If you set this to `True`, the `Trainer` will automatically keep track of what the best model is so far (checked at every `save_strategy`) so you always have a copy of the parameters on disk in the checkpoint file for the best version. If you don't set this, you might keep training and never save your best model! Set this to `True`.\n",
    "- `metric_for_best_model` - another important argument: this says how we should define our \"best\" model in terms of a metric. We could use loss on the training data with \"loss\", but since we have an evaluation dataset, we'll choose based on performance on that model. When looking at the metrics on the dev/evaluation/validation dataset, all of the metrics get prefixed with \"eval_\". In this case, use \"eval_f1\"\n",
    "- `greater_is_better` - if we're setting `metric_for_best_model` we need to tell the `Trainer` which direction is better, e.g., lower is better for \"loss\" but greater is better for metrics like \"f1\". \n",
    "- `report_to` - the `Trainer` code is hooked into common logging libraries. We'll use `wandb` like in Homework 2. You might not even need to do anything for it to log but you'll need to make sure you can get plots showing up on Weights & Biases for the homework.\n",
    "\n",
    "Here are a few useful arguments you won't need here but you might want to try or explore later:\n",
    "- `fp16` - Most floating-point computation is done with 32 bits. However, some modern GPUs and even CPUs can support floating-point operations with fewer bits. These operations are faster, though less precise. Because of the sheer number of calculations, it's often useful to prioritize speed and you can turn on 16-bit floating point by setting `fp16=True`. There are a bunch of other options like this if you're curious (good office hours discussion too).\n",
    "- `gradient_accumulation_steps` - For complex NLP tasks, sometimes we only have enough GPU memory for very small batches (e.g., 2 or 4 items). However, we can simulate big batches by asking the trainer to _accumulate_ (i.e., sum) the gradients across batches before taking the update step, which allows us to have arbitrarily large \"accumulated batches\". \n",
    "- `lr_scheduler_type` - In our previous homework, our SGD/AdamW optimizers all used the same learning rate at each step. But our model gets better every step, so sometimes we might want to be able to make smaller updates so we don't change too much and make the model worse. This argument lets you choose different learning rate [schedulers](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/optimizer_schedules#transformers.SchedulerType) that will dynamically change the learning rate based on how training is going.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a049398b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:17.262646Z",
     "start_time": "2024-03-19T20:41:17.244030Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # 训练后模型和输出文件将被保存的目录\n",
    "    overwrite_output_dir=True,  # 是否覆盖输出目录\n",
    "    learning_rate=2e-5,  # 学习率\n",
    "    per_device_train_batch_size=8,  # 每个设备的训练批次大小，根据GPU内存调整\n",
    "    per_device_eval_batch_size=8,  # 每个设备的评估批次大小，根据GPU内存调整\n",
    "    do_eval=True,  # 是否在训练期间进行评估\n",
    "    seed=12345,  # 随机种子\n",
    "    evaluation_strategy=\"steps\",  # 评估策略\n",
    "    eval_steps=50,  # 每50步进行一次评估\n",
    "    save_strategy=\"steps\",  # 保存策略\n",
    "    save_steps=50,  # 每50步保存一次模型\n",
    "    num_train_epochs=10,  # 训练的轮数\n",
    "    logging_dir=\"./logs\",  # 日志目录\n",
    "    load_best_model_at_end=True,  # 训练结束时加载最佳模型\n",
    "    metric_for_best_model=\"eval_f1\",  # 选择最佳模型的指标\n",
    "    greater_is_better=True,  # 指标越高越好\n",
    "    report_to=[\"wandb\"]  # 使用Weights & Biases进行记录\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a60e34",
   "metadata": {},
   "source": [
    "## Task 3.3.3 Defining some evaluation metrics\n",
    "\n",
    "How good is our model? We'll need to provide the `Trainer` some function that given some predictions, can evaluate how good the predictions are. When the `Trainer` instance calls this function, it will pass in a tuple that contains the logits of the model's predictions. These are the unnormalized weights for each of the labels (the logit is the inverse function of the sigmoid). We _could_ normalize these back with a softmax, but instead, we can simply figure out which dimension's value is largest and say that's the dimension label.\n",
    "\n",
    "The `compute_metrics` function can return a dictionary that maps a metric name to its value. This will let us track multiple metrics over time. All of these metrics also get recorded with `wandb` by `Trainer` too so we'll see how the model trains. **Important Note:** When the `Trainer` class evaluates on the development data, the key names for the matrics get prefixed with \"eval_\", so if we reported a dictionary with \"f1\" as a key, we'd see a corresponding metric of \"eval_f1\" in our logs.\n",
    "\n",
    "For our model, we'll compute binary F1. This only keeps track of the positive class, which is appropriate in our case where we want to know whether the model is good at finding a model in terms of its precision and recall. Use the `sklearn` to calculate these.\n",
    "\n",
    "**NOTE:** Earlier we had to convert all the labels/classes into IDs starting from 0 and this code helps explain why--each of the classes has its own dimension!\n",
    "\n",
    "**NOTE:** The labels we get in the `eval_pred` tuple are the same labels that we specified in our dataset. `Trainer` looks for this column so that it can pass it through the `AutoModel` and have it reported here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6687835b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:19.234891Z",
     "start_time": "2024-03-19T20:41:19.232440Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    # 从eval_pred中提取logits和标签\n",
    "    logits, labels = eval_pred\n",
    "    # 计算每个样本最可能的类别\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # 计算精确度、召回率和F1分数\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    # 返回一个包含这些指标的字典\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391c0bf",
   "metadata": {},
   "source": [
    "## Task 3.3.4: Setup the `Trainer`\n",
    "\n",
    "Finally! Let's specify the `Trainer` that's going to run the training. Most of our arguments and hyperparameters have already been specified in the `TrainingArguments` but there are still a few things we need to specify:\n",
    "\n",
    "- `model` - A pre-trained model to fine-tune\n",
    "- `args` - the `TrainingArguments` we just defined\n",
    "- `train_dataset` - the training portion of the tokenized dataset\n",
    "- `eval_dataset` - the portion of the tokenized dataset that we'll use during training to evaluate \n",
    "- `tokenizer` - the tokenizer model used to turn text into sequences\n",
    "- `compute_metrics` - the `compute_metrics` function we just defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd262185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T20:41:21.451427Z",
     "start_time": "2024-03-19T20:41:20.734371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16979\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # 之前加载的预训练模型\n",
    "    args=training_args,  # 之前定义的训练参数\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # 训练数据集\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # 评估数据集\n",
    "    tokenizer=tokenizer,  # 使用的分词器\n",
    "    compute_metrics=compute_metrics  # 评估函数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bacaf1",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-19T20:41:23.901623Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33myanzhuo\u001B[0m (\u001B[33myanzhuoteam\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>G:\\My Drive\\SI_630\\Homeworks\\Homework3\\wandb\\run-20240320_044126-7uamtxj6</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/yanzhuoteam/huggingface/runs/7uamtxj6' target=\"_blank\">vague-salad-1</a></strong> to <a href='https://wandb.ai/yanzhuoteam/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/yanzhuoteam/huggingface' target=\"_blank\">https://wandb.ai/yanzhuoteam/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/yanzhuoteam/huggingface/runs/7uamtxj6' target=\"_blank\">https://wandb.ai/yanzhuoteam/huggingface/runs/7uamtxj6</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='31' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  31/6750 00:52 < 3:23:30, 0.55 it/s, Epoch 0.04/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's train!\n",
    "trainer.train() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47be60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we finish training, we can evaluate the model on the dev set. Note that\n",
    "# since we specified the trainer to load the best model at the end, the \n",
    "# trainer will automatically load the best model for us to use here.\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137385a",
   "metadata": {},
   "source": [
    "# Task 3.5: Multilabel classification\n",
    "\n",
    "In the last part of the assignment, we'll work on the full data. Here, the task becomes a bit more complex. Each piece of text has between 0 and $k$ binary labels associated with it, specifying which of the $k$ values were observed. This type of task setup is called [_multilabel classification_](https://en.wikipedia.org/wiki/Multi-label_classification) where we want to predict multiple labels at the same time. You can contrast this with _multiclass classification_ where we want to predict which class of multiple classes is present, but we only make one prediction.\n",
    "\n",
    "Ideally, we want to predict all of them at once! For some motivation, if we tried to predict them each individually as we did earlier, we'd need to train separate classifiers for each, which is very computationally expensive. As a second motivation, often there are some shared relationships between labels. When the model gets to train on multilabel data, you can get improved performance when the model learns the correlation/relationships between labels.\n",
    "\n",
    "However, training a multilabel classifier will require us to modify how we set up the `Trainer` and model. In Task 3.5 you get to see another example of how to train using this new task type. \n",
    "\n",
    "## Task 3.5.1 Loading and preparing the data\n",
    "\n",
    "Start by loading the train, dev, and test `DataFrames` for the multilabel files provided with the assignment. These files will have many more columns indicating the presence of different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load in the three datasets using pandas\n",
    "\n",
    "train_df = None\n",
    "test_df = None \n",
    "dev_df = None \n",
    "\n",
    "# TODO: Convert the dataframes to HuggingFace's Dataset format\n",
    "\n",
    "# TODO: Create a DatasetDict object to hold the three datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754be70a",
   "metadata": {},
   "source": [
    "## Task 3.5.2: Preparing multilabel data\n",
    "\n",
    "Like in the earlier binary classifier, we need to get a list of labels for our data and a way of mapping them to their index. This time, we'll have more labels though. The major new wrinkle will come when we preprocess the data. Here, we'll need to encode our multilabel ground truth as a binary vector indicating which labels were present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Create the list of labels from the columns of the train_df\n",
    "# 2. Create a dictionary to map the label names to their index\n",
    "# 3. Create a dictionary to map the index to the label names\n",
    "\n",
    "labels = None \n",
    "id2label = None \n",
    "label2id = None \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c14b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(examples):\n",
    "  '''Preprocesses a batch of examples from a dataset and \n",
    "     returns a BatchEncoding object with the tokenized inputs'''\n",
    "\n",
    "  # TODO:\n",
    "  # 1. Get the text from the input examples\n",
    "  # 2. Tokenize the text using the tokenizer (see note below). Note that\n",
    "  #    this is going to return the BatchEncoding object you'll update\n",
    "  # 3. Get the labels from the input examples\n",
    "  # 4. Create a numpy array of shape (batch_size, num_labels) filled with zeros\n",
    "  # 5. For each of the non-zero labels, set the corresponding column \n",
    "  #    in the numpy array to 1. This is specifying the vector of which\n",
    "  #    labels are present\n",
    "  # 6. Add the numpy array to the BatchEncoding object with the key \"labels\"\n",
    "  # 7. Return the BatchEncoding object\n",
    "  #\n",
    "  # NOTE: The tokenizer object has a __call__ method that you can use to\n",
    "  # tokenize a batch of texts. The tokenizer will return a BatchEncoding\n",
    "  # that has all the information on the ids, masks, etc. for the tokens.\n",
    "  # The BatchEncoding also acts as a dictionary, so you can add the labels\n",
    "  # to it before you return it. For details see:\n",
    "  # https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__\n",
    "\n",
    "\n",
    "\n",
    "  return \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71751bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Map the preprocess_data function to the datasets to get the\n",
    "#       multilabeled data in the right format.\n",
    "multilabel_ds = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c0ff4",
   "metadata": {},
   "source": [
    "### Exploration time\n",
    "\n",
    "Let's get a sense of what is going on in the above. If it's helpful, try walking through these steps manually too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what happens if you tokenize some text\n",
    "text = \"This is a test of the tokenizer.\"\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "print(type(encoding))\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d14d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the new tokenized dataset looks like \n",
    "example = multilabel_ds['train'][0]\n",
    "\n",
    "# We can use the tokenizer to reverse the ID mapping to see what the text was\n",
    "print(tokenizer.decode(example['input_ids']))\n",
    "\n",
    "#NOTE: in the example below, we can see that the tokenizer has include the\n",
    "#      special tokens for the BERT model. The [CLS] token is the first token\n",
    "#      and the [SEP] token is the last token. The [PAD] token is used to pad\n",
    "#      the input to the maximum length specified in the tokenizer call so\n",
    "#      all inputs are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the labels look like\n",
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can reverse the label encoding to see what the labels are using our\n",
    "# dictionary we created earlier\n",
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75861dee",
   "metadata": {},
   "source": [
    "## Task 3.5.3 Setting up the model and `TrainingArguments`\n",
    "\n",
    "Now that the data is ready for us to use, let's create the model and `Trainer`. This time, we'll add a few more arguments when we load our `AutoModel`:\n",
    "\n",
    "- `problem_type` - we can specify what kind of problem we're going to train based on the pre-trained model. Here, we'll use \"multi_label_classification\"\n",
    "- `id2label` - the dictionary we just created from IDs to the label name\n",
    "- `label2id` - the dictionary we just created from label names to IDs \n",
    "\n",
    "We'll still need to set the `num_labels` argument too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf68ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the AutoModelForSequenceClassification model using the\n",
    "#       specified model type\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the TrainingArguments object with the specified arguments\n",
    "#       for the multilabel training. You can reuse the training_args from\n",
    "#       the previous training, but you'll want to change the output_dir\n",
    "#       to something different so you don't overwrite the previous binary\n",
    "#       classification model.\n",
    "multilabel_training_args = TrainingArguments(\n",
    "    # FILL IN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec045b8",
   "metadata": {},
   "source": [
    "## Task 3.3.4: Multilabel Evaluate Metric\n",
    "\n",
    "Our previous `compute_metrics` function used the metrics designed for binary prediction. We'll need to update the function slightly here so that we can score our multilabel predictions. Thankfully, the `sklearn` functions for scoring _can_ support multilabel predictions so we won't need to change those _but_ they aren't designed for binary so we'll switch to \"micro\" averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20aa134",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def compute_multilabel_metrics(eval_pred: EvalPrediction):\n",
    "    # TODO:\n",
    "    # 1. Get the logits and labels from the eval_pred\n",
    "    # 2. Compute the probabilities from the logits using the sigmoid function\n",
    "    # 3. Round the probabilities to get the binary predictions\n",
    "    # 4. Calculate micro-averaged precision, recal, and F1\n",
    "    # 5. Return the values as a dictionary with key names for indicating the metric\n",
    "\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7269d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the Trainer object's arguments for multilabel training\n",
    "#       This will be similar to the previous Trainer object, but you'll\n",
    "#       change the datasets and the metrics function.\n",
    "multilabel_trainer = Trainer(\n",
    "    # FILL IN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a03a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "multilabel_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the dev set\n",
    "multilabel_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the trainer to predict() on the test set and then score the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a78290",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "Let's see what exactly our model can do. Often it's helpful to work through simple examples of inputs and outputs to get a sense of what data and datatypes are flowing through the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's grab a text from the test set to see what the model predicts\n",
    "text = ds['test']['text'][2]\n",
    "print(text)\n",
    "\n",
    "# Now let's tokenize the text\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# We need to move the encoding to the device the model is on\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "# Now let's get the model's predictions\n",
    "outputs = multilabel_trainer.model(**encoding)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the outputs are a specific type of object that has the logits\n",
    "logits = outputs.logits\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll turn the logits into probabilities using the sigmoid function\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "\n",
    "# Round the probabilities to get the predicted labels.\n",
    "# Remember, these are whether each value label being present.\n",
    "predictions = probs.detach().numpy().round()\n",
    "\n",
    "# Turn predictions into actual label names using our\n",
    "# id2label dictionary\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
