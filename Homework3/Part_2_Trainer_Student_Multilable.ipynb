{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7242b8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-20T17:45:47.121546Z"
    },
    "id": "dd7242b8",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23324e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T22:46:47.479584Z",
     "start_time": "2024-03-19T22:46:47.165588Z"
    },
    "id": "23324e30"
   },
   "outputs": [],
   "source": [
    "model_name = \"google-bert/bert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137385a",
   "metadata": {
    "id": "a137385a"
   },
   "source": [
    "# Task 3.5: Multilabel classification\n",
    "\n",
    "In the last part of the assignment, we'll work on the full data. Here, the task becomes a bit more complex. Each piece of text has between 0 and $k$ binary labels associated with it, specifying which of the $k$ values were observed. This type of task setup is called [_multilabel classification_](https://en.wikipedia.org/wiki/Multi-label_classification) where we want to predict multiple labels at the same time. You can contrast this with _multiclass classification_ where we want to predict which class of multiple classes is present, but we only make one prediction.\n",
    "\n",
    "Ideally, we want to predict all of them at once! For some motivation, if we tried to predict them each individually as we did earlier, we'd need to train separate classifiers for each, which is very computationally expensive. As a second motivation, often there are some shared relationships between labels. When the model gets to train on multilabel data, you can get improved performance when the model learns the correlation/relationships between labels.\n",
    "\n",
    "However, training a multilabel classifier will require us to modify how we set up the `Trainer` and model. In Task 3.5 you get to see another example of how to train using this new task type.\n",
    "\n",
    "## Task 3.5.1 Loading and preparing the data\n",
    "\n",
    "Start by loading the train, dev, and test `DataFrames` for the multilabel files provided with the assignment. These files will have many more columns indicating the presence of different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f53e824-4ef1-4e0d-833c-7c3ae3f33962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed8b288",
   "metadata": {
    "id": "aed8b288"
   },
   "outputs": [],
   "source": [
    "train_data_path = 'si630-w24-train.multilabel.tsv'\n",
    "test_data_path = 'si630-w24-test.multilabel.tsv'\n",
    "dev_data_path = 'si630-w24-dev.multilabel.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_data_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_data_path, sep='\\t')\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'validation': dev_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754be70a",
   "metadata": {
    "id": "754be70a"
   },
   "source": [
    "## Task 3.5.2: Preparing multilabel data\n",
    "\n",
    "Like in the earlier binary classifier, we need to get a list of labels for our data and a way of mapping them to their index. This time, we'll have more labels though. The major new wrinkle will come when we preprocess the data. Here, we'll need to encode our multilabel ground truth as a binary vector indicating which labels were present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db9564a",
   "metadata": {
    "id": "8db9564a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-direction: thought',\n",
       " 'Self-direction: action',\n",
       " 'Stimulation',\n",
       " 'Hedonism',\n",
       " 'Achievement',\n",
       " 'Power: dominance',\n",
       " 'Power: resources',\n",
       " 'Face',\n",
       " 'Security: personal',\n",
       " 'Security: societal',\n",
       " 'Tradition',\n",
       " 'Conformity: rules',\n",
       " 'Conformity: interpersonal',\n",
       " 'Humility',\n",
       " 'Benevolence: caring',\n",
       " 'Benevolence: dependability',\n",
       " 'Universalism: concern',\n",
       " 'Universalism: nature',\n",
       " 'Universalism: tolerance',\n",
       " 'Universalism: objectivity']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_df.columns.tolist()[2:]  \n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c14b06",
   "metadata": {
    "id": "07c14b06"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    texts = examples[\"text\"]\n",
    "    tokenized_inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"np\")\n",
    "\n",
    "    label_data = {label: examples[label] for label in labels}\n",
    "    batch_size = len(texts)\n",
    "    num_labels = len(labels)\n",
    "    label_matrix = np.zeros((batch_size, 20))\n",
    "    for i, label in enumerate(labels):\n",
    "        label_vals = label_data[label]\n",
    "        for j, val in enumerate(label_vals):\n",
    "            if val == 1:\n",
    "                label_matrix[j, i] = 1\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_matrix\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71751bd8",
   "metadata": {
    "id": "71751bd8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8681c641441146e895b2c765f01b21cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5205cc2a5b3b42b992be8d1b064ca060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612f1f73f39c49a89af083daf9f4b43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1896 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multilabel_ds = dataset_dict.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c0ff4",
   "metadata": {
    "id": "1c0c0ff4"
   },
   "source": [
    "### Exploration time\n",
    "\n",
    "Let's get a sense of what is going on in the above. If it's helpful, try walking through these steps manually too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca2bb42b",
   "metadata": {
    "id": "ca2bb42b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': [101, 1188, 1110, 170, 2774, 1104, 1103, 22559, 17260, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test of the tokenizer.\"\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "print(type(encoding))\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d14d37",
   "metadata": {
    "id": "38d14d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] We should ban human cloning because as it will only cause huge issues when you have a bunch of the same humans running around all acting the same. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "example = multilabel_ds['train'][0]\n",
    "\n",
    "print(tokenizer.decode(example['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dbd5874",
   "metadata": {
    "id": "6dbd5874"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f720f24a",
   "metadata": {
    "id": "f720f24a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Security: societal']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75861dee",
   "metadata": {
    "id": "75861dee"
   },
   "source": [
    "## Task 3.5.3 Setting up the model and `TrainingArguments`\n",
    "\n",
    "Now that the data is ready for us to use, let's create the model and `Trainer`. This time, we'll add a few more arguments when we load our `AutoModel`:\n",
    "\n",
    "- `problem_type` - we can specify what kind of problem we're going to train based on the pre-trained model. Here, we'll use \"multi_label_classification\"\n",
    "- `id2label` - the dictionary we just created from IDs to the label name\n",
    "- `label2id` - the dictionary we just created from label names to IDs\n",
    "\n",
    "We'll still need to set the `num_labels` argument too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf68ac4",
   "metadata": {
    "id": "baf68ac4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-bert/bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=20,\n",
    "    problem_type=\"multi_label_classification\", \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fa8f0c6",
   "metadata": {
    "id": "9fa8f0c6"
   },
   "outputs": [],
   "source": [
    "multilabel_training_args = TrainingArguments(\n",
    "    output_dir=\"./results_multilabel\", \n",
    "    overwrite_output_dir=True,  \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8, \n",
    "    num_train_epochs=10, \n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=50, \n",
    "    save_strategy=\"no\",\n",
    "    do_eval=True,\n",
    "    logging_dir=\"./logs_multilabel\",\n",
    "    report_to=[\"wandb\"],\n",
    "    seed=12345\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5078b43f-cec9-4833-b9ef-1c8779c78de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "class MyBestModelSaver(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.best_metric = float('-inf')\n",
    "        self.best_model = None\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metrics = kwargs.get('metrics', {})\n",
    "        eval_metric = metrics.get('eval_f1', 0)\n",
    "        if eval_metric > self.best_metric:\n",
    "            self.best_metric = eval_metric\n",
    "            self.best_model = kwargs['model'].state_dict()\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.best_model is not None:\n",
    "            model_path = \"./best_model2\"\n",
    "            torch.save(self.best_model, model_path)\n",
    "            print(f\"Best model saved to {model_path} with F1 score: {self.best_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec045b8",
   "metadata": {
    "id": "0ec045b8"
   },
   "source": [
    "## Task 3.3.4: Multilabel Evaluate Metric\n",
    "\n",
    "Our previous `compute_metrics` function used the metrics designed for binary prediction. We'll need to update the function slightly here so that we can score our multilabel predictions. Thankfully, the `sklearn` functions for scoring _can_ support multilabel predictions so we won't need to change those _but_ they aren't designed for binary so we'll switch to \"micro\" averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f20aa134",
   "metadata": {
    "id": "f20aa134"
   },
   "outputs": [],
   "source": [
    "def compute_multilabel_metrics(eval_pred: EvalPrediction):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    predictions = np.round(probs)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='micro')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d7269d1",
   "metadata": {
    "id": "1d7269d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16979\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "multilabel_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=multilabel_training_args,\n",
    "    train_dataset=multilabel_ds[\"train\"],\n",
    "    eval_dataset=multilabel_ds[\"validation\"],\n",
    "    compute_metrics=compute_multilabel_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[MyBestModelSaver()],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a03a27",
   "metadata": {
    "id": "13a03a27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myanzhuo\u001b[0m (\u001b[33myanzhuoteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>G:\\My Drive\\SI_630\\Homeworks\\Homework3\\wandb\\run-20240321_014830-i041te1w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yanzhuoteam/huggingface/runs/i041te1w' target=\"_blank\">young-voice-20</a></strong> to <a href='https://wandb.ai/yanzhuoteam/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yanzhuoteam/huggingface' target=\"_blank\">https://wandb.ai/yanzhuoteam/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yanzhuoteam/huggingface/runs/i041te1w' target=\"_blank\">https://wandb.ai/yanzhuoteam/huggingface/runs/i041te1w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5351' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5351/6750 1:13:27 < 19:12, 1.21 it/s, Epoch 7.93/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.429205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.410953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.401465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.394240</td>\n",
       "      <td>0.626904</td>\n",
       "      <td>0.116528</td>\n",
       "      <td>0.196526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.385316</td>\n",
       "      <td>0.711849</td>\n",
       "      <td>0.124705</td>\n",
       "      <td>0.212231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.378306</td>\n",
       "      <td>0.707237</td>\n",
       "      <td>0.135241</td>\n",
       "      <td>0.227063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.371848</td>\n",
       "      <td>0.709841</td>\n",
       "      <td>0.154270</td>\n",
       "      <td>0.253456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.370021</td>\n",
       "      <td>0.733540</td>\n",
       "      <td>0.148923</td>\n",
       "      <td>0.247582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.363919</td>\n",
       "      <td>0.722354</td>\n",
       "      <td>0.191068</td>\n",
       "      <td>0.302201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.361834</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>0.220632</td>\n",
       "      <td>0.333809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.361085</td>\n",
       "      <td>0.654752</td>\n",
       "      <td>0.253499</td>\n",
       "      <td>0.365491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.356642</td>\n",
       "      <td>0.688973</td>\n",
       "      <td>0.257430</td>\n",
       "      <td>0.374814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.352214</td>\n",
       "      <td>0.723941</td>\n",
       "      <td>0.239189</td>\n",
       "      <td>0.359574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.349773</td>\n",
       "      <td>0.683767</td>\n",
       "      <td>0.293442</td>\n",
       "      <td>0.410651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.349215</td>\n",
       "      <td>0.683269</td>\n",
       "      <td>0.306337</td>\n",
       "      <td>0.423018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.349210</td>\n",
       "      <td>0.683808</td>\n",
       "      <td>0.309483</td>\n",
       "      <td>0.426112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.347488</td>\n",
       "      <td>0.703399</td>\n",
       "      <td>0.296116</td>\n",
       "      <td>0.416777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.345782</td>\n",
       "      <td>0.691282</td>\n",
       "      <td>0.301777</td>\n",
       "      <td>0.420142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.342492</td>\n",
       "      <td>0.692528</td>\n",
       "      <td>0.304608</td>\n",
       "      <td>0.423111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.342632</td>\n",
       "      <td>0.666081</td>\n",
       "      <td>0.357918</td>\n",
       "      <td>0.465630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.339705</td>\n",
       "      <td>0.708838</td>\n",
       "      <td>0.303979</td>\n",
       "      <td>0.425490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.340734</td>\n",
       "      <td>0.670546</td>\n",
       "      <td>0.353357</td>\n",
       "      <td>0.462822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.344412</td>\n",
       "      <td>0.692927</td>\n",
       "      <td>0.311212</td>\n",
       "      <td>0.429517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.338913</td>\n",
       "      <td>0.679593</td>\n",
       "      <td>0.336216</td>\n",
       "      <td>0.449868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.336789</td>\n",
       "      <td>0.723757</td>\n",
       "      <td>0.309011</td>\n",
       "      <td>0.433106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.339060</td>\n",
       "      <td>0.688819</td>\n",
       "      <td>0.343922</td>\n",
       "      <td>0.458779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.339153</td>\n",
       "      <td>0.679506</td>\n",
       "      <td>0.363422</td>\n",
       "      <td>0.473566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.337462</td>\n",
       "      <td>0.684133</td>\n",
       "      <td>0.365466</td>\n",
       "      <td>0.476425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.337026</td>\n",
       "      <td>0.680371</td>\n",
       "      <td>0.369555</td>\n",
       "      <td>0.478956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.338687</td>\n",
       "      <td>0.680568</td>\n",
       "      <td>0.369555</td>\n",
       "      <td>0.479005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.340572</td>\n",
       "      <td>0.683808</td>\n",
       "      <td>0.351313</td>\n",
       "      <td>0.464160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.339186</td>\n",
       "      <td>0.684776</td>\n",
       "      <td>0.360749</td>\n",
       "      <td>0.472551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.337360</td>\n",
       "      <td>0.657658</td>\n",
       "      <td>0.378833</td>\n",
       "      <td>0.480742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.337533</td>\n",
       "      <td>0.668402</td>\n",
       "      <td>0.383551</td>\n",
       "      <td>0.487410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.336519</td>\n",
       "      <td>0.674528</td>\n",
       "      <td>0.382293</td>\n",
       "      <td>0.488006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.335047</td>\n",
       "      <td>0.682030</td>\n",
       "      <td>0.367668</td>\n",
       "      <td>0.477777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.335961</td>\n",
       "      <td>0.680863</td>\n",
       "      <td>0.372071</td>\n",
       "      <td>0.481188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.336908</td>\n",
       "      <td>0.670601</td>\n",
       "      <td>0.375216</td>\n",
       "      <td>0.481194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.333628</td>\n",
       "      <td>0.674588</td>\n",
       "      <td>0.379462</td>\n",
       "      <td>0.485709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.666301</td>\n",
       "      <td>0.382136</td>\n",
       "      <td>0.485709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.335306</td>\n",
       "      <td>0.675493</td>\n",
       "      <td>0.377103</td>\n",
       "      <td>0.484004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.333628</td>\n",
       "      <td>0.657667</td>\n",
       "      <td>0.394559</td>\n",
       "      <td>0.493218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.334090</td>\n",
       "      <td>0.660519</td>\n",
       "      <td>0.399906</td>\n",
       "      <td>0.498188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.335734</td>\n",
       "      <td>0.663994</td>\n",
       "      <td>0.390627</td>\n",
       "      <td>0.491881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.335132</td>\n",
       "      <td>0.659184</td>\n",
       "      <td>0.406353</td>\n",
       "      <td>0.502773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.337379</td>\n",
       "      <td>0.653392</td>\n",
       "      <td>0.399906</td>\n",
       "      <td>0.496147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.335310</td>\n",
       "      <td>0.661939</td>\n",
       "      <td>0.396289</td>\n",
       "      <td>0.495770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.336866</td>\n",
       "      <td>0.676088</td>\n",
       "      <td>0.383708</td>\n",
       "      <td>0.489567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.336613</td>\n",
       "      <td>0.650332</td>\n",
       "      <td>0.415317</td>\n",
       "      <td>0.506910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.339544</td>\n",
       "      <td>0.660951</td>\n",
       "      <td>0.400063</td>\n",
       "      <td>0.498433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.337205</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.399748</td>\n",
       "      <td>0.495275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.339434</td>\n",
       "      <td>0.659097</td>\n",
       "      <td>0.406196</td>\n",
       "      <td>0.502627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.339399</td>\n",
       "      <td>0.648486</td>\n",
       "      <td>0.407611</td>\n",
       "      <td>0.500579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.339625</td>\n",
       "      <td>0.658637</td>\n",
       "      <td>0.404152</td>\n",
       "      <td>0.500926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.340220</td>\n",
       "      <td>0.654541</td>\n",
       "      <td>0.398962</td>\n",
       "      <td>0.495750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.342749</td>\n",
       "      <td>0.646335</td>\n",
       "      <td>0.413273</td>\n",
       "      <td>0.504173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.341666</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.415160</td>\n",
       "      <td>0.503625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.344035</td>\n",
       "      <td>0.638713</td>\n",
       "      <td>0.412014</td>\n",
       "      <td>0.500908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.342955</td>\n",
       "      <td>0.652529</td>\n",
       "      <td>0.401635</td>\n",
       "      <td>0.497226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.343506</td>\n",
       "      <td>0.648483</td>\n",
       "      <td>0.400063</td>\n",
       "      <td>0.494845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.341994</td>\n",
       "      <td>0.645043</td>\n",
       "      <td>0.414373</td>\n",
       "      <td>0.504596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.345667</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>0.417518</td>\n",
       "      <td>0.503318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.343955</td>\n",
       "      <td>0.638928</td>\n",
       "      <td>0.423809</td>\n",
       "      <td>0.509596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.345362</td>\n",
       "      <td>0.644317</td>\n",
       "      <td>0.406510</td>\n",
       "      <td>0.498505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.346077</td>\n",
       "      <td>0.632776</td>\n",
       "      <td>0.428684</td>\n",
       "      <td>0.511109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.343948</td>\n",
       "      <td>0.661586</td>\n",
       "      <td>0.402736</td>\n",
       "      <td>0.500684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.344582</td>\n",
       "      <td>0.639971</td>\n",
       "      <td>0.423494</td>\n",
       "      <td>0.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.346188</td>\n",
       "      <td>0.654499</td>\n",
       "      <td>0.406039</td>\n",
       "      <td>0.501165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.348247</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.424752</td>\n",
       "      <td>0.510731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.349873</td>\n",
       "      <td>0.648796</td>\n",
       "      <td>0.393930</td>\n",
       "      <td>0.490215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.348454</td>\n",
       "      <td>0.651943</td>\n",
       "      <td>0.406196</td>\n",
       "      <td>0.500533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.347466</td>\n",
       "      <td>0.648360</td>\n",
       "      <td>0.410285</td>\n",
       "      <td>0.502552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.348369</td>\n",
       "      <td>0.628467</td>\n",
       "      <td>0.434660</td>\n",
       "      <td>0.513898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.348759</td>\n",
       "      <td>0.645828</td>\n",
       "      <td>0.412643</td>\n",
       "      <td>0.503550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.349619</td>\n",
       "      <td>0.628662</td>\n",
       "      <td>0.435289</td>\n",
       "      <td>0.514403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.651269</td>\n",
       "      <td>0.403523</td>\n",
       "      <td>0.498301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.349863</td>\n",
       "      <td>0.640909</td>\n",
       "      <td>0.421293</td>\n",
       "      <td>0.508397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.350746</td>\n",
       "      <td>0.650766</td>\n",
       "      <td>0.414059</td>\n",
       "      <td>0.506103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.351001</td>\n",
       "      <td>0.637179</td>\n",
       "      <td>0.424752</td>\n",
       "      <td>0.509719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.352544</td>\n",
       "      <td>0.642787</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>0.501104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.352898</td>\n",
       "      <td>0.640468</td>\n",
       "      <td>0.421607</td>\n",
       "      <td>0.508487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.359550</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.454631</td>\n",
       "      <td>0.520573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.355904</td>\n",
       "      <td>0.643411</td>\n",
       "      <td>0.404623</td>\n",
       "      <td>0.496814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.355299</td>\n",
       "      <td>0.635779</td>\n",
       "      <td>0.419720</td>\n",
       "      <td>0.505636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.356426</td>\n",
       "      <td>0.640881</td>\n",
       "      <td>0.411700</td>\n",
       "      <td>0.501340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.356433</td>\n",
       "      <td>0.632548</td>\n",
       "      <td>0.426639</td>\n",
       "      <td>0.509579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.359033</td>\n",
       "      <td>0.621946</td>\n",
       "      <td>0.440321</td>\n",
       "      <td>0.515606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.358580</td>\n",
       "      <td>0.631443</td>\n",
       "      <td>0.423809</td>\n",
       "      <td>0.507199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.358972</td>\n",
       "      <td>0.622267</td>\n",
       "      <td>0.438591</td>\n",
       "      <td>0.514528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.359581</td>\n",
       "      <td>0.630229</td>\n",
       "      <td>0.428841</td>\n",
       "      <td>0.510387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.360028</td>\n",
       "      <td>0.621494</td>\n",
       "      <td>0.435603</td>\n",
       "      <td>0.512204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.361279</td>\n",
       "      <td>0.633278</td>\n",
       "      <td>0.416575</td>\n",
       "      <td>0.502561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.359474</td>\n",
       "      <td>0.640145</td>\n",
       "      <td>0.416260</td>\n",
       "      <td>0.504479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.360915</td>\n",
       "      <td>0.640338</td>\n",
       "      <td>0.416889</td>\n",
       "      <td>0.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.360661</td>\n",
       "      <td>0.625681</td>\n",
       "      <td>0.433716</td>\n",
       "      <td>0.512306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.361697</td>\n",
       "      <td>0.634768</td>\n",
       "      <td>0.415160</td>\n",
       "      <td>0.501997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.363104</td>\n",
       "      <td>0.624654</td>\n",
       "      <td>0.425539</td>\n",
       "      <td>0.506220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.361978</td>\n",
       "      <td>0.630060</td>\n",
       "      <td>0.425853</td>\n",
       "      <td>0.508211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.363082</td>\n",
       "      <td>0.635571</td>\n",
       "      <td>0.412486</td>\n",
       "      <td>0.500286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.364139</td>\n",
       "      <td>0.619681</td>\n",
       "      <td>0.439692</td>\n",
       "      <td>0.514396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.365144</td>\n",
       "      <td>0.615150</td>\n",
       "      <td>0.443151</td>\n",
       "      <td>0.515174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.364825</td>\n",
       "      <td>0.626931</td>\n",
       "      <td>0.427583</td>\n",
       "      <td>0.508414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.364789</td>\n",
       "      <td>0.628692</td>\n",
       "      <td>0.421764</td>\n",
       "      <td>0.504847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.365807</td>\n",
       "      <td>0.627313</td>\n",
       "      <td>0.421135</td>\n",
       "      <td>0.503952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.366536</td>\n",
       "      <td>0.623771</td>\n",
       "      <td>0.429156</td>\n",
       "      <td>0.508478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.367585</td>\n",
       "      <td>0.613686</td>\n",
       "      <td>0.441422</td>\n",
       "      <td>0.513491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='203' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [203/237 00:14 < 00:02, 13.55 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16979\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\16979\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\16979\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the model!\n",
    "multilabel_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967e7d9",
   "metadata": {
    "id": "4967e7d9"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the dev set\n",
    "multilabel_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69ed15-d41a-458a-a9d3-74c85e86cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = multilabel_trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092cf2dff6131d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "new_labels = np.zeros((labels.size, 2))\n",
    "for i, label in enumerate(labels):\n",
    "    new_labels[i, label] = 1\n",
    "\n",
    "print(new_labels.shape)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(new_labels, test_predictions, average='micro')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a78290",
   "metadata": {
    "id": "e2a78290"
   },
   "source": [
    "### Exploration\n",
    "\n",
    "Let's see what exactly our model can do. Often it's helpful to work through simple examples of inputs and outputs to get a sense of what data and datatypes are flowing through the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9f51e",
   "metadata": {
    "id": "dda9f51e"
   },
   "outputs": [],
   "source": [
    "# First, let's grab a text from the test set to see what the model predicts\n",
    "text = multilabel_ds['test']['text'][2]\n",
    "print(text)\n",
    "\n",
    "# Now let's tokenize the text\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# We need to move the encoding to the device the model is on\n",
    "encoding = {k: v.to(multilabel_trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "# Now let's get the model's predictions\n",
    "outputs = multilabel_trainer.model(**encoding)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad7cc4",
   "metadata": {
    "id": "b9ad7cc4"
   },
   "outputs": [],
   "source": [
    "# Note that the outputs are a specific type of object that has the logits\n",
    "logits = outputs.logits\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a29e1",
   "metadata": {
    "id": "0e2a29e1"
   },
   "outputs": [],
   "source": [
    "# We'll turn the logits into probabilities using the sigmoid function\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "\n",
    "# Round the probabilities to get the predicted labels.\n",
    "# Remember, these are whether each value label being present.\n",
    "predictions = probs.detach().numpy().round()\n",
    "\n",
    "# Turn predictions into actual label names using our\n",
    "# id2label dictionary\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
