{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:47.755781100Z",
     "start_time": "2024-01-30T05:05:45.353919200Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Representing Text Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f293206069c1f37b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_dev = pd.read_csv(\"data/dev.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:47.821400300Z",
     "start_time": "2024-01-30T05:05:47.758782200Z"
    }
   },
   "id": "50f0203c436f94c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1.1: Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a47f75884c1fd3af"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "stop_words = {\n",
    "    'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', \n",
    "    'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', \n",
    "    'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', \n",
    "    'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', \n",
    "    'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', \n",
    "    'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', \n",
    "    'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', \n",
    "    'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', \n",
    "    'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', \n",
    "    'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', \n",
    "    'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', \n",
    "    'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', \n",
    "    'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than',\n",
    "    '<h>', '</h>'\n",
    "}\n",
    "\n",
    "stop_punctuations = '''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~—'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:47.839014600Z",
     "start_time": "2024-01-30T05:05:47.820640900Z"
    }
   },
   "id": "a4e2e158991663cc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Split the text by whitespace and return the list of tokens\n",
    "    return text.split()\n",
    "\n",
    "def better_tokenize(input_str, punctuations=stop_punctuations, stopwords=stop_words):\n",
    "    # Remove line breaks and filter out <> enclosed text\n",
    "    cleaned_str = re.sub(r'\\n|\\<[^<>]*\\>', '', input_str)\n",
    "\n",
    "    # Remove punctuations\n",
    "    for punctuation in punctuations:\n",
    "        cleaned_str = cleaned_str.replace(punctuation, '')\n",
    "\n",
    "    # Convert to lowercase and split into words\n",
    "    words = cleaned_str.lower().split()\n",
    "\n",
    "    # Filter out stopwords\n",
    "    filtered_words = filter(lambda word: word not in stopwords, words)\n",
    "\n",
    "    return list(filtered_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:47.840016Z",
     "start_time": "2024-01-30T05:05:47.824928600Z"
    }
   },
   "id": "a6e304a64dc6e150"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'text', 'goes']\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Your sample text goes here.\"\n",
    "tokens = better_tokenize(input_str)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:47.870874400Z",
     "start_time": "2024-01-30T05:05:47.834012500Z"
    }
   },
   "id": "de239779172d4c41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1.2: Building the Term-Document Matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82595ee382bf6f61"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def build_term_freq_dict(dataframe, text_col, min_freq=2, tokenize_function=better_tokenize):\n",
    "    term_counter = Counter()\n",
    "\n",
    "    # Iterate through each document and update term frequencies\n",
    "    for document in dataframe[text_col]:\n",
    "        terms = tokenize_function(document)\n",
    "        term_counter.update(terms)\n",
    "\n",
    "    # Retain terms that meet the minimum frequency threshold\n",
    "    filtered_terms = {term: freq for term, freq in term_counter.items() if freq >= min_freq}\n",
    "\n",
    "    return filtered_terms\n",
    "\n",
    "# Example usage\n",
    "term_dictionary = build_term_freq_dict(df_train, 'text', 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:48.047457100Z",
     "start_time": "2024-01-30T05:05:47.865877600Z"
    }
   },
   "id": "919b342192769e6c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# def createCompressedSparseRowMatrix(documents, vocab={}, frequencyFilter=term_dictionary):\n",
    "#     rowStart = [0]\n",
    "#     colIndices = []\n",
    "#     values = []\n",
    "# \n",
    "#     print(\"Constructing compressed sparse row (CSR) matrix...\")\n",
    "#     for document in tqdm(documents):\n",
    "#         for word in document:\n",
    "#             if word in frequencyFilter:  # Exclude terms with low frequency\n",
    "#                 wordIndex = vocab[word] if word in vocab else len(vocab)\n",
    "#                 vocab[word] = wordIndex\n",
    "#                 colIndices.append(wordIndex)\n",
    "#                 values.append(1)\n",
    "#         rowStart.append(len(colIndices))\n",
    "# \n",
    "#     csrMatrix = sparse.csr_matrix((values, colIndices, rowStart), dtype=int)\n",
    "#     return csrMatrix, vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:48.047457100Z",
     "start_time": "2024-01-30T05:05:48.017930200Z"
    }
   },
   "id": "90240e81cd23ec27"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def createCompressedSparseRowMatrix(documents, vocab=None, frequencyFilter=term_dictionary):\n",
    "    rowStart = [0]\n",
    "    colIndices = []\n",
    "    values = []\n",
    "    new_vocab = {}\n",
    "    if vocab is None:\n",
    "        print(\"Constructing compressed sparse row (CSR) matrix...\")\n",
    "        for document in tqdm(documents):\n",
    "            for word in document:\n",
    "                if word in frequencyFilter:  # Exclude terms with low frequency\n",
    "                    wordIndex = new_vocab[word] if word in new_vocab else len(new_vocab)\n",
    "                    new_vocab[word] = wordIndex\n",
    "                    colIndices.append(wordIndex)\n",
    "                    values.append(1)\n",
    "            rowStart.append(len(colIndices))\n",
    "\n",
    "        csrMatrix = sparse.csr_matrix((values, colIndices, rowStart), dtype=int)\n",
    "    else:\n",
    "        new_vocab = vocab.copy()\n",
    "        print(\"Constructing compressed sparse row (CSR) matrix...\")\n",
    "        for document in tqdm(documents):\n",
    "            for word in document:\n",
    "                if word in frequencyFilter and word in new_vocab:  # 仅处理存在于词汇表中的单词\n",
    "                    wordIndex = new_vocab[word]\n",
    "                    colIndices.append(wordIndex)\n",
    "                    values.append(1)\n",
    "            rowStart.append(len(colIndices))\n",
    "            \n",
    "        csrMatrix = sparse.csr_matrix((values, colIndices, rowStart), shape=(len(documents), len(new_vocab)), dtype=int)\n",
    "    return csrMatrix, new_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:48.047457100Z",
     "start_time": "2024-01-30T05:05:48.026494700Z"
    }
   },
   "id": "f62eb0863d08fb4f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document list for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7328/7328 [00:00<00:00, 18073.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing compressed sparse row (CSR) matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7328/7328 [00:00<00:00, 215323.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the document list\n",
    "document_list = []\n",
    "print(\"Creating document list for training...\")\n",
    "for idx, record in tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "    document_list.append(better_tokenize(record[\"text\"]))\n",
    "\n",
    "# Generate the CSR matrix\n",
    "training_matrix, training_vocab = createCompressedSparseRowMatrix(document_list, vocab=None, frequencyFilter=term_dictionary)\n",
    "\n",
    "# Add a bias column to the matrix\n",
    "bias_column = np.ones((training_matrix.shape[0], 1))\n",
    "training_matrix_with_bias = sparse.hstack([training_matrix, bias_column]).tocsr()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:48.538263700Z",
     "start_time": "2024-01-30T05:05:48.033448500Z"
    }
   },
   "id": "6caf5ae665f2d238"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2: Logistic Regression in numpy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70672e4c20b18f97"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # Using np.exp for element-wise exponential\n",
    "    return np.divide(1, np.add(1, np.exp(np.negative(X))))\n",
    "\n",
    "def log_likelihood(X, y, beta):\n",
    "    sum = 0\n",
    "    for i in range(X.shape[0]):  # 遍历样本\n",
    "        x_i = X[i].toarray().flatten()\n",
    "        sum += y[i] * np.dot(beta, x_i) - np.log10(1 + np.exp(np.dot(beta, x_i)))\n",
    "    return sum\n",
    "\n",
    "def compute_gradient(x, y, beta):\n",
    "    # Using np.subtract and np.outer for vectorized operations\n",
    "    prediction_error = np.subtract(sigmoid(np.dot(beta, x)), y)\n",
    "    gradient = np.multiply(prediction_error, x)\n",
    "    return gradient"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:48.538263700Z",
     "start_time": "2024-01-30T05:05:48.504422600Z"
    }
   },
   "id": "f9bed74041c07ba1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, learning_rate=5e-5, num_step=1000, is_plot=False):\n",
    "    # Initialize variables\n",
    "    vocab_size = X.shape[1]  # Size of the vocabulary\n",
    "    beta_params = np.zeros(vocab_size)  # Initializing beta parameters\n",
    "    prev_log_likelihood = log_likelihood(X, y, beta_params)  # Initial log-likelihood\n",
    "\n",
    "    # Initialize plot-related lists if plotting is enabled\n",
    "    steps, log_likelihoods = ([], []) if is_plot else (None, None)\n",
    "\n",
    "    print(\"Running Logistic Regression...\")\n",
    "    for step in tqdm(range(num_step)):\n",
    "        # Update beta using a gradient step\n",
    "        current_index = step % vocab_size\n",
    "        X_sample = X[current_index].toarray().ravel()\n",
    "        y_sample = y[current_index]\n",
    "        gradient = compute_gradient(X_sample, y_sample, beta_params)\n",
    "        beta_params -= learning_rate * gradient\n",
    "\n",
    "        # Record log-likelihood and step count at every 100th step\n",
    "        if is_plot and step % 100 == 0:\n",
    "            steps.append(step)\n",
    "            log_likelihoods.append(log_likelihood(X, y, beta_params))\n",
    "\n",
    "    # Return results based on whether plotting is enabled\n",
    "    return (beta_params, steps, log_likelihoods) if is_plot else beta_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:48.539257900Z",
     "start_time": "2024-01-30T05:05:48.511299500Z"
    }
   },
   "id": "807cbeb634e3abb3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def predict(text, model_coefficients, vocabulary=training_vocab):\n",
    "    feature_vector = np.zeros(len(vocabulary) + 1)\n",
    "    \n",
    "    word_counts = Counter(better_tokenize(text))\n",
    "    for word, count in word_counts.items():\n",
    "        if word in vocabulary:\n",
    "            feature_vector[vocabulary[word]] = count\n",
    "    feature_vector[-1] = 1  # Add bias term\n",
    "\n",
    "    prediction = sigmoid(np.dot(model_coefficients, feature_vector))\n",
    "    return int(prediction >= 0.152)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:48.539257900Z",
     "start_time": "2024-01-30T05:05:48.517743800Z"
    }
   },
   "id": "86ca524192dc704b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2.1: Plot log-likelihood"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5857c08699b9eb52"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 14541.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Mapping labels\n",
    "label_map = {0: 0, 1: 1}\n",
    "train_labels = np.array([label_map[label] for label in df_train['label']])\n",
    "\n",
    "# Perform logistic regression\n",
    "# regression_beta, iteration_steps, log_likelihoods = logistic_regression(\n",
    "regression_beta = logistic_regression(\n",
    "    X=training_matrix_with_bias,\n",
    "    y=train_labels,\n",
    "    num_step=10000,\n",
    "    # is_plot=True\n",
    "    is_plot=False\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:49.676074600Z",
     "start_time": "2024-01-30T05:05:48.523257600Z"
    }
   },
   "id": "e3457b9f7b996be7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# # Plotting the relationship between steps and log-likelihood\n",
    "# fig, plot_axis = plt.subplots(figsize=(10, 6))\n",
    "# sns.lineplot(x=iteration_steps, y=log_likelihoods, ax=plot_axis)\n",
    "# plot_axis.set_xlabel(\"Steps\")\n",
    "# plot_axis.set_ylabel(\"Log-likelihood (per 100 steps)\")\n",
    "# plot_axis.set_title(\"Loss v.s. Log-likelihood for Full Train Data\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:05:49.694752Z",
     "start_time": "2024-01-30T05:05:49.680233400Z"
    }
   },
   "id": "7fedcd6273d16cc5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2.2: Make prediction on validation dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8d06439b6957bf2"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [00:32<00:00, 15210.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the beta\n",
    "beta = logistic_regression(X = training_matrix_with_bias,\n",
    "                           y = train_labels, \n",
    "                           learning_rate = 5e-5, \n",
    "                           num_step = 500000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:22.996657200Z",
     "start_time": "2024-01-30T05:05:49.684750100Z"
    }
   },
   "id": "5bf98f138be537d5"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction on validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1047/1047 [00:00<00:00, 22029.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# make prediction\n",
    "y_test = [label_map[p] for p in df_dev[\"label\"]]\n",
    "y_pred = []\n",
    "print(\"Starting prediction on validation dataset...\")\n",
    "for i in tqdm(range(len(df_dev))):\n",
    "    y_pred.append(predict(df_dev[\"text\"][i], beta))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:23.083479Z",
     "start_time": "2024-01-30T05:06:22.999658700Z"
    }
   },
   "id": "a31bf111bd27fa1f"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3626373626373627"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:23.083479Z",
     "start_time": "2024-01-30T05:06:23.057028700Z"
    }
   },
   "id": "47306a3f28787542"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2.3: Make prediction on test dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cd4c6d945541412"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2094/2094 [00:00<00:00, 12673.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reverse label dictionary\n",
    "label_reverse_map = {0: 0, 1: 1}\n",
    "\n",
    "# Function to predict labels\n",
    "def make_predictions(data):\n",
    "    predictions = []\n",
    "    print(\"Predicting labels for test dataset...\")\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        text_content = str(row['text'])\n",
    "        predictions.append(predict(text_content, beta))\n",
    "    return predictions\n",
    "\n",
    "# Generate predictions\n",
    "predicted_labels = make_predictions(df_test)\n",
    "\n",
    "# Create result dataframe and save to CSV\n",
    "result_df = pd.DataFrame({\n",
    "    'par_id': df_test['par_id'],\n",
    "    'label': [label_reverse_map[label] for label in predicted_labels]\n",
    "})\n",
    "# result_df.to_csv(\"part2_result.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:23.286352800Z",
     "start_time": "2024-01-30T05:06:23.069473500Z"
    }
   },
   "id": "7f2c8730ca908573"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3: Logistic Regression in PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c096b5cc63d52aab"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def to_sparse_tensor(sparse_matrix: csr_matrix):\n",
    "    sparse_matrix = sparse_matrix.tocoo()  # 将矩阵转换为COO格式\n",
    "    indices = np.vstack((sparse_matrix.row, sparse_matrix.col))  # 获取坐标\n",
    "    indices = torch.LongTensor(indices)  # 将坐标转换为torch张量\n",
    "    values = torch.FloatTensor(sparse_matrix.data)  # 将值转换为torch张量\n",
    "    shape = torch.Size(sparse_matrix.shape)  # 获取矩阵的形状\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:23.286352800Z",
     "start_time": "2024-01-30T05:06:23.264802Z"
    }
   },
   "id": "169743168f6904ab"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(LogisticRegressionPyTorch, self).__init__()\n",
    "        self.linear = nn.Linear(input_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:23.286352800Z",
     "start_time": "2024-01-30T05:06:23.269850400Z"
    }
   },
   "id": "9c5321e61f3a298a"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document list for dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1047/1047 [00:00<00:00, 19366.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing compressed sparse row (CSR) matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1047/1047 [00:00<00:00, 189621.15it/s]\n",
      "C:\\Users\\16979\\AppData\\Local\\Temp\\ipykernel_5196\\1845610626.py:7: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:607.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "dev_labels = np.array([label_map[label] for label in df_dev['label']])\n",
    "# Prepare the document list\n",
    "document_list = []\n",
    "print(\"Creating document list for dev...\")\n",
    "for idx, record in tqdm(df_dev.iterrows(), total=df_dev.shape[0]):\n",
    "    document_list.append(better_tokenize(record[\"text\"]))\n",
    "\n",
    "# Generate the CSR matrix\n",
    "dev_matrix, dev_vocab = createCompressedSparseRowMatrix(document_list, vocab=training_vocab, frequencyFilter=term_dictionary)\n",
    "\n",
    "# Add a bias column to the matrix\n",
    "bias_column = np.ones((dev_matrix.shape[0], 1))\n",
    "dev_matrix_with_bias = sparse.hstack([dev_matrix, bias_column]).tocsr()\n",
    "\n",
    "# 使用to_sparse_tensor函数将稀疏矩阵转换为PyTorch的稀疏张量\n",
    "X_train = to_sparse_tensor(training_matrix_with_bias)\n",
    "X_dev = to_sparse_tensor(dev_matrix_with_bias)\n",
    "\n",
    "# 接下来，将标签也转换为张量\n",
    "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "y_dev = torch.tensor(dev_labels, dtype=torch.float32).unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:23.415405300Z",
     "start_time": "2024-01-30T05:06:23.277357100Z"
    }
   },
   "id": "b8b7e268a2d839bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 任务1：1000步训练并每20步报告损失"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a703076f0c461377"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 0.6903110146522522\n",
      "Step 20, Loss: 0.38507160544395447\n",
      "Step 40, Loss: 0.33181700110435486\n",
      "Step 60, Loss: 0.3208434581756592\n",
      "Step 80, Loss: 0.4340411424636841\n",
      "Step 100, Loss: 0.3424816429615021\n",
      "Step 120, Loss: 0.23766645789146423\n",
      "Step 140, Loss: 0.42486169934272766\n",
      "Step 160, Loss: 0.170060932636261\n",
      "Step 180, Loss: 0.4934781789779663\n",
      "Step 200, Loss: 0.1974051594734192\n",
      "Step 220, Loss: 0.1549217700958252\n",
      "Step 240, Loss: 0.3302558362483978\n",
      "Step 260, Loss: 0.2734345495700836\n",
      "Step 280, Loss: 0.24036286771297455\n",
      "Step 300, Loss: 0.2562035918235779\n",
      "Step 320, Loss: 0.40244612097740173\n",
      "Step 340, Loss: 0.19411596655845642\n",
      "Step 360, Loss: 0.32827919721603394\n",
      "Step 380, Loss: 0.3956741690635681\n",
      "Step 400, Loss: 0.2102421671152115\n",
      "Step 420, Loss: 0.2836611270904541\n",
      "Step 440, Loss: 0.21319742500782013\n",
      "Step 460, Loss: 0.2656530737876892\n",
      "Step 480, Loss: 0.2881655693054199\n",
      "Step 500, Loss: 0.3474210500717163\n",
      "Step 520, Loss: 0.2889809310436249\n",
      "Step 540, Loss: 0.22974801063537598\n",
      "Step 560, Loss: 0.29008597135543823\n",
      "Step 580, Loss: 0.2924955189228058\n",
      "Step 600, Loss: 0.19538141787052155\n",
      "Step 620, Loss: 0.3827013671398163\n",
      "Step 640, Loss: 0.26528140902519226\n",
      "Step 660, Loss: 0.3229060769081116\n",
      "Step 680, Loss: 0.24193687736988068\n",
      "Step 700, Loss: 0.4362980127334595\n",
      "Step 720, Loss: 0.2816676199436188\n",
      "Step 740, Loss: 0.2677531838417053\n",
      "Step 760, Loss: 0.21236805617809296\n",
      "Step 780, Loss: 0.331378310918808\n",
      "Step 800, Loss: 0.30471348762512207\n",
      "Step 820, Loss: 0.21795034408569336\n",
      "Step 840, Loss: 0.19115078449249268\n",
      "Step 860, Loss: 0.19793057441711426\n",
      "Step 880, Loss: 0.32477614283561707\n",
      "Step 900, Loss: 0.3542032241821289\n",
      "Step 920, Loss: 0.22828680276870728\n",
      "Step 940, Loss: 0.3232184946537018\n",
      "Step 960, Loss: 0.29334598779678345\n",
      "Step 980, Loss: 0.2915489077568054\n"
     ]
    }
   ],
   "source": [
    "# 创建TensorDataset和DataLoader以进行批处理\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "batch_size = 64  # 或者您选择的其他批次大小\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "model = LogisticRegressionPyTorch(X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "def train_model_1000_steps(model, dataloader, criterion, optimizer, total_steps=1000, report_interval=20):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    while step < total_steps:\n",
    "        for inputs, labels in dataloader:\n",
    "            if step >= total_steps:\n",
    "                break\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % report_interval == 0:\n",
    "                print(f'Step {step}, Loss: {loss.item()}')\n",
    "            step += 1\n",
    "\n",
    "# 调用函数训练模型\n",
    "train_model_1000_steps(model, dataloader, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:42.745597500Z",
     "start_time": "2024-01-30T05:06:23.361403300Z"
    }
   },
   "id": "129fba3539a1ae22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 任务2：至少1个epoch的训练并计算损失和F1分数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f7b788e042f08ad"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 0.23298797011375427, F1 Score: 0.0404\n",
      "Step 50, Loss: 0.30274245142936707, F1 Score: 0.0600\n",
      "Step 100, Loss: 0.2189456671476364, F1 Score: 0.0600\n"
     ]
    }
   ],
   "source": [
    "def train_model_1_epoch(model, dataloader, criterion, optimizer, evaluation_interval=50):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for epoch in range(1):  # 1个epoch\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % evaluation_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    dev_outputs = model(X_dev).squeeze()\n",
    "                    dev_predictions = (dev_outputs >= 0.5).float()\n",
    "                    f1 = f1_score(y_dev.numpy(), dev_predictions.numpy())\n",
    "                    print(f'Step {step}, Loss: {loss.item()}, F1 Score: {f1:.4f}')\n",
    "                model.train()\n",
    "            step += 1\n",
    "\n",
    "# 调用函数训练模型\n",
    "train_model_1_epoch(model, dataloader, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:44.846592300Z",
     "start_time": "2024-01-30T05:06:42.750694400Z"
    }
   },
   "id": "bcf2b2a5650e7c30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 任务3：添加正则化(L2惩罚)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa42ddb0d351a652"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with L2 penalty = 0\n",
      "Step 0, Loss: 0.7011913061141968, F1 Score: 0.0336\n",
      "Step 50, Loss: 0.32451537251472473, F1 Score: 0.0000\n",
      "Step 100, Loss: 0.40147462487220764, F1 Score: 0.0000\n",
      "Training with L2 penalty = 0.001\n",
      "Step 0, Loss: 0.6821038722991943, F1 Score: 0.0600\n",
      "Step 50, Loss: 0.31333431601524353, F1 Score: 0.0000\n",
      "Step 100, Loss: 0.3402955234050751, F1 Score: 0.0000\n",
      "Training with L2 penalty = 0.1\n",
      "Step 0, Loss: 0.692952036857605, F1 Score: 0.0345\n",
      "Step 50, Loss: 0.4544515907764435, F1 Score: 0.0000\n",
      "Step 100, Loss: 0.2909027636051178, F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "l2_values = [0, 0.001, 0.1]\n",
    "for l2 in l2_values:\n",
    "    model = LogisticRegressionPyTorch(X_train.shape[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=l2)\n",
    "    print(f\"Training with L2 penalty = {l2}\")\n",
    "    train_model_1_epoch(model, dataloader, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:51.302162700Z",
     "start_time": "2024-01-30T05:06:44.846592300Z"
    }
   },
   "id": "898b592383591a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 任务4：使用不同的优化器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3bbc354ed5685aa"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with optimizer RMSprop\n",
      "Step 0, Loss: 0.7103024125099182, F1 Score: 0.1521\n",
      "Step 50, Loss: 0.7111021876335144, F1 Score: 0.1521\n",
      "Step 100, Loss: 0.708824872970581, F1 Score: 0.1521\n",
      "Training with optimizer AdamW\n",
      "Step 0, Loss: 0.6967037916183472, F1 Score: 0.1372\n",
      "Step 50, Loss: 0.6896767020225525, F1 Score: 0.1372\n",
      "Step 100, Loss: 0.687341570854187, F1 Score: 0.1372\n"
     ]
    }
   ],
   "source": [
    "optimizers = {\n",
    "    'RMSprop': optim.RMSprop(model.parameters(), lr=0.01),\n",
    "    'AdamW': optim.AdamW(model.parameters(), lr=0.01)\n",
    "}\n",
    "\n",
    "for opt_name, optimizer in optimizers.items():\n",
    "    model = LogisticRegressionPyTorch(X_train.shape[1])\n",
    "    print(f\"Training with optimizer {opt_name}\")\n",
    "    train_model_1_epoch(model, dataloader, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:06:55.171281600Z",
     "start_time": "2024-01-30T05:06:51.293161100Z"
    }
   },
   "id": "b7b99fc75d652c8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 任务5：不同的分词方法对比"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caeef8f7d34c4a70"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document list for training with simple_tokenize...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7328/7328 [00:00<00:00, 18719.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing compressed sparse row (CSR) matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7328/7328 [00:00<00:00, 174289.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss: 0.6953454613685608, F1 Score: 0.1372\n",
      "Epoch 0, Step 50, Loss: 0.6914268732070923, F1 Score: 0.1372\n",
      "Epoch 0, Step 100, Loss: 0.6905796527862549, F1 Score: 0.1372\n",
      "Epoch 0, Step 0, Loss: 0.7059181928634644, F1 Score: 0.1242\n",
      "Epoch 0, Step 50, Loss: 0.33203646540641785, F1 Score: 0.0000\n",
      "Epoch 0, Step 100, Loss: 0.31209224462509155, F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenize(input_str):\n",
    "    # Convert to lowercase and split into words based on whitespace\n",
    "    return input_str.lower().split()\n",
    "\n",
    "# Prepare the document list using simple_tokenize\n",
    "simple_document_list = []\n",
    "print(\"Creating document list for training with simple_tokenize...\")\n",
    "for idx, record in tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "    simple_document_list.append(simple_tokenize(record[\"text\"]))\n",
    "\n",
    "# Generate the CSR matrix using simple_tokenize\n",
    "simple_training_matrix, simple_training_vocab = createCompressedSparseRowMatrix(simple_document_list, vocab=training_vocab, frequencyFilter=term_dictionary)\n",
    "\n",
    "# Add a bias column to the matrix\n",
    "simple_bias_column = np.ones((simple_training_matrix.shape[0], 1))\n",
    "simple_training_matrix_with_bias = sparse.hstack([simple_training_matrix, simple_bias_column]).tocsr()\n",
    "\n",
    "# Convert the new matrix to a PyTorch sparse tensor\n",
    "simple_X_train = to_sparse_tensor(simple_training_matrix_with_bias)\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, batch_size, model, criterion, optimizer, num_epochs=1, evaluation_interval=50):\n",
    "    # 创建TensorDataset和DataLoader以进行批处理\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 存储评估结果\n",
    "    f1_scores = []\n",
    "\n",
    "    # 训练模型\n",
    "    step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 每隔一定步数进行评估\n",
    "            if step % evaluation_interval == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    dev_outputs = model(X_dev).squeeze()\n",
    "                    dev_predictions = (dev_outputs >= 0.5).float()\n",
    "                    f1 = f1_score(y_dev.numpy(), dev_predictions.numpy())\n",
    "                    f1_scores.append(f1)\n",
    "                    print(f'Epoch {epoch}, Step {step}, Loss: {loss.item()}, F1 Score: {f1:.4f}')\n",
    "                model.train()\n",
    "            step += 1\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "# 使用better_tokenize的训练数据\n",
    "f1_scores_better = train_and_evaluate(X_train, y_train, batch_size, model, criterion, optimizer)\n",
    "\n",
    "# 使用simple_tokenize的训练数据\n",
    "model_simple = LogisticRegressionPyTorch(simple_X_train.shape[1])  # 假设simple_X_train是简单分词的结果\n",
    "optimizer_simple = optim.SGD(model_simple.parameters(), lr=0.1)  # 使用相同的学习率\n",
    "f1_scores_simple = train_and_evaluate(simple_X_train, y_train, batch_size, model_simple, criterion, optimizer_simple)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:07:59.632136600Z",
     "start_time": "2024-01-30T05:07:54.731655200Z"
    }
   },
   "id": "ebca476578737105"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 任务6：学习率的影响"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb72f3170ed7a3f6"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate 0.001\n",
      "Step 0, Loss: 0.6954245567321777, F1 Score: 0.1088\n",
      "Step 50, Loss: 0.6852775812149048, F1 Score: 0.0912\n",
      "Step 100, Loss: 0.6572492718696594, F1 Score: 0.0000\n",
      "Training with learning rate 0.1\n",
      "Step 0, Loss: 0.6867693066596985, F1 Score: 0.0200\n",
      "Step 50, Loss: 0.3838656544685364, F1 Score: 0.0000\n",
      "Step 100, Loss: 0.18682701885700226, F1 Score: 0.0000\n",
      "Training with learning rate 1\n",
      "Step 0, Loss: 0.6797922849655151, F1 Score: 0.0000\n",
      "Step 50, Loss: 0.25589367747306824, F1 Score: 0.0000\n",
      "Step 100, Loss: 0.2588452100753784, F1 Score: 0.0777\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.1, 1]\n",
    "for lr in learning_rates:\n",
    "    model = LogisticRegressionPyTorch(X_train.shape[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    print(f\"Training with learning rate {lr}\")\n",
    "    train_model_1_epoch(model, dataloader, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:09:18.823993100Z",
     "start_time": "2024-01-30T05:09:12.300160300Z"
    }
   },
   "id": "6cfc3b93f008543c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
