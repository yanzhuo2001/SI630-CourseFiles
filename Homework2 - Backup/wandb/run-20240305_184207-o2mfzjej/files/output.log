
Step 500, Avg Loss: 0.6433961237370968
Step 1000, Avg Loss: 0.5296456232219935
Step 1500, Avg Loss: 0.4497604690399021
Step 2000, Avg Loss: 0.44424202137067914
Step 2500, Avg Loss: 0.3966269247396849
Step 3000, Avg Loss: 0.3663795781407971
Step 3500, Avg Loss: 0.3616273194097448
Step 4000, Avg Loss: 0.4268269411008805
Step 4500, Avg Loss: 0.33344488705822733
Step 5000, Avg Loss: 0.3815228582355194
-----Step 5000, F1 Score: 0.8301924221384646-----
Step 5500, Avg Loss: 0.44950330850406317
Step 6000, Avg Loss: 0.3680499059354188
Step 6500, Avg Loss: 0.3568153776796826
Step 7000, Avg Loss: 0.3891559804998396
Step 7500, Avg Loss: 0.33280476552521576
Step 8000, Avg Loss: 0.35958074172533816
Step 8500, Avg Loss: 0.3470920455358573
Step 9000, Avg Loss: 0.3409939298251411
Step 9500, Avg Loss: 0.36490991805749945
Step 10000, Avg Loss: 0.3904758805107558
-----Step 10000, F1 Score: 0.8407803018905772-----
Step 10500, Avg Loss: 0.35199558012280613
Step 11000, Avg Loss: 0.3540826144638704
Step 11500, Avg Loss: 0.36232920785882744
Step 12000, Avg Loss: 0.31391427797204235
Step 12500, Avg Loss: 0.40988346095278394
Step 13000, Avg Loss: 0.4205520646509831
Step 13500, Avg Loss: 0.3893837888800772
Step 14000, Avg Loss: 0.37694494383310667
Step 14500, Avg Loss: 0.42129804141283966
Step 15000, Avg Loss: 0.33381922825158106
-----Step 15000, F1 Score: 0.8471857828820355-----
Step 15500, Avg Loss: 0.3947018239139579
Step 16000, Avg Loss: 0.37690855803899465
Step 16500, Avg Loss: 0.40215764088672584
Step 17000, Avg Loss: 0.34805206893221474
Step 17500, Avg Loss: 0.3524613924203441
Step 18000, Avg Loss: 0.335800572532462
Step 18500, Avg Loss: 0.3218236326773185
Step 19000, Avg Loss: 0.3127971113054664
Step 19500, Avg Loss: 0.3581829887560452
Step 20000, Avg Loss: 0.2926484600921103
-----Step 20000, F1 Score: 0.8420116175624052-----
Step 20500, Avg Loss: 0.3458974934149301
Step 21000, Avg Loss: 0.34307404592284
Step 21500, Avg Loss: 0.3725346214571619
Step 22000, Avg Loss: 0.3268915114440024
Step 22500, Avg Loss: 0.30894159946212313
Step 23000, Avg Loss: 0.35046530171833
Step 23500, Avg Loss: 0.40767586789140475
Step 24000, Avg Loss: 0.3773068164724391
Step 24500, Avg Loss: 0.36340163321141156
Step 25000, Avg Loss: 0.34767145283112766
-----Step 25000, F1 Score: 0.8555739138886189-----
Step 25500, Avg Loss: 0.3240000560823828
Step 26000, Avg Loss: 0.3819984440971166
Step 26500, Avg Loss: 0.3265131610436365
Step 27000, Avg Loss: 0.3084043309878325
Step 27500, Avg Loss: 0.32761026997942827
Step 28000, Avg Loss: 0.3042038095181342
Step 28500, Avg Loss: 0.3620942802489735
Step 29000, Avg Loss: 0.29785474894789515
Step 29500, Avg Loss: 0.3522312195999548
Step 30000, Avg Loss: 0.3347787070815684
-----Step 30000, F1 Score: 0.8541499599037691-----
Step 30500, Avg Loss: 0.311839581467153
Step 31000, Avg Loss: 0.30045557681680657
Step 31500, Avg Loss: 0.33678317682887426
Step 32000, Avg Loss: 0.3657228429901879
Step 32500, Avg Loss: 0.32714413158073147
Step 33000, Avg Loss: 0.33767475881031717
Step 33500, Avg Loss: 0.2992401293298462
Step 34000, Avg Loss: 0.3338223926244536
Step 34500, Avg Loss: 0.3491800188935886
Step 35000, Avg Loss: 0.3847005850322312
-----Step 35000, F1 Score: 0.859147919569978-----
Step 35500, Avg Loss: 0.3444779592882842
Step 36000, Avg Loss: 0.35775739678367974
Step 36500, Avg Loss: 0.33399805420218037
Step 37000, Avg Loss: 0.3205440923575079
Step 37500, Avg Loss: 0.33023979583862817
Step 38000, Avg Loss: 0.3283105435351608
Step 38500, Avg Loss: 0.30252387128415287
Step 39000, Avg Loss: 0.3470855384321185
Step 39500, Avg Loss: 0.3640208348920569
Step 40000, Avg Loss: 0.30921242075128247
-----Step 40000, F1 Score: 0.862329206333547-----
Step 40500, Avg Loss: 0.31224076457187766
Step 41000, Avg Loss: 0.3184725773120881
Step 41500, Avg Loss: 0.339454962076823
Step 42000, Avg Loss: 0.3478926814851584
Step 42500, Avg Loss: 0.30849771746085025
Step 43000, Avg Loss: 0.284185512324344
Step 43500, Avg Loss: 0.3204492228105082
Step 44000, Avg Loss: 0.36594553289528997
Step 44500, Avg Loss: 0.3385779985652771
Step 45000, Avg Loss: 0.3309523199755931
-----Step 45000, F1 Score: 0.8622838761560112-----
Step 45500, Avg Loss: 0.3692741327516269
Step 46000, Avg Loss: 0.3152880358088005
Step 46500, Avg Loss: 0.28959527038440136
Step 47000, Avg Loss: 0.343410518879391
Step 47500, Avg Loss: 0.33124070078943624
Step 48000, Avg Loss: 0.2912083362807898
Step 48500, Avg Loss: 0.32350604106705577
Step 49000, Avg Loss: 0.3076307919030078
Step 49500, Avg Loss: 0.31525240021206263
Step 50000, Avg Loss: 0.309803473594191
-----Step 50000, F1 Score: 0.8679207822504591-----
Step 50500, Avg Loss: 0.37136655681033154
Step 51000, Avg Loss: 0.3283958870997594
Step 51500, Avg Loss: 0.326444477908517
Step 52000, Avg Loss: 0.3300235342980013
Step 52500, Avg Loss: 0.3365361043455705
Step 53000, Avg Loss: 0.30462867857208764
Step 53500, Avg Loss: 0.32895426749754736
Step 54000, Avg Loss: 0.3292024801245643
Step 54500, Avg Loss: 0.2728588886265343
Step 55000, Avg Loss: 0.3705423542241697
-----Step 55000, F1 Score: 0.8688395061728396-----
Step 55500, Avg Loss: 0.3287290684109321
Step 56000, Avg Loss: 0.33027222274705126
Step 56500, Avg Loss: 0.3457256307185162
Step 57000, Avg Loss: 0.32910430707465277
Step 57500, Avg Loss: 0.32742907689329875
Step 58000, Avg Loss: 0.295194008500599
Step 58500, Avg Loss: 0.30204797320612126
Step 59000, Avg Loss: 0.294147542309498
Step 59500, Avg Loss: 0.31012015734561466
Step 60000, Avg Loss: 0.29111452102787233
-----Step 60000, F1 Score: 0.8707773203035865-----
Step 60500, Avg Loss: 0.352682490938867
Step 61000, Avg Loss: 0.28983986532335987
Step 61500, Avg Loss: 0.31669693984827607
Step 62000, Avg Loss: 0.3199831437782705
Step 62500, Avg Loss: 0.3305750441934652
Step 63000, Avg Loss: 0.33072057962087276
Step 63500, Avg Loss: 0.3382113361698757
Step 64000, Avg Loss: 0.32886235592817686
Step 64500, Avg Loss: 0.31512790436511934
Step 65000, Avg Loss: 0.3297518400037661
-----Step 65000, F1 Score: 0.8698008128041744-----
Step 65500, Avg Loss: 0.29084277522833146
Step 66000, Avg Loss: 0.2797474106686277
Step 66500, Avg Loss: 0.3043010239469477
Step 67000, Avg Loss: 0.3080440267233425
Step 67500, Avg Loss: 0.32584843547999387
Step 68000, Avg Loss: 0.30473864952255464
Step 68500, Avg Loss: 0.31339469244427165
Step 69000, Avg Loss: 0.27998830897698646
Step 69500, Avg Loss: 0.30487453644561174
Step 70000, Avg Loss: 0.3031066264916808
-----Step 70000, F1 Score: 0.8674969375255206-----
Step 70500, Avg Loss: 0.30581635326387185
Step 71000, Avg Loss: 0.37668357753720194
Step 71500, Avg Loss: 0.3184644475575988
Step 72000, Avg Loss: 0.3055958188180357
Step 72500, Avg Loss: 0.2966681898771203
Step 73000, Avg Loss: 0.3315118773245631
Step 73500, Avg Loss: 0.28954620416936816
Step 74000, Avg Loss: 0.3017355980610446
Step 74500, Avg Loss: 0.2964206783861082
Step 75000, Avg Loss: 0.3122449215814158
-----Step 75000, F1 Score: 0.8686172674448501-----
Step 75500, Avg Loss: 0.2901165993348113
Step 76000, Avg Loss: 0.28845116349754246
Step 76500, Avg Loss: 0.2864099882121736
Step 77000, Avg Loss: 0.3138742964676494
Step 77500, Avg Loss: 0.299740240786472
Step 78000, Avg Loss: 0.33441168833573104
Step 78500, Avg Loss: 0.3130418705537595
Step 79000, Avg Loss: 0.3398526485936309
Step 79500, Avg Loss: 0.27359853153124275
Step 80000, Avg Loss: 0.32152870748089846
-----Step 80000, F1 Score: 0.871659652147457-----
Step 80500, Avg Loss: 0.29785656002881294
Step 81000, Avg Loss: 0.35855169465218206
Step 81500, Avg Loss: 0.34072428623536505
Step 82000, Avg Loss: 0.29087022690533193
Step 82500, Avg Loss: 0.28140395562645426
Step 83000, Avg Loss: 0.36521304480821526
Step 83500, Avg Loss: 0.2908391087855164
Step 84000, Avg Loss: 0.29755578120570864
Step 84500, Avg Loss: 0.3373704153031122
Step 85000, Avg Loss: 0.33650037024288215
-----Step 85000, F1 Score: 0.8731234866828087-----
Step 85500, Avg Loss: 0.30080713643083984
Step 86000, Avg Loss: 0.3188961707149865
Step 86500, Avg Loss: 0.31937505590062937
Step 87000, Avg Loss: 0.3629702550312795
Step 87500, Avg Loss: 0.28396349446200475
Step 88000, Avg Loss: 0.2846133240005365
Step 88500, Avg Loss: 0.338583836314021
Step 89000, Avg Loss: 0.30261192578918056
Step 89500, Avg Loss: 0.31229120972419694
Step 90000, Avg Loss: 0.3351963034274813
-----Step 90000, F1 Score: 0.8719628994858353-----
Step 90500, Avg Loss: 0.3300475799769033
Step 91000, Avg Loss: 0.2861098295019328
Step 91500, Avg Loss: 0.2935606746208373
Step 92000, Avg Loss: 0.31504812285402295
Step 92500, Avg Loss: 0.34505848804768174
Step 93000, Avg Loss: 0.2936763083150072
Step 93500, Avg Loss: 0.2597495618658695
Step 94000, Avg Loss: 0.2868822638357433
Step 94500, Avg Loss: 0.2752139997094332
Step 95000, Avg Loss: 0.32190284153602716
-----Step 95000, F1 Score: 0.8728635682158921-----
Step 95500, Avg Loss: 0.31796812668407254
Step 96000, Avg Loss: 0.3071474831795213
Step 96500, Avg Loss: 0.27899227077723016
Step 97000, Avg Loss: 0.3328315599704656
Step 97500, Avg Loss: 0.3031600121097581
Step 98000, Avg Loss: 0.28354645465782957
Step 98500, Avg Loss: 0.3242128622190794
Step 99000, Avg Loss: 0.3589768742083252
Step 99500, Avg Loss: 0.31707139713328797
Step 100000, Avg Loss: 0.31431269636422804
-----Step 100000, F1 Score: 0.8711574855506112-----
Step 100500, Avg Loss: 0.26939441185861507
Step 101000, Avg Loss: 0.3073698430402801
Step 101500, Avg Loss: 0.32775714014366897
Step 102000, Avg Loss: 0.3341453454798502
Step 102500, Avg Loss: 0.34831864621774367
Step 103000, Avg Loss: 0.32738602363425523
Step 103500, Avg Loss: 0.275698287789381
Step 104000, Avg Loss: 0.33451682518169035
Step 104500, Avg Loss: 0.3091708961143377
Step 105000, Avg Loss: 0.34989187510262854
-----Step 105000, F1 Score: 0.8751566180524232-----
Step 105500, Avg Loss: 0.3150535866807477
Step 106000, Avg Loss: 0.3168931862422469
Step 106500, Avg Loss: 0.2574393847903012
Step 107000, Avg Loss: 0.3140995478554978
Step 107500, Avg Loss: 0.3103024866345481
Step 108000, Avg Loss: 0.33719903686942804
Step 108500, Avg Loss: 0.3675907093049173
Step 109000, Avg Loss: 0.286247494750427
Step 109500, Avg Loss: 0.2533572790279541
Step 110000, Avg Loss: 0.29672564065594814
-----Step 110000, F1 Score: 0.8729016786570744-----
Step 110500, Avg Loss: 0.30940229496298427
Step 111000, Avg Loss: 0.32241802261798874
Step 111500, Avg Loss: 0.3362482928952895
Step 112000, Avg Loss: 0.317782219213349
Step 112500, Avg Loss: 0.3198565517731229
Step 113000, Avg Loss: 0.316638845706022
Step 113500, Avg Loss: 0.31457893058133596
Step 114000, Avg Loss: 0.2621934863922
Step 114500, Avg Loss: 0.33737445252840736
Step 115000, Avg Loss: 0.26556243088268344
-----Step 115000, F1 Score: 0.8744264074604036-----
Step 115500, Avg Loss: 0.279010372832679
Step 116000, Avg Loss: 0.3099856591768023
Step 116500, Avg Loss: 0.2694257975542423
Step 117000, Avg Loss: 0.32190324131729087
Step 117500, Avg Loss: 0.2962659562322224
Step 118000, Avg Loss: 0.28409391948920165
Step 118500, Avg Loss: 0.32682528251313486
Step 119000, Avg Loss: 0.30382467044342776
Step 119500, Avg Loss: 0.34128229374981855
Step 120000, Avg Loss: 0.32730171343075837
-----Step 120000, F1 Score: 0.8740253164556963-----
Step 120500, Avg Loss: 0.2916840344154705
Step 121000, Avg Loss: 0.34376362814775346
Step 121500, Avg Loss: 0.35980409469396546
Step 122000, Avg Loss: 0.3420199951646355
Step 122500, Avg Loss: 0.3127437319741002
Step 123000, Avg Loss: 0.3431442993490491
Step 123500, Avg Loss: 0.30218137864538197
Step 124000, Avg Loss: 0.257864372219592
Step 124500, Avg Loss: 0.3512807477749443
Step 125000, Avg Loss: 0.29910249603143896
-----Step 125000, F1 Score: 0.8742169146436961-----
Step 125500, Avg Loss: 0.3121245249581116
Step 126000, Avg Loss: 0.3684065022432187
Step 126500, Avg Loss: 0.2856977479990164
Step 127000, Avg Loss: 0.29008267841999624
Step 127500, Avg Loss: 0.27704140201333577
Step 128000, Avg Loss: 0.2745022008718343
Step 128500, Avg Loss: 0.32723619757403866
Step 129000, Avg Loss: 0.26088941990913006
Step 129500, Avg Loss: 0.31506645062196187
Step 130000, Avg Loss: 0.33440743896453934
-----Step 130000, F1 Score: 0.8715676678172171-----
Step 130500, Avg Loss: 0.2805338883352579
Step 131000, Avg Loss: 0.28764493103936434
Step 131500, Avg Loss: 0.3054340914461518
Step 132000, Avg Loss: 0.2676961004876066
Step 132500, Avg Loss: 0.29416944433941533
Step 133000, Avg Loss: 0.290718635128218
Step 133500, Avg Loss: 0.33104938800229183
Step 134000, Avg Loss: 0.3485420618643984
Step 134500, Avg Loss: 0.2946961346194039
Step 135000, Avg Loss: 0.2901767185060946
-----Step 135000, F1 Score: 0.8734559991799498-----
Step 135500, Avg Loss: 0.3176825954508408
Step 136000, Avg Loss: 0.28942936729809343
Step 136500, Avg Loss: 0.36613046298174595
Step 137000, Avg Loss: 0.29627680650243565
Step 137500, Avg Loss: 0.32676331540709364
Step 138000, Avg Loss: 0.31843652646284315
Step 138500, Avg Loss: 0.3404495548492851
Step 139000, Avg Loss: 0.33358239905629306
Step 139500, Avg Loss: 0.34502076167598716
Step 140000, Avg Loss: 0.32238183091013345
-----Step 140000, F1 Score: 0.8762846236664383-----
Step 140500, Avg Loss: 0.27786456711518986
Step 141000, Avg Loss: 0.3231304585291946
Step 141500, Avg Loss: 0.3555154223286081
Step 142000, Avg Loss: 0.3054078288426972
Step 142500, Avg Loss: 0.3003501205535686
Step 143000, Avg Loss: 0.265601075012637
Step 143500, Avg Loss: 0.3289166416063672
Step 144000, Avg Loss: 0.33612311486073304
Step 144500, Avg Loss: 0.3161069791436021
Step 145000, Avg Loss: 0.27147881249607597
-----Step 145000, F1 Score: 0.8743602609131962-----
Step 145500, Avg Loss: 0.2513696567972715
Step 146000, Avg Loss: 0.28638576592077153
Step 146500, Avg Loss: 0.3370408260856202
Step 147000, Avg Loss: 0.263060449887229
Step 147500, Avg Loss: 0.27702374482354936
Step 148000, Avg Loss: 0.3120105684104783
Step 148500, Avg Loss: 0.3230418394816879
Step 149000, Avg Loss: 0.2922810088494075
Step 149500, Avg Loss: 0.3151362984437146
Step 150000, Avg Loss: 0.3275471025846446
-----Step 150000, F1 Score: 0.8699296357615894-----
Step 150500, Avg Loss: 0.3651982665494434
Step 151000, Avg Loss: 0.3204694542615398
Step 151500, Avg Loss: 0.2897008845181845
Step 152000, Avg Loss: 0.3721118501915189
Step 152500, Avg Loss: 0.27203594787763813
Step 153000, Avg Loss: 0.32334531084560875
Step 153500, Avg Loss: 0.29827840261538224
Step 154000, Avg Loss: 0.2899536459712508
Step 154500, Avg Loss: 0.3223594165377799
Step 155000, Avg Loss: 0.3662287474251043
-----Step 155000, F1 Score: 0.8605505564133965-----
Step 155500, Avg Loss: 0.3497011150041071
Step 156000, Avg Loss: 0.2947136678955867
Step 156500, Avg Loss: 0.3246171646320763
Step 157000, Avg Loss: 0.3671342073599808
Step 157500, Avg Loss: 0.29084961584689883
Step 158000, Avg Loss: 0.2956073942045332
Step 158500, Avg Loss: 0.27649621321514134
Step 159000, Avg Loss: 0.32732308001594357
Step 159500, Avg Loss: 0.3005950160864813
Step 160000, Avg Loss: 0.29640880036385897
-----Step 160000, F1 Score: 0.8769215608986992-----
   inst_id                                               text  predicted_label
0        0  Really sad review as I absolutely loved the fi...              0.0
1        1  Excellent content, perfect for Christians who ...              1.0
2        2  This is an okay book if you need advice on bud...              0.0
3        3  This is one book you can't put down! This book...              1.0
4        4  There were to many names that I had no idenity...              0.0
   inst_id                                               text  predicted_label
0        0  Really sad review as I absolutely loved the fi...              0.0
1        1  Excellent content, perfect for Christians who ...              1.0
2        2  This is an okay book if you need advice on bud...              0.0
3        3  This is one book you can't put down! This book...              1.0
4        4  There were to many names that I had no idenity...              0.0
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="RegexpTokenizer" type="ABCMeta" qualifier="abc" value="%3Cclass %27nltk.tokenize.regexp.RegexpTokenizer%27&gt;" isContainer="True" />
<var name="avg_loss" type="float" qualifier="builtins" value="0.29640880036385897" />
<var name="batch_size" type="int" qualifier="builtins" value="1" />
<var name="best_f1" type="float64" qualifier="numpy" value="0.87783708285529" shape="()" />
<var name="dev_data_path" type="str" qualifier="builtins" value="sentiment.dev.csv" />
<var name="dev_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x000001393A311310&gt;" isContainer="True" shape="20000" />
<var name="dev_f1" type="float64" qualifier="numpy" value="0.8769215608986992" shape="()" />
<var name="dev_list" type="list" qualifier="builtins" value="%5B%28%5B  96   96  193    4 7409   62   71   27  234   64  329   11   96    1%2C  626  314 6874 1798 1887 1015  177   96  104    4  105  106 4055   11%2C  264   11    1 1682  989   13 6360%5D%2C array%280%29%29%2C %28%5B   96   288     7  1155    71     3  1052   223   281    83 15441  2086%2C   268   604   140     7  1277    24  9765  1467    96 17953    98   948%2C 11506  2236   724   397     7    70    96     7 15441   778  2086   268%2C     7  3507    58    77  6360%5D%2C array%280%29%29%2C %28%5B  96    1   18 1805   27    0   21   96   15   88 4956   61    0   36%2C   61   96   13  104  190 4475   98   40   83 1700   61   96 1590   96%2C   96  550  200   81 1890    7   21  307   96 1039   96  769  200  172%2C   11  668 2260   96   96 1583   47   33 3175   15 3365  268    7  117%2C   24   33 4959%5D%2C array%281%29%29%2C %28%5B  96  193    4   18 1282   13  739 3771  859   96 3501 4825   77  128%2C 1230  881   96  128 3429  200  716 1742%5D%2C array%280%29%29%2C %28%5B  96  124  310  288    0   21   96    1  310   64   33 4226    6   96%2C  407  408 1684  151   62   11 ..." isContainer="True" shape="20000" />
<var name="dev_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000001393A310A10&gt;" isContainer="True" shape="20000" />
<var name="device" type="device" qualifier="torch" value="device%28type=%27cpu%27%29" isContainer="True" />
<var name="embedding_size" type="int" qualifier="builtins" value="50" />
<var name="embeddings_file_path" type="str" qualifier="builtins" value="./models/min8_model_embeddings_state_dict.pt" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="1" />
<var name="f" type="BufferedReader" qualifier="_io" value="%3C_io.BufferedReader name=%27./models/min8_model_index_to_word.pkl%27&gt;" isContainer="True" />
<var name="f1_scores" type="list" qualifier="builtins" value="%5B0.761490923136346%2C 0.7840658533000147%2C 0.7809473124002129%2C 0.8145033636723388%2C 0.8178663493952013%2C 0.81693175987686%2C 0.818798103878893%2C 0.830536327872118%2C 0.8309065058587047%2C 0.8301924221384646%2C 0.8166427851191423%2C 0.8313647360182155%2C 0.8309484360094391%2C 0.8328888441604186%2C 0.836755497305956%2C 0.834162095657969%2C 0.8350798657035303%2C 0.8378378378378378%2C 0.8354547320878669%2C 0.8407803018905772%2C 0.8412208331224377%2C 0.8360842935741721%2C 0.8394326384331937%2C 0.8445810764651345%2C 0.8394111914653536%2C 0.8452060069310743%2C 0.8404368808737618%2C 0.8462062256809338%2C 0.8447697468740469%2C 0.8471857828820355%2C 0.8463494000406752%2C 0.8501038883941823%2C 0.850648375306664%2C 0.8510081426909655%2C 0.8497909615767469%2C 0.8504141303263147%2C 0.851115896632733%2C 0.8529226596929633%2C 0.8517444766570029%2C 0.8420116175624052%2C 0.85371350186165%2C 0.8525364138623807%2C 0.8437872892347601%2C 0.8453703703703703%2C 0.8475609756097561%2C 0.8395165528113505%2C 0.8505047430629534%2C 0.8509593707730073%2C 0.85300289797142%2C 0.8555739138886189%2C 0.8545097446..." isContainer="True" shape="320" />
<var name="index_to_word" type="dict" qualifier="builtins" value="%7B0%3A %27this%27%2C 1%3A %27was%27%2C 2%3A %27bought%27%2C 3%3A %27as%27%2C 4%3A %27a%27%2C 5%3A %27gift%27%2C 6%3A %27but%27%2C 7%3A %27the%27%2C 8%3A %27person%27%2C 9%3A %27who%27%2C 10%3A %27got%27%2C 11%3A %27it%27%2C 12%3A %27loved%27%2C 13%3A %27and%27%2C 14%3A %27they%27%2C 15%3A %27will%27%2C 16%3A %27use%27%2C 17%3A %27soon%27%2C 18%3A %27very%27%2C 19%3A %27well%27%2C 20%3A %27written%27%2C 21%3A %27book%27%2C 22%3A %27on%27%2C 23%3A %27period%27%2C 24%3A %27of%27%2C 25%3A %27world%27%2C 26%3A %27history%27%2C 27%3A %27with%27%2C 28%3A %27which%27%2C 29%3A %27i%27%2C 30%3A %27am%27%2C 31%3A %27familiar%27%2C 32%3A %27despite%27%2C 33%3A %27my%27%2C 34%3A %27familiarity%27%2C 35%3A %27subject%27%2C 36%3A %27area%27%2C 37%3A %27learned%27%2C 38%3A %27lot%27%2C 39%3A %27new%27%2C 40%3A %27information%27%2C 41%3A %27also%27%2C 42%3A %27one%27%2C 43%3A %27best%27%2C 44%3A %27concise%27%2C 45%3A %27descriptions%27%2C 46%3A %27wwii%27%2C 47%3A %27that%27%2C 48%3A %27have%27%2C 49%3A %27ever%27%2C 50%3A %27read%27%2C 51%3A %27thought%27%2C 52%3A %27provoking%27%2C 53%3A %27hot%27%2C 54%3A %27cross%27%2C 55%3A %27buns%27%2C 56%3A %27contains%27%2C 57%3A %27cast%27%2C 58%3A %27characters%27%2C 59%3A %27you%27%2C 60%3A %27fall%27%2C 61%3A %27in%27%2C 62%3A %27love%27%2C 63%3A %27want%27%2C 64%3A %27to%27%2C 65%3A %27hang%27%2C 66%3A %27out%27%2C 67%3A %27has%27%2C 68%3A %27few%27%2C 69%3A %27different%27%2C 70%3A %27plot%27%2C 71%3A %27story%27%2C 72%3A %27lines%27%2C 73%3A %27intersect%27%2C 74%3A %27unexpected%27%2C 75%3A %27ways%27%2C 76%3A %27stories%27%2C 77%3A %27are%27%2C 78%3A %27full%27%2C 79%3A %27h..." isContainer="True" shape="31512" />
<var name="index_to_word_file" type="str" qualifier="builtins" value="./models/min8_model_index_to_word.pkl" />
<var name="labels" type="Tensor" qualifier="torch" value="tensor%28%5B0.%5D%29" isContainer="True" shape="(1,)" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="loss" type="Tensor" qualifier="torch" value="tensor%280.0433%2C grad_fn=%3CBinaryCrossEntropyBackward0&gt;%29" isContainer="True" shape="()" />
<var name="loss_function" type="BCELoss" qualifier="torch.nn.modules.loss" value="BCELoss%28%29" isContainer="True" />
<var name="losses" type="list" qualifier="builtins" value="%5B0.6433961237370968%2C 0.5296456232219935%2C 0.4497604690399021%2C 0.44424202137067914%2C 0.3966269247396849%2C 0.3663795781407971%2C 0.3616273194097448%2C 0.4268269411008805%2C 0.33344488705822733%2C 0.3815228582355194%2C 0.44950330850406317%2C 0.3680499059354188%2C 0.3568153776796826%2C 0.3891559804998396%2C 0.33280476552521576%2C 0.35958074172533816%2C 0.3470920455358573%2C 0.3409939298251411%2C 0.36490991805749945%2C 0.3904758805107558%2C 0.35199558012280613%2C 0.3540826144638704%2C 0.36232920785882744%2C 0.31391427797204235%2C 0.40988346095278394%2C 0.4205520646509831%2C 0.3893837888800772%2C 0.37694494383310667%2C 0.42129804141283966%2C 0.33381922825158106%2C 0.3947018239139579%2C 0.37690855803899465%2C 0.40215764088672584%2C 0.34805206893221474%2C 0.3524613924203441%2C 0.335800572532462%2C 0.3218236326773185%2C 0.3127971113054664%2C 0.3581829887560452%2C 0.2926484600921103%2C 0.3458974934149301%2C 0.34307404592284%2C 0.3725346214571619%2C 0.3268915114440024%2C 0.30894159946212313%2C 0.35046530171833%2C 0.40767586789140475%2C 0.3773068164724391%2C 0.36340163321141156%2C 0.347..." isContainer="True" shape="320" />
<var name="max_steps" type="int" qualifier="builtins" value="1000000000" />
<var name="model" type="DocumentAttentionClassifier" qualifier="__main__" value="DocumentAttentionClassifier%28%0A  %28embeddings%29%3A Embedding%2831512%2C 50%29%0A  %28linear%29%3A Linear%28in_features=200%2C out_features=1%2C bias=True%29%0A%29" isContainer="True" />
<var name="num_heads" type="int" qualifier="builtins" value="4" />
<var name="optimizer" type="AdamW" qualifier="torch.optim.adamw" value="AdamW %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0.01%0A%29" isContainer="True" />
<var name="patience" type="int" qualifier="builtins" value="100000" />
<var name="prediction" type="float" qualifier="builtins" value="0.0" />
<var name="predictions" type="list" qualifier="builtins" value="%5B0.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C..." isContainer="True" shape="20000" />
<var name="probability" type="Tensor" qualifier="torch" value="tensor%28%5B0.4419%5D%29" isContainer="True" shape="(1,)" />
<var name="running_loss" type="float" qualifier="builtins" value="0.0" />
<var name="sent_dev_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0      Picturing Perfect is a sappy love story with l...      0%5D %5B1      Seems like the same story as any other series ...      0%5D %5B2      I was very pleased with this book. I will be t...      1%5D %5B3      It is a very light and rather silly novel. The...      0%5D %5B4      I did not like this book. It was not to my tas...      0%5D %5B...                                                  ...    ...%5D %5B19995  Great content%2C the story is fantastic%2C but sho...      1%5D %5B19996  Typical book club book... Incest%2C child abuse%2C...      0%5D %5B19997  Fascinating book. Shorter than most Russell Ba...      1%5D %5B19998  This book is not well-organized%2C which is impo...      0%5D %5B19999  CAN WE CALL THIS A CLASSIC OF THE GENRE%3F I THI...      1%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_test_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27text%27%2C %27predicted_label%27%5D %5B0            0  Really sad review as I absolutely loved the fi...   %5D %5B1            1  Excellent content%2C perfect for Christians who ...   %5D %5B2            2  This is an okay book if you need advice on bud...   %5D %5B3            3  This is one book you can%27t put down%21 This book...   %5D %5B4            4  There were to many names that I had no idenity...   %5D %5B...        ...                                                ...   %5D %5B19995    19995  I found this book to be a very entertaining an...   %5D %5B19996    19996  Wow%2C what a Middle School%21 Read this book your...   %5D %5B19997    19997  Not what I expected. Not enough about circular...   %5D %5B19998    19998  I like Joanne Fluke%27s mystery series starring ...   %5D %5B19999    19999  Grow some chickens%2C improve your land%2C make al...   %5D %5B%5D %5B0                  0.0  %5D %5B1                  1.0  %5D %5B2                  0.0  %5D %5B3                  1.0  %5D %5B4                  0.0  %5D %5B...                ...  %5D %5B19995         ..." isContainer="True" shape="(20000, 3)" />
<var name="sent_train_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0       It was what I needed. There was no markings or...      1%5D %5B1       A cute little book. My wife gets the family wa...      1%5D %5B2       I bought these for 40.00 and for the price I j...      0%5D %5B3       It was interesting and enjoyable reading. Shor...      1%5D %5B4       A perfect ending to an amazing story. This was...      1%5D %5B...                                                   ...    ...%5D %5B159995  After reading every book Stephen King has to o...      1%5D %5B159996  Baby boomers who are experiencing %22aging eyeba...      0%5D %5B159997  Must read%2C must have%2C must read again. This bo...      1%5D %5B159998  Dr. Chopra%27s books are always enlightening and...      1%5D %5B159999  Boooring%21%21 Just enough to keep you intrigued f...      0%5D %5B%5D" isContainer="True" shape="(160000, 2)" />
<var name="step" type="int" qualifier="builtins" value="159999" />
<var name="steps_since_improvement" type="int" qualifier="builtins" value="18500" />
<var name="test_data" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   882%2C   623%2C     3%2C    96%2C  1127%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  3992%2C     4%2C  6360...8268%2C  7397%2C     7%2C%0A         2072%2C   759%2C     7%2C   724%2C    42%2C   335%2C   384%2C   513%2C    98%2C   626%2C%0A          102%2C    13%2C   449%5D%29%2C tensor%28%5B  96%2C 1658%2C 1419%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2630%2C  314%2C  801%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1874%2C   61%2C    7%2C  690%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  4834%2C    21%2C   335%2C    59%2C   658%2C   630%2C    22%2C%0A         8006%2C    13%2C  3910%2C    96%2C   359%2C   345...  716%2C    96%2C   309%2C   496%2C    94%2C   716%2C  1100%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 18680%2C    64%2C     7%2C  2850%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  505%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 8709%2C   59%2C   ...7%2C    4%2C  765%2C   13%2C%0A        3532%2C  674%2C   86%2C  675%2C   64%2C  287%2C  193%2C  536%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  676%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_data_path" type="str" qualifier="builtins" value="sentiment.test.csv" />
<var name="test_list" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   882%2C   623%2C     3%2C    96%2C  1127%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  3992%2C     4%2C  6360...8268%2C  7397%2C     7%2C%0A         2072%2C   759%2C     7%2C   724%2C    42%2C   335%2C   384%2C   513%2C    98%2C   626%2C%0A          102%2C    13%2C   449%5D%29%2C tensor%28%5B  96%2C 1658%2C 1419%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2630%2C  314%2C  801%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1874%2C   61%2C    7%2C  690%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  4834%2C    21%2C   335%2C    59%2C   658%2C   630%2C    22%2C%0A         8006%2C    13%2C  3910%2C    96%2C   359%2C   345...  716%2C    96%2C   309%2C   496%2C    94%2C   716%2C  1100%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 18680%2C    64%2C     7%2C  2850%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  505%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 8709%2C   59%2C   ...7%2C    4%2C  765%2C   13%2C%0A        3532%2C  674%2C   86%2C  675%2C   64%2C  287%2C  193%2C  536%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  676%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_output" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27predicted_label%27%5D %5B0            0              0.0%5D %5B1            1              1.0%5D %5B2            2              0.0%5D %5B3            3              1.0%5D %5B4            4              0.0%5D %5B...        ...              ...%5D %5B19995    19995              1.0%5D %5B19996    19996              1.0%5D %5B19997    19997              0.0%5D %5B19998    19998              1.0%5D %5B19999    19999              0.0%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="tokenizer" type="RegexpTokenizer" qualifier="nltk.tokenize.regexp" value="RegexpTokenizer%28pattern=%27%5C%5Cw%2B%27%2C gaps=False%2C discard_empty=True%2C flags=re.UNICODE%7Cre.MULTILINE%7Cre.DOTALL%29" isContainer="True" />
<var name="train_data_path" type="str" qualifier="builtins" value="sentiment.train.csv" />
<var name="train_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x000001393E799C10&gt;" isContainer="True" shape="160000" />
<var name="train_list" type="list" qualifier="builtins" value="%5B%28%5B   96     1   287    96   248    96     1   204 15861   291   110    24%2C    47    96   193   306    64    50    13    93%5D%2C array%281%29%29%2C %28%5B  96  881  234   21   96 1185 2453    7  690 1918 1913  268   96   96%2C   85 1677   96   21   67    7 1155  881 1567%5D%2C array%281%29%29%2C %28%5B  96    2  320   98 3525 6863   13   98    7  365   96  128 5628   14%2C  151   88  105 9355   96   88   96  123   59 1943  320   59  175   11%2C   15  128   88   96  140  105 9355 5715%5D%2C array%280%29%29%2C %28%5B  96    1  357   13  333  341   96   76 1762   83   96  241 1624 3035%2C   24   96   13 1190  726   61  305   95   96   76 1418   48    4  696%2C  117   96  115    4  485   24   96   96  356   76   96   62%5D%2C array%281%29%29%2C %28%5B  96 1419 1227   64  314  676   71   96    1  314 1126 1543 1544   98%2C  265 1892   96    1 7230   98 3168   87  310 1577  335   14  151  538%2C  232  287   14 1892  249    0    1    4  612  707   96   12   11%5D%2C array%281%29%29%2C %28%5B   96    64    88   314  1761  3159    24   345    24    96    26    96%2C    13  1172   341    ..." isContainer="True" shape="160000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x00000139501FF6E0&gt;" isContainer="True" shape="160000" />
<var name="vocab_size" type="int" qualifier="builtins" value="31512" />
<var name="word_ids" type="Tensor" qualifier="torch" value="tensor%28%5B  96%2C  345%2C 8627%2C 1964%2C  384%2C 1356%2C   80%2C 4073%2C   24%2C 1100%2C  677%2C   19%2C%0A          13%2C   96%2C   96%2C   96%2C   94%2C   61%2C    0%2C   21%2C   13%2C   61%2C   96%2C   24%2C%0A          96%2C   96%2C  180%5D%29" isContainer="True" shape="(27,)" />
<var name="word_to_index" type="dict" qualifier="builtins" value="%7B%270%27%3A 6702%2C %2700%27%3A 6863%2C %27000%27%3A 7514%2C %27001%27%3A 30981%2C %27007%27%3A 24912%2C %2701%27%3A 22404%2C %2702%27%3A 26120%2C %2703%27%3A 10671%2C %2704%27%3A 7841%2C %2705%27%3A 27679%2C %2706%27%3A 26455%2C %2707%27%3A 6896%2C %2708%27%3A 22268%2C %2709%27%3A 20433%2C %271%27%3A 790%2C %2710%27%3A 1673%2C %27100%27%3A 867%2C %271000%27%3A 10559%2C %2710000%27%3A 26943%2C %271000s%27%3A 19271%2C %271001%27%3A 7698%2C %27100s%27%3A 22864%2C %27100th%27%3A 876%2C %27101%27%3A 6839%2C %27101st%27%3A 24072%2C %27102%27%3A 25759%2C %27103%27%3A 26906%2C %27104%27%3A 26905%2C %27105%27%3A 24302%2C %27106%27%3A 11425%2C %27107%27%3A 730%2C %27108%27%3A 14447%2C %27109%27%3A 29063%2C %2710k%27%3A 15558%2C %2710th%27%3A 22221%2C %2710x%27%3A 20838%2C %2711%27%3A 3780%2C %27110%27%3A 17693%2C %271100%27%3A 30068%2C %27111%27%3A 17694%2C %27112%27%3A 10721%2C %27113%27%3A 27963%2C %27114%27%3A 22242%2C %27115%27%3A 14446%2C %27116%27%3A 18274%2C %27117%27%3A 31273%2C %27118%27%3A 20166%2C %27119%27%3A 30849%2C %2711th%27%3A 10297%2C %2712%27%3A 1670%2C %27120%27%3A 13593%2C %271200%27%3A 13289%2C %27121%27%3A 28197%2C %27122%27%3A 30823%2C %27123%27%3A 17217%2C %27124%27%3A 31073%2C %27125%27%3A 22605%2C %27126%27%3A 31155%2C %27127%27%3A 29305%2C %27128%27%3A 30348%2C %27129%27%3A 31304%2C %2712th%27%3A 10490%2C %2713%27%3A 3625%2C %27130%27%3A 20434%2C %271300%27%3A 16365%2C %27131%27%3A 21746%2C %27132%27%3A 29400%2C %27133%27%3A 29657%2C %27134%27%3A 27283%2C %27135%27%3A 27142%2C %27136%27%3A 26607%2C %27137%27%3A 31375%2C %27138%27%3A 2365..." isContainer="True" shape="31512" />
<var name="word_to_index_file" type="str" qualifier="builtins" value="./models/min8_model_word_to_index.pkl" />
