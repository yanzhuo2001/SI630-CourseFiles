<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="RegexpTokenizer" type="ABCMeta" qualifier="abc" value="%3Cclass %27nltk.tokenize.regexp.RegexpTokenizer%27&gt;" isContainer="True" />
<var name="attention_weights" type="Tensor" qualifier="torch" value="tensor%28%5B%5B%5B0.0120%2C 0.0041%2C 0.0132%2C 0.0173%5D%2C%0A         %5B0.0167%2C 0.0061%2C 0.0357%2C 0.0196%5D%2C%0A         %5B0.0540%2C 0.0120%2C 0.0443%2C 0.0203...3%2C 0.1828%5D%2C%0A         %5B0.0078%2C 0.0012%2C 0.0047%2C 0.0079%5D%2C%0A         %5B0.0293%2C 0.0347%2C 0.0214%2C 0.0332%5D%5D%5D%2C grad_fn=%3CSoftmaxBackward0&gt;%29" isContainer="True" shape="(1, 33, 4)" />
<var name="attn" type="ndarray" qualifier="numpy" value="%5B%5B8.1722513e-03 2.9605694e-04 8.0119846e-03 4.9284082e-03%5D%2C %5B1.1427826e-03 4.5748031e-05 1.9062110e-03 8.5575948e-04%5D%2C %5B1.5225257e-02 3.9807730e-04 8.7602269e-03 5.5086752e-03%5D%2C %5B3.1372048e-02 2.0775135e-04 8.1211150e-02 1.4176906e-02%5D%2C %5B1.5077013e-02 1.7085290e-03 1.5967628e-02 8.2628084e-03%5D%2C %5B1.0386348e-01 1.8149866e-04 1.3101365e-02 5.0815684e-03%5D%2C %5B2.1305671e-02 6.0440815e-04 2.0615125e-02 1.6641432e-02%5D%2C %5B6.1360754e-02 4.7780643e-04 4.3846495e-02 1.0230976e-02%5D%2C %5B2.9793960e-01 9.8847431e-01 6.0838401e-02 1.2182018e-03%5D%2C %5B5.2959781e-02 1.3867033e-03 6.3151114e-02 7.1251872e-03%5D%2C %5B8.1722513e-03 2.9605694e-04 8.0119846e-03 4.9284082e-03%5D%2C %5B1.7948503e-02 6.9852220e-04 2.5809772e-02 1.7620185e-02%5D%2C %5B1.4546009e-02 2.1632554e-04 6.8509099e-03 4.1279383e-03%5D%2C %5B1.2405593e-02 5.1783305e-04 2.0639626e-02 7.7297976e-03%5D%2C %5B1.9856916e-01 5.0768146e-04 2.2162767e-02 8.1453955e-01%5D%2C %5B2.1305671e-02 6.0440815e-04 2.0615125e-02 1.6641432e-02%5D%2C %5B1.6373649e-02 4.4407716e-04 1.5210218e-02 5.7556075e-0..." isContainer="True" shape="(22, 4)" />
<var name="avg_loss" type="float" qualifier="builtins" value="0.3015319093145663" />
<var name="batch_size" type="int" qualifier="builtins" value="1" />
<var name="best_f1" type="float" qualifier="builtins" value="0.0" />
<var name="dev_data_path" type="str" qualifier="builtins" value="sentiment.dev.csv" />
<var name="dev_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x000001D37480F7A0&gt;" isContainer="True" shape="20000" />
<var name="dev_f1" type="float64" qualifier="numpy" value="0.8653254676549186" shape="()" />
<var name="dev_list" type="list" qualifier="builtins" value="%5B%28%5B  96   96  193    4 7609   62   71   27  234   64  329   11   96    1%2C  629  314 7041 1811 1900 1018  177   96  104    4  105  106 4116   11%2C  264   11    1 1693  992   13 6499%5D%2C array%280%29%29%2C %28%5B   96   288     7  1160    71     3  1056   223   281    83 16366  2101%2C   268   607   140     7  1283    24 10093  1475    96 19270    98   951%2C 11996  2253   727   398     7    70    96     7 16366   781  2101   268%2C     7  3551    58    77  6499%5D%2C array%280%29%29%2C %28%5B  96    1   18 1818   27    0   21   96   15   88 5047   61    0   36%2C   61   96   13  104  190 4549   98   40   83 1712   61   96 1601   96%2C   96  551  200   81 1903    7   21  307   96 1043   96  772  200  172%2C   11  671 2278   96   96 1594   47   33 3211   15 3408  268    7  117%2C   24   33 5050%5D%2C array%281%29%29%2C %28%5B  96  193    4   18 1288   13  742 3822  862   96 3545 4912   77  128%2C 1236  884   96  128 3472  200  719 1755%5D%2C array%280%29%29%2C %28%5B  96  124  310  288    0   21   96    1  310   64   33 4288    6   96%2C  408  409 1695  151   62   11 ..." isContainer="True" shape="20000" />
<var name="dev_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000001D35F9C15B0&gt;" isContainer="True" shape="20000" />
<var name="device" type="device" qualifier="torch" value="device%28type=%27cpu%27%29" isContainer="True" />
<var name="embedding_size" type="int" qualifier="builtins" value="50" />
<var name="embeddings_file_path" type="str" qualifier="builtins" value="./models/1st_model_embeddings_state_dict.pt" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="1" />
<var name="f" type="BufferedReader" qualifier="_io" value="%3C_io.BufferedReader name=%27./models/1st_model_index_to_word.pkl%27&gt;" isContainer="True" />
<var name="f1_scores" type="list" qualifier="builtins" value="%5B%5D" isContainer="True" shape="0" />
<var name="index_to_word" type="dict" qualifier="builtins" value="%7B0%3A %27this%27%2C 1%3A %27was%27%2C 2%3A %27bought%27%2C 3%3A %27as%27%2C 4%3A %27a%27%2C 5%3A %27gift%27%2C 6%3A %27but%27%2C 7%3A %27the%27%2C 8%3A %27person%27%2C 9%3A %27who%27%2C 10%3A %27got%27%2C 11%3A %27it%27%2C 12%3A %27loved%27%2C 13%3A %27and%27%2C 14%3A %27they%27%2C 15%3A %27will%27%2C 16%3A %27use%27%2C 17%3A %27soon%27%2C 18%3A %27very%27%2C 19%3A %27well%27%2C 20%3A %27written%27%2C 21%3A %27book%27%2C 22%3A %27on%27%2C 23%3A %27period%27%2C 24%3A %27of%27%2C 25%3A %27world%27%2C 26%3A %27history%27%2C 27%3A %27with%27%2C 28%3A %27which%27%2C 29%3A %27i%27%2C 30%3A %27am%27%2C 31%3A %27familiar%27%2C 32%3A %27despite%27%2C 33%3A %27my%27%2C 34%3A %27familiarity%27%2C 35%3A %27subject%27%2C 36%3A %27area%27%2C 37%3A %27learned%27%2C 38%3A %27lot%27%2C 39%3A %27new%27%2C 40%3A %27information%27%2C 41%3A %27also%27%2C 42%3A %27one%27%2C 43%3A %27best%27%2C 44%3A %27concise%27%2C 45%3A %27descriptions%27%2C 46%3A %27wwii%27%2C 47%3A %27that%27%2C 48%3A %27have%27%2C 49%3A %27ever%27%2C 50%3A %27read%27%2C 51%3A %27thought%27%2C 52%3A %27provoking%27%2C 53%3A %27hot%27%2C 54%3A %27cross%27%2C 55%3A %27buns%27%2C 56%3A %27contains%27%2C 57%3A %27cast%27%2C 58%3A %27characters%27%2C 59%3A %27you%27%2C 60%3A %27fall%27%2C 61%3A %27in%27%2C 62%3A %27love%27%2C 63%3A %27want%27%2C 64%3A %27to%27%2C 65%3A %27hang%27%2C 66%3A %27out%27%2C 67%3A %27has%27%2C 68%3A %27few%27%2C 69%3A %27different%27%2C 70%3A %27plot%27%2C 71%3A %27story%27%2C 72%3A %27lines%27%2C 73%3A %27intersect%27%2C 74%3A %27unexpected%27%2C 75%3A %27ways%27%2C 76%3A %27stories%27%2C 77%3A %27are%27%2C 78%3A %27full%27%2C 79%3A %27h..." isContainer="True" shape="40547" />
<var name="index_to_word_file" type="str" qualifier="builtins" value="./models/1st_model_index_to_word.pkl" />
<var name="label" type="float" qualifier="builtins" value="1.0" />
<var name="labels" type="Tensor" qualifier="torch" value="tensor%28%5B1.%5D%29" isContainer="True" shape="(1,)" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="loss" type="Tensor" qualifier="torch" value="tensor%280.3730%2C grad_fn=%3CBinaryCrossEntropyWithLogitsBackward0&gt;%29" isContainer="True" shape="()" />
<var name="loss_function" type="BCELoss" qualifier="torch.nn.modules.loss" value="BCELoss%28%29" isContainer="True" />
<var name="losses" type="list" qualifier="builtins" value="%5B%5D" isContainer="True" shape="0" />
<var name="max_steps" type="int" qualifier="builtins" value="1000000000" />
<var name="model" type="DocumentAttentionClassifier" qualifier="__main__" value="DocumentAttentionClassifier%28%0A  %28embeddings%29%3A Embedding%2840547%2C 50%29%0A  %28linear%29%3A Linear%28in_features=200%2C out_features=1%2C bias=True%29%0A%29" isContainer="True" />
<var name="model1" type="DocumentAttentionClassifier" qualifier="__main__" value="DocumentAttentionClassifier%28%0A  %28embeddings%29%3A Embedding%2840547%2C 50%29%0A  %28linear%29%3A Linear%28in_features=200%2C out_features=1%2C bias=True%29%0A%29" isContainer="True" />
<var name="negative_samples" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B17160  I liked the Ogre Detective series but this is ...      0%5D %5B186    Seriously%2C what is going on here%3F Treating oth...      0%5D" isContainer="True" shape="(2, 2)" />
<var name="num_heads" type="int" qualifier="builtins" value="4" />
<var name="optimizer" type="AdamW" qualifier="torch.optim.adamw" value="AdamW %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0.01%0A%29" isContainer="True" />
<var name="patience" type="int" qualifier="builtins" value="30000" />
<var name="positive_samples" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B13479  Great analysis of what is happening to young a...      1%5D %5B12266  It was a good read. A little slow for me at ti...      1%5D" isContainer="True" shape="(2, 2)" />
<var name="pred" type="float" qualifier="builtins" value="1.0" />
<var name="prediction" type="float" qualifier="builtins" value="1.0" />
<var name="predictions" type="list" qualifier="builtins" value="%5B0.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C 1.0%2C 0.0%2C 0.0%2C 0.0%2C 1.0%2C..." isContainer="True" shape="20000" />
<var name="probability" type="Tensor" qualifier="torch" value="tensor%28%5B0.7899%5D%29" isContainer="True" shape="(1,)" />
<var name="running_loss" type="float" qualifier="builtins" value="0.3729633688926697" />
<var name="s" type="str" qualifier="builtins" value="%0AI%27m a big fan of his%2C and I have to say that this was a BIG letdown. It features%3A Stilted dialogue%2C no character development%2C no suspense%2C no description of Indian tradition and poor editing.%0A%0AAvoid at all costs.%0A" />
<var name="sample_text" type="str" qualifier="builtins" value="Although this product arrived on time and was nicely packaged%2C I found its quality lacking and performance subpar compared to other brands." />
<var name="selected_samples" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B13479  Great analysis of what is happening to young a...      1%5D %5B12266  It was a good read. A little slow for me at ti...      1%5D %5B17160  I liked the Ogre Detective series but this is ...      0%5D %5B186    Seriously%2C what is going on here%3F Treating oth...      0%5D" isContainer="True" shape="(4, 2)" />
<var name="sent_dev_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0      Picturing Perfect is a sappy love story with l...      0%5D %5B1      Seems like the same story as any other series ...      0%5D %5B2      I was very pleased with this book. I will be t...      1%5D %5B3      It is a very light and rather silly novel. The...      0%5D %5B4      I did not like this book. It was not to my tas...      0%5D %5B...                                                  ...    ...%5D %5B19995  Great content%2C the story is fantastic%2C but sho...      1%5D %5B19996  Typical book club book... Incest%2C child abuse%2C...      0%5D %5B19997  Fascinating book. Shorter than most Russell Ba...      1%5D %5B19998  This book is not well-organized%2C which is impo...      0%5D %5B19999  CAN WE CALL THIS A CLASSIC OF THE GENRE%3F I THI...      1%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_test_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27text%27%2C %27predicted_label%27%5D %5B0            0  Really sad review as I absolutely loved the fi...   %5D %5B1            1  Excellent content%2C perfect for Christians who ...   %5D %5B2            2  This is an okay book if you need advice on bud...   %5D %5B3            3  This is one book you can%27t put down%21 This book...   %5D %5B4            4  There were to many names that I had no idenity...   %5D %5B...        ...                                                ...   %5D %5B19995    19995  I found this book to be a very entertaining an...   %5D %5B19996    19996  Wow%2C what a Middle School%21 Read this book your...   %5D %5B19997    19997  Not what I expected. Not enough about circular...   %5D %5B19998    19998  I like Joanne Fluke%27s mystery series starring ...   %5D %5B19999    19999  Grow some chickens%2C improve your land%2C make al...   %5D %5B%5D %5B0                  0.0  %5D %5B1                  1.0  %5D %5B2                  0.0  %5D %5B3                  1.0  %5D %5B4                  0.0  %5D %5B...                ...  %5D %5B19995         ..." isContainer="True" shape="(20000, 3)" />
<var name="sent_train_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0       It was what I needed. There was no markings or...      1%5D %5B1       A cute little book. My wife gets the family wa...      1%5D %5B2       I bought these for 40.00 and for the price I j...      0%5D %5B3       It was interesting and enjoyable reading. Shor...      1%5D %5B4       A perfect ending to an amazing story. This was...      1%5D %5B...                                                   ...    ...%5D %5B159995  After reading every book Stephen King has to o...      1%5D %5B159996  Baby boomers who are experiencing %22aging eyeba...      0%5D %5B159997  Must read%2C must have%2C must read again. This bo...      1%5D %5B159998  Dr. Chopra%27s books are always enlightening and...      1%5D %5B159999  Boooring%21%21 Just enough to keep you intrigued f...      0%5D %5B%5D" isContainer="True" shape="(160000, 2)" />
<var name="step" type="int" qualifier="builtins" value="0" />
<var name="steps_since_improvement" type="int" qualifier="builtins" value="0" />
<var name="test_data" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   885%2C   626%2C     3%2C    96%2C  1132%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4050%2C     4%2C  6499...8504%2C  7595%2C     7%2C%0A         2087%2C   762%2C     7%2C   727%2C    42%2C   335%2C   385%2C   514%2C    98%2C   629%2C%0A          102%2C    13%2C   450%5D%29%2C tensor%28%5B  96%2C 1669%2C 1427%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2651%2C  314%2C  804%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1887%2C   61%2C    7%2C  693%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  4921%2C    21%2C   335%2C    59%2C   661%2C   633%2C    22%2C%0A         8229%2C    13%2C  3966%2C    96%2C   360%2C   345...  719%2C    96%2C   309%2C   497%2C    94%2C   719%2C  1105%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 20119%2C    64%2C     7%2C  2881%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  506%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 8965%2C   59%2C   ...7%2C    4%2C  768%2C   13%2C%0A        3576%2C  677%2C   86%2C  678%2C   64%2C  287%2C  193%2C  537%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  679%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_data_path" type="str" qualifier="builtins" value="sentiment.test.csv" />
<var name="test_list" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   885%2C   626%2C     3%2C    96%2C  1132%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4050%2C     4%2C  6499...8504%2C  7595%2C     7%2C%0A         2087%2C   762%2C     7%2C   727%2C    42%2C   335%2C   385%2C   514%2C    98%2C   629%2C%0A          102%2C    13%2C   450%5D%29%2C tensor%28%5B  96%2C 1669%2C 1427%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2651%2C  314%2C  804%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1887%2C   61%2C    7%2C  693%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  4921%2C    21%2C   335%2C    59%2C   661%2C   633%2C    22%2C%0A         8229%2C    13%2C  3966%2C    96%2C   360%2C   345...  719%2C    96%2C   309%2C   497%2C    94%2C   719%2C  1105%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 20119%2C    64%2C     7%2C  2881%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  506%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 8965%2C   59%2C   ...7%2C    4%2C  768%2C   13%2C%0A        3576%2C  677%2C   86%2C  678%2C   64%2C  287%2C  193%2C  537%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  679%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_output" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27predicted_label%27%5D %5B0            0              0.0%5D %5B1            1              1.0%5D %5B2            2              0.0%5D %5B3            3              1.0%5D %5B4            4              0.0%5D %5B...        ...              ...%5D %5B19995    19995              1.0%5D %5B19996    19996              1.0%5D %5B19997    19997              0.0%5D %5B19998    19998              1.0%5D %5B19999    19999              1.0%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="text" type="str" qualifier="builtins" value="Seriously%2C what is going on here%3F Treating other living creatures with respect will be the downfall of mankind%3F I doubt it. Treating each other with no respect will." />
<var name="texts" type="list" qualifier="builtins" value="%5B%27Great analysis of what is happening to young adults today. Not a blame or political book.%27%2C %22It was a good read. A little slow for me at times but I still kept reading. I wish it wouldn%27t have taken so long to get to the plot.%22%2C %27I liked the Ogre Detective series but this is like it was written by a different person. The plot is just so improbable as ju...ith this series. I read the run up for some of the sequels and it seems like more of the same. Reads like a young adult novel.%27%2C %27Seriously%2C what is going on here%3F Treating other living creatures with respect will be the downfall of mankind%3F I doubt it. Treating each other with no respect will.%27%5D" isContainer="True" shape="4" />
<var name="tokenizer" type="RegexpTokenizer" qualifier="nltk.tokenize.regexp" value="RegexpTokenizer%28pattern=%27%5C%5Cw%2B%27%2C gaps=False%2C discard_empty=True%2C flags=re.UNICODE%7Cre.MULTILINE%7Cre.DOTALL%29" isContainer="True" />
<var name="tokens" type="list" qualifier="builtins" value="%5B%27%3CUNK&gt;%27%2C %27this%27%2C %27product%27%2C %27arrived%27%2C %27on%27%2C %27time%27%2C %27and%27%2C %27was%27%2C %27nicely%27%2C %27packaged%27%2C %27%3CUNK&gt;%27%2C %27found%27%2C %27its%27%2C %27quality%27%2C %27lacking%27%2C %27and%27%2C %27performance%27%2C %27subpar%27%2C %27compared%27%2C %27to%27%2C %27other%27%2C %27brands%27%5D" isContainer="True" shape="22" />
<var name="train_data_path" type="str" qualifier="builtins" value="sentiment.train.csv" />
<var name="train_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x000001D379DD9340&gt;" isContainer="True" shape="160000" />
<var name="train_list" type="list" qualifier="builtins" value="%5B%28%5B   96     1   287    96   248    96     1   204 16850   291   110    24%2C    47    96   193   306    64    50    13    93%5D%2C array%281%29%29%2C %28%5B  96  884  234   21   96 1191 2472    7  693 1931 1926  268   96   96%2C   85 1688   96   21   67    7 1160  884 1578%5D%2C array%281%29%29%2C %28%5B  96    2  320   98 3569 7030   13   98    7  366   96  128 5745   14%2C  151   88  105 9655   96   88   96  123   59 1956  320   59  175   11%2C   15  128   88   96  140  105 9655 5834%5D%2C array%280%29%29%2C %28%5B  96    1  358   13  333  341   96   76 1775   83   96  241 1635 3070%2C   24   96   13 1196  729   61  305   95   96   76 1426   48    4  699%2C  117   96  115    4  486   24   96   96  356   76   96   62%5D%2C array%281%29%29%2C %28%5B  96 1427 1233   64  314  679   71   96    1  314 1131 1554 1555   98%2C  265 1905   96    1 7419   98 3204   87  310 1588  335   14  151  539%2C  232  287   14 1905  249    0    1    4  615  710   96   12   11%5D%2C array%281%29%29%2C %28%5B   96    64    88   314  1774  3195    24   345    24    96    26    96%2C    13  1178   341    ..." isContainer="True" shape="160000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000001D379A37FB0&gt;" isContainer="True" shape="160000" />
<var name="vocab_size" type="int" qualifier="builtins" value="40547" />
<var name="word_ids" type="Tensor" qualifier="torch" value="tensor%28%5B  96%2C  345%2C 8879%2C 1977%2C  385%2C 1364%2C   80%2C 4134%2C   24%2C 1105%2C  680%2C   19%2C%0A          13%2C   96%2C   96%2C   96%2C   94%2C   61%2C    0%2C   21%2C   13%2C   61%2C   96%2C   24%2C%0A          96%2C   96%2C  180%5D%29" isContainer="True" shape="(27,)" />
<var name="word_to_index" type="dict" qualifier="builtins" value="%7B%270%27%3A 6860%2C %2700%27%3A 7030%2C %27000%27%3A 7717%2C %27001%27%3A 38825%2C %27007%27%3A 28099%2C %2700am%27%3A 25993%2C %2701%27%3A 24762%2C %2702%27%3A 29846%2C %2703%27%3A 11089%2C %27039%27%3A 38140%2C %2704%27%3A 8057%2C %2705%27%3A 32280%2C %2706%27%3A 30353%2C %2707%27%3A 7063%2C %2708%27%3A 24586%2C %2709%27%3A 22275%2C %270f%27%3A 37971%2C %271%27%3A 793%2C %2710%27%3A 1684%2C %27100%27%3A 870%2C %271000%27%3A 10968%2C %2710000%27%3A 31123%2C %271000s%27%3A 20833%2C %271000x%27%3A 33996%2C %271001%27%3A 7910%2C %27100k%27%3A 39109%2C %27100s%27%3A 25367%2C %27100th%27%3A 879%2C %27101%27%3A 7005%2C %27101st%27%3A 26963%2C %27102%27%3A 29336%2C %27103%27%3A 31073%2C %27104%27%3A 31072%2C %27105%27%3A 27289%2C %27106%27%3A 11911%2C %271066%27%3A 34895%2C %27107%27%3A 733%2C %27108%27%3A 15245%2C %27109%27%3A 34772%2C %2710k%27%3A 16506%2C %2710th%27%3A 24530%2C %2710x%27%3A 22765%2C %2710yr%27%3A 28921%2C %2711%27%3A 3831%2C %27110%27%3A 18961%2C %271100%27%3A 36651%2C %27111%27%3A 18962%2C %27112%27%3A 11145%2C %27113%27%3A 32771%2C %27114%27%3A 24557%2C %27115%27%3A 15244%2C %27116%27%3A 19634%2C %27117%27%3A 39597%2C %27118%27%3A 21937%2C %27119%27%3A 38487%2C %2711pm%27%3A 25792%2C %2711th%27%3A 10687%2C %2712%27%3A 1681%2C %27120%27%3A 14299%2C %271200%27%3A 13958%2C %27121%27%3A 33184%2C %27122%27%3A 38431%2C %27123%27%3A 18402%2C %27124%27%3A 39041%2C %27125%27%3A 25024%2C %27126%27%3A 39242%2C %27127%27%3A 35211%2C %27128%27%3A 37280%2C %27129%27%3A 39683%2C %2712th%27%3A 10890%2C %2712yr%27%3A 17641%2C %2713%27%3A 3671%2C %27130%27..." isContainer="True" shape="40547" />
<var name="word_to_index_file" type="str" qualifier="builtins" value="./models/1st_model_word_to_index.pkl" />
</xml>
Step 500, Avg Loss: 0.655061335504055
Step 1000, Avg Loss: 0.5566916610598565
Step 1500, Avg Loss: 0.46554081403836606
Step 2000, Avg Loss: 0.43220002366043625
Step 2500, Avg Loss: 0.39325531210564074
Step 3000, Avg Loss: 0.4083384552244097
Step 3500, Avg Loss: 0.4055701124528423
Step 4000, Avg Loss: 0.4453341857958585
Step 4500, Avg Loss: 0.3829473962089978
Step 5000, Avg Loss: 0.41397623348329216
-----Step 5000, F1 Score: 0.8343821796024373-----
Step 5500, Avg Loss: 0.36562428286485377
Step 6000, Avg Loss: 0.3563226468572393
Step 6500, Avg Loss: 0.36717829256015827
Step 7000, Avg Loss: 0.38949184804415565
Step 7500, Avg Loss: 0.36211359037971125
Step 8000, Avg Loss: 0.4012853099788772
Step 8500, Avg Loss: 0.31585708750842606
Step 9000, Avg Loss: 0.3085434564985335
Step 9500, Avg Loss: 0.38693955140304753
Step 10000, Avg Loss: 0.399293676894973
-----Step 10000, F1 Score: 0.8390628197258031-----
Step 10500, Avg Loss: 0.3220315198650351
Step 11000, Avg Loss: 0.3334921585964039
Step 11500, Avg Loss: 0.39685247659008016
Step 12000, Avg Loss: 0.3216622564527206
Step 12500, Avg Loss: 0.3599272537072102
Step 13000, Avg Loss: 0.3632250410089036
Step 13500, Avg Loss: 0.3893645988833159
Step 14000, Avg Loss: 0.41502772556897255
Step 14500, Avg Loss: 0.32810667595791165
Step 15000, Avg Loss: 0.3657497005451005
-----Step 15000, F1 Score: 0.8511300121506683-----
Step 15500, Avg Loss: 0.3518296580882743
Step 16000, Avg Loss: 0.3741741471828427
Step 16500, Avg Loss: 0.34977467126003464
Step 17000, Avg Loss: 0.37957598435808904
Step 17500, Avg Loss: 0.33953398919198663
Step 18000, Avg Loss: 0.34161268145730717
Step 18500, Avg Loss: 0.3379720386797562
Step 19000, Avg Loss: 0.34124296310549834
Step 19500, Avg Loss: 0.38557456008344887
Step 20000, Avg Loss: 0.3067841129291337
-----Step 20000, F1 Score: 0.8506232408524327-----
Step 20500, Avg Loss: 0.3423742385371588
Step 21000, Avg Loss: 0.34476010282873176
Step 21500, Avg Loss: 0.3483820790734026
Step 22000, Avg Loss: 0.3303311295737512
Step 22500, Avg Loss: 0.3422999491506489
Step 23000, Avg Loss: 0.3427428753073327
Step 23500, Avg Loss: 0.3120900869718753
Step 24000, Avg Loss: 0.3308851383803412
Step 24500, Avg Loss: 0.32570203292462974
Step 25000, Avg Loss: 0.3003466576858191
-----Step 25000, F1 Score: 0.855243866679699-----
Step 25500, Avg Loss: 0.33370673050160987
Step 26000, Avg Loss: 0.39737588797038187
Step 26500, Avg Loss: 0.33249555756466
Step 27000, Avg Loss: 0.3273057618444145
Step 27500, Avg Loss: 0.3323885233271867
Step 28000, Avg Loss: 0.3347598678997747
Step 28500, Avg Loss: 0.31602565402063193
Step 29000, Avg Loss: 0.29880928060784934
Step 29500, Avg Loss: 0.32489965547039173
Step 30000, Avg Loss: 0.3201836820901954
-----Step 30000, F1 Score: 0.8548203080433543-----
Step 30500, Avg Loss: 0.3319173305830045
Step 31000, Avg Loss: 0.29013699370325774
Step 31500, Avg Loss: 0.3485552630380553
Step 32000, Avg Loss: 0.32447342798503814
Step 32500, Avg Loss: 0.30707083659630735
Step 33000, Avg Loss: 0.3281411457769573
Step 33500, Avg Loss: 0.3618230665991432
Step 34000, Avg Loss: 0.32684180658467815
Step 34500, Avg Loss: 0.3369850017929566
Step 35000, Avg Loss: 0.2883051551027311
-----Step 35000, F1 Score: 0.8598674145843958-----
Step 35500, Avg Loss: 0.34833653060378494
Step 36000, Avg Loss: 0.30450672490248687
Step 36500, Avg Loss: 0.3339391446232039
Step 37000, Avg Loss: 0.34095423157926413
Step 37500, Avg Loss: 0.3679327264871972
Step 38000, Avg Loss: 0.34326753235403157
Step 38500, Avg Loss: 0.3611754680008162
Step 39000, Avg Loss: 0.3256119550795702
Step 39500, Avg Loss: 0.317638018244179
Step 40000, Avg Loss: 0.3504453957136429
-----Step 40000, F1 Score: 0.864484824478883-----
Step 40500, Avg Loss: 0.311380277066055
Step 41000, Avg Loss: 0.30226955009630185
Step 41500, Avg Loss: 0.3234483097893826
Step 42000, Avg Loss: 0.28614118704893915
Step 42500, Avg Loss: 0.3275270104780793
Step 43000, Avg Loss: 0.3256502447378443
Step 43500, Avg Loss: 0.327639827481471
Step 44000, Avg Loss: 0.3235760491594847
Step 44500, Avg Loss: 0.32614017485023944
Step 45000, Avg Loss: 0.33730802688960104
-----Step 45000, F1 Score: 0.8615733347071248-----
Step 45500, Avg Loss: 0.29499916621492955
Step 46000, Avg Loss: 0.30509423630970195
Step 46500, Avg Loss: 0.30366237771711896
Step 47000, Avg Loss: 0.37967843968190573
Step 47500, Avg Loss: 0.32380044607294256
Step 48000, Avg Loss: 0.3655896482702956
Step 48500, Avg Loss: 0.3467447143929312
Step 49000, Avg Loss: 0.36860270642247633
Step 49500, Avg Loss: 0.3118623832263693
Step 50000, Avg Loss: 0.30433514187627586
-----Step 50000, F1 Score: 0.8656018698870277-----
Step 50500, Avg Loss: 0.32100462746054836
Step 51000, Avg Loss: 0.3377042368243929
Step 51500, Avg Loss: 0.2816899109808037
Step 52000, Avg Loss: 0.3218556404559058
Step 52500, Avg Loss: 0.34158544649448597
Step 53000, Avg Loss: 0.27529135924695586
Step 53500, Avg Loss: 0.32348316949788203
Step 54000, Avg Loss: 0.3422257318712509
Step 54500, Avg Loss: 0.24986988255662437
Step 55000, Avg Loss: 0.3303921257701513
-----Step 55000, F1 Score: 0.8691859040337205-----
Step 55500, Avg Loss: 0.2952145568347478
Step 56000, Avg Loss: 0.3506088370066427
Step 56500, Avg Loss: 0.32571350490432815
Step 57000, Avg Loss: 0.3276991548955702
Step 57500, Avg Loss: 0.3267485475414869
Step 58000, Avg Loss: 0.3283593335054256
Step 58500, Avg Loss: 0.32062960648862643
Step 59000, Avg Loss: 0.3359727395967639
Step 59500, Avg Loss: 0.3426613819688646
Step 60000, Avg Loss: 0.35851736955414526
-----Step 60000, F1 Score: 0.8567341316096218-----
Step 60500, Avg Loss: 0.3172684670697217
Step 61000, Avg Loss: 0.38033197940705576
Step 61500, Avg Loss: 0.32290031140737485
Step 62000, Avg Loss: 0.33658064620207734
Step 62500, Avg Loss: 0.30073038884587006
Step 63000, Avg Loss: 0.34679653760395013
Step 63500, Avg Loss: 0.3265627568718628
Step 64000, Avg Loss: 0.3300050285622201
Step 64500, Avg Loss: 0.2878665730742141
Step 65000, Avg Loss: 0.3730775216305556
-----Step 65000, F1 Score: 0.8661863098301595-----
Step 65500, Avg Loss: 0.30635809530036934
Step 66000, Avg Loss: 0.2997060247494351
Step 66500, Avg Loss: 0.31710114761818115
Step 67000, Avg Loss: 0.30540230399651047
Step 67500, Avg Loss: 0.32517965875010124
Step 68000, Avg Loss: 0.33257269389223254
Step 68500, Avg Loss: 0.2607891938024295
Step 69000, Avg Loss: 0.3743322427785024
Step 69500, Avg Loss: 0.3039626016521033
Step 70000, Avg Loss: 0.30384708996469273
-----Step 70000, F1 Score: 0.8683106517957638-----
Step 70500, Avg Loss: 0.29163164430508914
Step 71000, Avg Loss: 0.29398469528906573
Step 71500, Avg Loss: 0.28183689172415327
Step 72000, Avg Loss: 0.3047079088299579
Step 72500, Avg Loss: 0.3607324289911412
Step 73000, Avg Loss: 0.3155693571417942
Step 73500, Avg Loss: 0.2506750091944559
Step 74000, Avg Loss: 0.3201625304446825
Step 74500, Avg Loss: 0.29360601722439517
Step 75000, Avg Loss: 0.3038437250961724
-----Step 75000, F1 Score: 0.8726049436887524-----
Step 75500, Avg Loss: 0.3401106970476394
Step 76000, Avg Loss: 0.31789235764928164
Step 76500, Avg Loss: 0.3234738182511646
Step 77000, Avg Loss: 0.316573701888934
Step 77500, Avg Loss: 0.3155289272570153
Step 78000, Avg Loss: 0.3001953825755518
Step 78500, Avg Loss: 0.29327446130521273
Step 79000, Avg Loss: 0.28620399688616455
Step 79500, Avg Loss: 0.3166481745967061
Step 80000, Avg Loss: 0.3398128130885889
-----Step 80000, F1 Score: 0.8732842649691664-----
Step 80500, Avg Loss: 0.2637680770024017
Step 81000, Avg Loss: 0.2769294276048604
Step 81500, Avg Loss: 0.31465802802014875
Step 82000, Avg Loss: 0.35218075701934687
Step 82500, Avg Loss: 0.3089717893760148
Step 83000, Avg Loss: 0.31478585815575205
Step 83500, Avg Loss: 0.3324637556664566
Step 84000, Avg Loss: 0.3502803910428629
Step 84500, Avg Loss: 0.34188501959323186
Step 85000, Avg Loss: 0.3206499959546654
-----Step 85000, F1 Score: 0.8711079298532645-----
Step 85500, Avg Loss: 0.29632243566448596
Step 86000, Avg Loss: 0.3489341894842073
Step 86500, Avg Loss: 0.325691427074089
Step 87000, Avg Loss: 0.3291251138109947
Step 87500, Avg Loss: 0.29702444399557865
Step 88000, Avg Loss: 0.3270279187838787
Step 88500, Avg Loss: 0.29078114864350935
Step 89000, Avg Loss: 0.3005389195452408
Step 89500, Avg Loss: 0.30495473998937817
Step 90000, Avg Loss: 0.3724643824368868
-----Step 90000, F1 Score: 0.8746846089150546-----
Step 90500, Avg Loss: 0.34413426894752774
Step 91000, Avg Loss: 0.28010799514473
Step 91500, Avg Loss: 0.33317108268574885
Step 92000, Avg Loss: 0.3540477835212878
Step 92500, Avg Loss: 0.34867302653117804
Step 93000, Avg Loss: 0.3503047143145814
Step 93500, Avg Loss: 0.29116185981089804
Step 94000, Avg Loss: 0.3412060911156004
Step 94500, Avg Loss: 0.27851300792908296
Step 95000, Avg Loss: 0.32153887149527144
-----Step 95000, F1 Score: 0.8725961538461539-----
Step 95500, Avg Loss: 0.35808505369658816
Step 96000, Avg Loss: 0.33182527434034276
Step 96500, Avg Loss: 0.3185695093520044
Step 97000, Avg Loss: 0.32686476356579985
Step 97500, Avg Loss: 0.3357728725377601
Step 98000, Avg Loss: 0.3097274868065724
Step 98500, Avg Loss: 0.2784153466538046
Step 99000, Avg Loss: 0.31879471378086965
Step 99500, Avg Loss: 0.3327032217140004
Step 100000, Avg Loss: 0.3251860197959322
-----Step 100000, F1 Score: 0.8686255338856584-----
Step 100500, Avg Loss: 0.29098145397979713
Step 101000, Avg Loss: 0.28311701521009675
Step 101500, Avg Loss: 0.34854865405791496
Step 102000, Avg Loss: 0.28258568517146343
Step 102500, Avg Loss: 0.30832325119839515
Step 103000, Avg Loss: 0.30330871425079386
Step 103500, Avg Loss: 0.2847616585326796
Step 104000, Avg Loss: 0.2881155976404571
Step 104500, Avg Loss: 0.3372368139431928
Step 105000, Avg Loss: 0.3304746216162166
-----Step 105000, F1 Score: 0.8718319310380427-----
Step 105500, Avg Loss: 0.31958066771324956
Step 106000, Avg Loss: 0.24865787737657957
Step 106500, Avg Loss: 0.30195824795335646
Step 107000, Avg Loss: 0.319525527300284
Step 107500, Avg Loss: 0.3227000510143589
Step 108000, Avg Loss: 0.27735855161361384
Step 108500, Avg Loss: 0.3395152590834186
Step 109000, Avg Loss: 0.3162657569061266
Step 109500, Avg Loss: 0.2812274883100472
Step 110000, Avg Loss: 0.2766199654480042
-----Step 110000, F1 Score: 0.875975975975976-----
Step 110500, Avg Loss: 0.29065180662838613
Step 111000, Avg Loss: 0.2632576425320003
Step 111500, Avg Loss: 0.31444811706492326
Step 112000, Avg Loss: 0.2872640816549683
Step 112500, Avg Loss: 0.2900415344130051
Step 113000, Avg Loss: 0.30204147523496794
Step 113500, Avg Loss: 0.30294985680584796
Step 114000, Avg Loss: 0.29732571509842093
Step 114500, Avg Loss: 0.29765234643575966
Step 115000, Avg Loss: 0.3015908555989736
-----Step 115000, F1 Score: 0.8767177071063997-----
Step 115500, Avg Loss: 0.3767491718164529
Step 116000, Avg Loss: 0.35493646598281337
Step 116500, Avg Loss: 0.33414889368024886
Step 117000, Avg Loss: 0.3417069251358771
Step 117500, Avg Loss: 0.3069474432657589
Step 118000, Avg Loss: 0.3259167595754584
Step 118500, Avg Loss: 0.33665515348740155
Step 119000, Avg Loss: 0.2966531152197058
Step 119500, Avg Loss: 0.3234648017759828
Step 120000, Avg Loss: 0.2905462993174224
-----Step 120000, F1 Score: 0.8733900117090058-----
Step 120500, Avg Loss: 0.3112547285034634
Step 121000, Avg Loss: 0.25630192367269045
Step 121500, Avg Loss: 0.305617639039745
Step 122000, Avg Loss: 0.30685245690586815
Step 122500, Avg Loss: 0.28712146105017744
Step 123000, Avg Loss: 0.3606405757676207
Step 123500, Avg Loss: 0.2829413702320162
Step 124000, Avg Loss: 0.30969976858749715
Step 124500, Avg Loss: 0.30785078426875406
Step 125000, Avg Loss: 0.3415575385307857
-----Step 125000, F1 Score: 0.8691311807830292-----
Step 125500, Avg Loss: 0.3187042071802498
Step 126000, Avg Loss: 0.3044789489931682
Step 126500, Avg Loss: 0.2925334151988791
Step 127000, Avg Loss: 0.2890601993450182
Step 127500, Avg Loss: 0.31432845674884446
Step 128000, Avg Loss: 0.2614190488953973
Step 128500, Avg Loss: 0.2885253477162244
Step 129000, Avg Loss: 0.30703686312878564
Step 129500, Avg Loss: 0.25712364089845685
Step 130000, Avg Loss: 0.28149773687871127
-----Step 130000, F1 Score: 0.8715601049328738-----
Step 130500, Avg Loss: 0.2803040594693284
Step 131000, Avg Loss: 0.2920060329947155
Step 131500, Avg Loss: 0.3134694436737627
Step 132000, Avg Loss: 0.2585946567625742
Step 132500, Avg Loss: 0.31839834055839855
Step 133000, Avg Loss: 0.31970376299123743
Step 133500, Avg Loss: 0.29292284710463357
Step 134000, Avg Loss: 0.3062829697357665
Step 134500, Avg Loss: 0.31219666016398695
Step 135000, Avg Loss: 0.31014324495452456
-----Step 135000, F1 Score: 0.8781683580918762-----
Step 135500, Avg Loss: 0.3109275230115163
Step 136000, Avg Loss: 0.2969001294607151
Step 136500, Avg Loss: 0.27587926441010496
Step 137000, Avg Loss: 0.3124925312996856
Step 137500, Avg Loss: 0.24658981229462187
Step 138000, Avg Loss: 0.3485549693741923
Step 138500, Avg Loss: 0.34598595049217695
Step 139000, Avg Loss: 0.25752646205260044
Step 139500, Avg Loss: 0.3175040192033521
Step 140000, Avg Loss: 0.34317708220606435
-----Step 140000, F1 Score: 0.8723850442432612-----
Step 140500, Avg Loss: 0.3365858071180783
Step 141000, Avg Loss: 0.3576917295892963
Step 141500, Avg Loss: 0.2755460660629906
Step 142000, Avg Loss: 0.3201398923191409
Step 142500, Avg Loss: 0.30107350781955755
Step 143000, Avg Loss: 0.3280357120601698
Step 143500, Avg Loss: 0.2534653697349622
Step 144000, Avg Loss: 0.24978182270131344
Step 144500, Avg Loss: 0.3425731232475664
Step 145000, Avg Loss: 0.29688713918950815
-----Step 145000, F1 Score: 0.8746006389776357-----
Step 145500, Avg Loss: 0.27314891392126445
Step 146000, Avg Loss: 0.3364896295645235
Step 146500, Avg Loss: 0.26384302552798183
Step 147000, Avg Loss: 0.3041445098591903
Step 147500, Avg Loss: 0.3137864627580566
Step 148000, Avg Loss: 0.3175876455105026
Step 148500, Avg Loss: 0.3267262907359982
Step 149000, Avg Loss: 0.2540217922888187
Step 149500, Avg Loss: 0.33027803936186684
Step 150000, Avg Loss: 0.2843024944802528
-----Step 150000, F1 Score: 0.8745483230698763-----
Step 150500, Avg Loss: 0.3495984120266803
Step 151000, Avg Loss: 0.3300690339507419
Step 151500, Avg Loss: 0.2834204527996917
Step 152000, Avg Loss: 0.2558215488137803
Step 152500, Avg Loss: 0.3616426433544548
Step 153000, Avg Loss: 0.3533059223114979
Step 153500, Avg Loss: 0.2674081390566298
Step 154000, Avg Loss: 0.32642463654736636
Step 154500, Avg Loss: 0.29524812955834207
Step 155000, Avg Loss: 0.361946004070458
-----Step 155000, F1 Score: 0.876102724955891-----
Step 155500, Avg Loss: 0.3564234411109064
Step 156000, Avg Loss: 0.3079648832682651
Step 156500, Avg Loss: 0.3374681109233934
Step 157000, Avg Loss: 0.3592718206292702
Step 157500, Avg Loss: 0.3286976951520046
Step 158000, Avg Loss: 0.31744998502134697
Step 158500, Avg Loss: 0.31659497139706216
Step 159000, Avg Loss: 0.31136708172681393
Step 159500, Avg Loss: 0.3332313623021764
Step 160000, Avg Loss: 0.29820984919351756
-----Step 160000, F1 Score: 0.8715822722337381-----
   inst_id                                               text  predicted_label
0        0  Really sad review as I absolutely loved the fi...              0.0
1        1  Excellent content, perfect for Christians who ...              1.0
2        2  This is an okay book if you need advice on bud...              1.0
3        3  This is one book you can't put down! This book...              1.0
4        4  There were to many names that I had no idenity...              0.0