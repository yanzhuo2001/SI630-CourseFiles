<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="RegexpTokenizer" type="ABCMeta" qualifier="abc" value="%3Cclass %27nltk.tokenize.regexp.RegexpTokenizer%27&gt;" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="1" />
<var name="best_f1" type="float" qualifier="builtins" value="0.0" />
<var name="dev_data_path" type="str" qualifier="builtins" value="sentiment.dev.csv" />
<var name="dev_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x0000024A83FAB740&gt;" isContainer="True" shape="20000" />
<var name="dev_list" type="list" qualifier="builtins" value="%5B%28%5B  96   96  193    4 7786   62   71   27  234   64  329   11   96    1%2C  631  314 7197 1822 1913 1025  177   96  104    4  105  106 4180   11%2C  264   11    1 1704  999   13 6634%5D%2C array%280%29%29%2C %28%5B   96   288     7  1167    71     3  1063   223   281    83 17240  2121%2C   268   609   140     7  1290    24 10395  1482    96 20524    98   958%2C 12462  2274   731   398     7    70    96     7 17240   785  2121   268%2C     7  3599    58    77  6634%5D%2C array%280%29%29%2C %28%5B  96    1   18 1829   27    0   21   96   15   88 5135   61    0   36%2C   61   96   13  104  190 4625   98   40   83 1723   61   96 1612   96%2C   96  552  200   81 1917    7   21  307   96 1050   96  776  200  172%2C   11  674 2299   96   96 1605   47   33 3251   15 3454  268    7  117%2C   24   33 5138%5D%2C array%281%29%29%2C %28%5B  96  193    4   18 1295   13  746 3874  866   96 3593 4996   77  128%2C 1243  889   96  128 3519  200  723 1766%5D%2C array%280%29%29%2C %28%5B  96  124  310  288    0   21   96    1  310   64   33 4357    6   96%2C  408  409 1706  151   62   11 ..." isContainer="True" shape="20000" />
<var name="dev_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x0000024A86243350&gt;" isContainer="True" shape="20000" />
<var name="device" type="device" qualifier="torch" value="device%28type=%27cpu%27%29" isContainer="True" />
<var name="embedding_size" type="int" qualifier="builtins" value="50" />
<var name="embeddings_file_path" type="str" qualifier="builtins" value="./models/1st_model_embeddings_state_dict.pt" />
<var name="epochs" type="int" qualifier="builtins" value="1" />
<var name="f" type="BufferedReader" qualifier="_io" value="%3C_io.BufferedReader name=%27./models/1st_model_index_to_word.pkl%27&gt;" isContainer="True" />
<var name="f1_scores" type="list" qualifier="builtins" value="%5B%5D" isContainer="True" shape="0" />
<var name="index_to_word" type="dict" qualifier="builtins" value="%7B0%3A %27this%27%2C 1%3A %27was%27%2C 2%3A %27bought%27%2C 3%3A %27as%27%2C 4%3A %27a%27%2C 5%3A %27gift%27%2C 6%3A %27but%27%2C 7%3A %27the%27%2C 8%3A %27person%27%2C 9%3A %27who%27%2C 10%3A %27got%27%2C 11%3A %27it%27%2C 12%3A %27loved%27%2C 13%3A %27and%27%2C 14%3A %27they%27%2C 15%3A %27will%27%2C 16%3A %27use%27%2C 17%3A %27soon%27%2C 18%3A %27very%27%2C 19%3A %27well%27%2C 20%3A %27written%27%2C 21%3A %27book%27%2C 22%3A %27on%27%2C 23%3A %27period%27%2C 24%3A %27of%27%2C 25%3A %27world%27%2C 26%3A %27history%27%2C 27%3A %27with%27%2C 28%3A %27which%27%2C 29%3A %27i%27%2C 30%3A %27am%27%2C 31%3A %27familiar%27%2C 32%3A %27despite%27%2C 33%3A %27my%27%2C 34%3A %27familiarity%27%2C 35%3A %27subject%27%2C 36%3A %27area%27%2C 37%3A %27learned%27%2C 38%3A %27lot%27%2C 39%3A %27new%27%2C 40%3A %27information%27%2C 41%3A %27also%27%2C 42%3A %27one%27%2C 43%3A %27best%27%2C 44%3A %27concise%27%2C 45%3A %27descriptions%27%2C 46%3A %27wwii%27%2C 47%3A %27that%27%2C 48%3A %27have%27%2C 49%3A %27ever%27%2C 50%3A %27read%27%2C 51%3A %27thought%27%2C 52%3A %27provoking%27%2C 53%3A %27hot%27%2C 54%3A %27cross%27%2C 55%3A %27buns%27%2C 56%3A %27contains%27%2C 57%3A %27cast%27%2C 58%3A %27characters%27%2C 59%3A %27you%27%2C 60%3A %27fall%27%2C 61%3A %27in%27%2C 62%3A %27love%27%2C 63%3A %27want%27%2C 64%3A %27to%27%2C 65%3A %27hang%27%2C 66%3A %27out%27%2C 67%3A %27has%27%2C 68%3A %27few%27%2C 69%3A %27different%27%2C 70%3A %27plot%27%2C 71%3A %27story%27%2C 72%3A %27lines%27%2C 73%3A %27intersect%27%2C 74%3A %27unexpected%27%2C 75%3A %27ways%27%2C 76%3A %27stories%27%2C 77%3A %27are%27%2C 78%3A %27full%27%2C 79%3A %27h..." isContainer="True" shape="54767" />
<var name="index_to_word_file" type="str" qualifier="builtins" value="./models/1st_model_index_to_word.pkl" />
<var name="learning_rate" type="float" qualifier="builtins" value="5e-05" />
<var name="loss_function" type="BCELoss" qualifier="torch.nn.modules.loss" value="BCELoss%28%29" isContainer="True" />
<var name="losses" type="list" qualifier="builtins" value="%5B%5D" isContainer="True" shape="0" />
<var name="lr" type="float" qualifier="builtins" value="5e-05" />
<var name="max_steps" type="int" qualifier="builtins" value="1000000000" />
<var name="model" type="DocumentAttentionClassifier" qualifier="__main__" value="DocumentAttentionClassifier%28%0A  %28embeddings%29%3A Embedding%2854767%2C 50%29%0A  %28linear%29%3A Linear%28in_features=200%2C out_features=1%2C bias=True%29%0A%29" isContainer="True" />
<var name="num_heads" type="int" qualifier="builtins" value="4" />
<var name="optimizer" type="AdamW" qualifier="torch.optim.adamw" value="AdamW %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 5e-05%0A    maximize%3A False%0A    weight_decay%3A 0.01%0A%29" isContainer="True" />
<var name="patience" type="int" qualifier="builtins" value="100000" />
<var name="sent_dev_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0      Picturing Perfect is a sappy love story with l...      0%5D %5B1      Seems like the same story as any other series ...      0%5D %5B2      I was very pleased with this book. I will be t...      1%5D %5B3      It is a very light and rather silly novel. The...      0%5D %5B4      I did not like this book. It was not to my tas...      0%5D %5B...                                                  ...    ...%5D %5B19995  Great content%2C the story is fantastic%2C but sho...      1%5D %5B19996  Typical book club book... Incest%2C child abuse%2C...      0%5D %5B19997  Fascinating book. Shorter than most Russell Ba...      1%5D %5B19998  This book is not well-organized%2C which is impo...      0%5D %5B19999  CAN WE CALL THIS A CLASSIC OF THE GENRE%3F I THI...      1%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_test_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27text%27%5D %5B0            0  Really sad review as I absolutely loved the fi...%5D %5B1            1  Excellent content%2C perfect for Christians who ...%5D %5B2            2  This is an okay book if you need advice on bud...%5D %5B3            3  This is one book you can%27t put down%21 This book...%5D %5B4            4  There were to many names that I had no idenity...%5D %5B...        ...                                                ...%5D %5B19995    19995  I found this book to be a very entertaining an...%5D %5B19996    19996  Wow%2C what a Middle School%21 Read this book your...%5D %5B19997    19997  Not what I expected. Not enough about circular...%5D %5B19998    19998  I like Joanne Fluke%27s mystery series starring ...%5D %5B19999    19999  Grow some chickens%2C improve your land%2C make al...%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_train_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0       It was what I needed. There was no markings or...      1%5D %5B1       A cute little book. My wife gets the family wa...      1%5D %5B2       I bought these for 40.00 and for the price I j...      0%5D %5B3       It was interesting and enjoyable reading. Shor...      1%5D %5B4       A perfect ending to an amazing story. This was...      1%5D %5B...                                                   ...    ...%5D %5B159995  After reading every book Stephen King has to o...      1%5D %5B159996  Baby boomers who are experiencing %22aging eyeba...      0%5D %5B159997  Must read%2C must have%2C must read again. This bo...      1%5D %5B159998  Dr. Chopra%27s books are always enlightening and...      1%5D %5B159999  Boooring%21%21 Just enough to keep you intrigued f...      0%5D %5B%5D" isContainer="True" shape="(160000, 2)" />
<var name="steps_since_improvement" type="int" qualifier="builtins" value="0" />
<var name="test_data" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   890%2C   628%2C     3%2C    96%2C  1139%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4112%2C     4%2C  6634...8720%2C  7772%2C     7%2C%0A         2106%2C   766%2C     7%2C   731%2C    42%2C   335%2C   385%2C   515%2C    98%2C   631%2C%0A          102%2C    13%2C   451%5D%29%2C tensor%28%5B  96%2C 1680%2C 1434%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2683%2C  314%2C  808%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1900%2C   61%2C    7%2C  696%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  5005%2C    21%2C   335%2C    59%2C   663%2C   635%2C    22%2C%0A         8435%2C    13%2C  4026%2C    96%2C   360%2C   345...  723%2C    96%2C   309%2C   498%2C    94%2C   723%2C  1112%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 21503%2C    64%2C     7%2C  2917%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  507%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 9205%2C   59%2C   ...7%2C    4%2C  772%2C   13%2C%0A        3626%2C  680%2C   86%2C  681%2C   64%2C  287%2C  193%2C  538%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  682%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_data_path" type="str" qualifier="builtins" value="sentiment.test.csv" />
<var name="test_list" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   890%2C   628%2C     3%2C    96%2C  1139%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4112%2C     4%2C  6634...8720%2C  7772%2C     7%2C%0A         2106%2C   766%2C     7%2C   731%2C    42%2C   335%2C   385%2C   515%2C    98%2C   631%2C%0A          102%2C    13%2C   451%5D%29%2C tensor%28%5B  96%2C 1680%2C 1434%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2683%2C  314%2C  808%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1900%2C   61%2C    7%2C  696%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  5005%2C    21%2C   335%2C    59%2C   663%2C   635%2C    22%2C%0A         8435%2C    13%2C  4026%2C    96%2C   360%2C   345...  723%2C    96%2C   309%2C   498%2C    94%2C   723%2C  1112%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 21503%2C    64%2C     7%2C  2917%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  507%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 9205%2C   59%2C   ...7%2C    4%2C  772%2C   13%2C%0A        3626%2C  680%2C   86%2C  681%2C   64%2C  287%2C  193%2C  538%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  682%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="tokenizer" type="RegexpTokenizer" qualifier="nltk.tokenize.regexp" value="RegexpTokenizer%28pattern=%27%5C%5Cw%2B%27%2C gaps=False%2C discard_empty=True%2C flags=re.UNICODE%7Cre.MULTILINE%7Cre.DOTALL%29" isContainer="True" />
<var name="train_data_path" type="str" qualifier="builtins" value="sentiment.train.csv" />
<var name="train_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x0000024A85E166C0&gt;" isContainer="True" shape="160000" />
<var name="train_list" type="list" qualifier="builtins" value="%5B%28%5B   96     1   287    96   248    96     1   204 17795   291   110    24%2C    47    96   193   306    64    50    13    93%5D%2C array%281%29%29%2C %28%5B  96  889  234   21   96 1198 2498    7  696 1946 1941  268   96   96%2C   85 1699   96   21   67    7 1167  889 1589%5D%2C array%281%29%29%2C %28%5B  96    2  320   98 3619 7186   13   98    7  366   96  128 5856   14%2C  151   88  105 9931   96   88   96  123   59 1971  320   59  175   11%2C   15  128   88   96  140  105 9931 5948%5D%2C array%280%29%29%2C %28%5B  96    1  358   13  333  341   96   76 1786   83   96  241 1646 3108%2C   24   96   13 1203  733   61  305   95   96   76 1433   48    4  702%2C  117   96  115    4  487   24   96   96  356   76   96   62%5D%2C array%281%29%29%2C %28%5B  96 1434 1240   64  314  682   71   96    1  314 1138 1565 1566   98%2C  265 1920   96    1 7590   98 3244   87  310 1599  335   14  151  540%2C  232  287   14 1920  249    0    1    4  617  713   96   12   11%5D%2C array%281%29%29%2C %28%5B   96    64    88   314  1785  3235    24   345    24    96    26    96%2C    13  1185   341    ..." isContainer="True" shape="160000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x0000024A85E625A0&gt;" isContainer="True" shape="160000" />
<var name="vocab_size" type="int" qualifier="builtins" value="54767" />
<var name="word_to_index" type="dict" qualifier="builtins" value="%7B%270%27%3A 7011%2C %2700%27%3A 7186%2C %27000%27%3A 7903%2C %27001%27%3A 49428%2C %27003%27%3A 50032%2C %27005%27%3A 54582%2C %27007%27%3A 31481%2C %2700am%27%3A 28695%2C %2701%27%3A 27136%2C %2702%27%3A 33916%2C %2703%27%3A 11477%2C %27039%27%3A 47815%2C %2704%27%3A 8254%2C %27047%27%3A 52250%2C %2705%27%3A 37506%2C %2706%27%3A 34657%2C %2707%27%3A 7221%2C %2708%27%3A 26910%2C %2709%27%3A 24096%2C %270f%27%3A 47448%2C %271%27%3A 797%2C %2710%27%3A 1695%2C %27100%27%3A 874%2C %271000%27%3A 11350%2C %2710000%27%3A 35776%2C %27100000%27%3A 25201%2C %271000000%27%3A 29683%2C %271000s%27%3A 22349%2C %271000x%27%3A 40244%2C %271001%27%3A 8102%2C %27100k%27%3A 50021%2C %27100lbs%27%3A 20517%2C %27100s%27%3A 27884%2C %27100th%27%3A 884%2C %27100x%27%3A 38961%2C %27101%27%3A 7160%2C %27101st%27%3A 29946%2C %27102%27%3A 33226%2C %27103%27%3A 35688%2C %271031%27%3A 52765%2C %27104%27%3A 35687%2C %27105%27%3A 30396%2C %271050%27%3A 44952%2C %27106%27%3A 12373%2C %271066%27%3A 41729%2C %27107%27%3A 737%2C %27108%27%3A 16005%2C %27109%27%3A 41553%2C %271092%27%3A 53073%2C %2710am%27%3A 28786%2C %2710k%27%3A 17404%2C %2710min%27%3A 53254%2C %2710th%27%3A 26843%2C %2710x%27%3A 24694%2C %2710years%27%3A 37449%2C %2710yr%27%3A 32636%2C %2710yrs%27%3A 54312%2C %2711%27%3A 3883%2C %27110%27%3A 20178%2C %271100%27%3A 44794%2C %27111%27%3A 20179%2C %27112%27%3A 11541%2C %27113%27%3A 38280%2C %27114%27%3A 26876%2C %27115%27%3A 16004%2C %27116%27%3A 20922%2C %27117%27%3A 51318%2C %27118%27%3A 23682%2C %27119%27%3A 48641%2C %2711g%27%3A 42267%2C %2711pm%27%3A 284..." isContainer="True" shape="54767" />
<var name="word_to_index_file" type="str" qualifier="builtins" value="./models/1st_model_word_to_index.pkl" />
</xml>
Step 500, Avg Loss: 0.73669300994277
Step 1000, Avg Loss: 0.7272139386683703
Step 1500, Avg Loss: 0.7045738829225302
Step 2000, Avg Loss: 0.7268919925391674
Step 2500, Avg Loss: 0.707372724443674
Step 3000, Avg Loss: 0.709195172816515
Step 3500, Avg Loss: 0.7224807038083673
Step 4000, Avg Loss: 0.6997374452203512
Step 4500, Avg Loss: 0.6910611676126719
Step 5000, Avg Loss: 0.6965497943758965
-----Step 5000, F1 Score: 0.5510268231349539-----
Step 5500, Avg Loss: 0.6808420803695917
Step 6000, Avg Loss: 0.6910826377719641
Step 6500, Avg Loss: 0.6919247459173202
Step 7000, Avg Loss: 0.6650436120778322
Step 7500, Avg Loss: 0.6807743585631252
Step 8000, Avg Loss: 0.6795846753418445
Step 8500, Avg Loss: 0.6971616626754403
Step 9000, Avg Loss: 0.6954638570472598
Step 9500, Avg Loss: 0.6666544487550855
Step 10000, Avg Loss: 0.6598847579360009
-----Step 10000, F1 Score: 0.6074732914174718-----
Step 10500, Avg Loss: 0.6903803596049547
Step 11000, Avg Loss: 0.6605892859622836
Step 11500, Avg Loss: 0.6646740023978055
Step 12000, Avg Loss: 0.6692084616646171
Step 12500, Avg Loss: 0.6481632799208165
Step 13000, Avg Loss: 0.6528801095187664
Step 13500, Avg Loss: 0.6643355054408312
Step 14000, Avg Loss: 0.6659310574531555
Step 14500, Avg Loss: 0.6951596502549946
Step 15000, Avg Loss: 0.693802834674716
-----Step 15000, F1 Score: 0.6352941176470588-----
Step 15500, Avg Loss: 0.675558649122715
Step 16000, Avg Loss: 0.6713350623548031
Step 16500, Avg Loss: 0.6574761789888144
Step 17000, Avg Loss: 0.6686000064015388
Step 17500, Avg Loss: 0.6691205003559589
Step 18000, Avg Loss: 0.6782835072949529
Step 18500, Avg Loss: 0.6560225018560887
Step 19000, Avg Loss: 0.6576138383820653
Step 19500, Avg Loss: 0.6604282735884189
Step 20000, Avg Loss: 0.6421295806244016
-----Step 20000, F1 Score: 0.6543988421379097-----
Step 20500, Avg Loss: 0.6648684914857149
Step 21000, Avg Loss: 0.665509123776108
Step 21500, Avg Loss: 0.6303441652953625
Step 22000, Avg Loss: 0.6470306032747031
Step 22500, Avg Loss: 0.658343497902155
Step 23000, Avg Loss: 0.6397511106394231
Step 23500, Avg Loss: 0.6582917216718197
Step 24000, Avg Loss: 0.6372774410545826
Step 24500, Avg Loss: 0.6484236812591553
Step 25000, Avg Loss: 0.6227297076657414
-----Step 25000, F1 Score: 0.678607879159012-----
Step 25500, Avg Loss: 0.6277576473224908
Step 26000, Avg Loss: 0.6401359843239188
Step 26500, Avg Loss: 0.6173368785530329
Step 27000, Avg Loss: 0.6505686109736561
Step 27500, Avg Loss: 0.6282026564627886
Step 28000, Avg Loss: 0.6362372853308916
Step 28500, Avg Loss: 0.6240832544565201
Step 29000, Avg Loss: 0.6662240190804005
Step 29500, Avg Loss: 0.6381332209128886
Step 30000, Avg Loss: 0.6450540106892586
-----Step 30000, F1 Score: 0.6871996279262054-----
Step 30500, Avg Loss: 0.6208684663772583
Step 31000, Avg Loss: 0.6225422757640481
Step 31500, Avg Loss: 0.6211826317906379
Step 32000, Avg Loss: 0.6352018896639348
Step 32500, Avg Loss: 0.6281101394295693
Step 33000, Avg Loss: 0.6080741261411459
Step 33500, Avg Loss: 0.6457099880687892
Step 34000, Avg Loss: 0.6325918104276061
Step 34500, Avg Loss: 0.6271381211653352
Step 35000, Avg Loss: 0.6140276567302644
-----Step 35000, F1 Score: 0.704988171339407-----
Step 35500, Avg Loss: 0.6322030770853162
Step 36000, Avg Loss: 0.6208495188094676
Step 36500, Avg Loss: 0.6226274361014366
Step 37000, Avg Loss: 0.6126567980498075
Step 37500, Avg Loss: 0.6231709261387587
Step 38000, Avg Loss: 0.6358848492838443
Step 38500, Avg Loss: 0.6099340260513126
Step 39000, Avg Loss: 0.6207488988228143
Step 39500, Avg Loss: 0.6030453638657928
Step 40000, Avg Loss: 0.5782693940997123
-----Step 40000, F1 Score: 0.7101849014199612-----
Step 40500, Avg Loss: 0.5987571188658476
Step 41000, Avg Loss: 0.6262660259036348
Step 41500, Avg Loss: 0.6130386796605307
Step 42000, Avg Loss: 0.6029637270094826
Step 42500, Avg Loss: 0.648186957379803
Step 43000, Avg Loss: 0.5846702577322721
Step 43500, Avg Loss: 0.6007614613249898
Step 44000, Avg Loss: 0.6207933041602373
Step 44500, Avg Loss: 0.6238865937367082
Step 45000, Avg Loss: 0.610596625784412
-----Step 45000, F1 Score: 0.7224745181148451-----
Step 45500, Avg Loss: 0.5839840639606119
Step 46000, Avg Loss: 0.6157456542123109
Step 46500, Avg Loss: 0.5807998347207903
Step 47000, Avg Loss: 0.5956841270197183
Step 47500, Avg Loss: 0.6064332385659218
Step 48000, Avg Loss: 0.5919254254028201
Step 48500, Avg Loss: 0.5929257259070874
Step 49000, Avg Loss: 0.57127176900208
Step 49500, Avg Loss: 0.5942640857640654
Step 50000, Avg Loss: 0.5900239245444536
-----Step 50000, F1 Score: 0.7304971605061273-----
Step 50500, Avg Loss: 0.5838732724320144
Step 51000, Avg Loss: 0.5390610498636961
Step 51500, Avg Loss: 0.5868357584178447
Step 52000, Avg Loss: 0.591638725893572
Step 52500, Avg Loss: 0.5816537167802454
Step 53000, Avg Loss: 0.5599300715532154
Step 53500, Avg Loss: 0.5851490638116374
Step 54000, Avg Loss: 0.5824552634842693
Step 54500, Avg Loss: 0.5776820774488151
Step 55000, Avg Loss: 0.573729142203927
-----Step 55000, F1 Score: 0.7348242811501597-----
Step 55500, Avg Loss: 0.5866514935977757
Step 56000, Avg Loss: 0.5845446338746697
Step 56500, Avg Loss: 0.562676999669522
Step 57000, Avg Loss: 0.5735445244954899
Step 57500, Avg Loss: 0.6158030879199505
Step 58000, Avg Loss: 0.5570869868397713
Step 58500, Avg Loss: 0.5589689388656989
Step 59000, Avg Loss: 0.539646323531866
Step 59500, Avg Loss: 0.5861669002417474
Step 60000, Avg Loss: 0.5915137613504193
-----Step 60000, F1 Score: 0.7423425206591123-----
Step 60500, Avg Loss: 0.5845708616776392
Step 61000, Avg Loss: 0.5553631817577407
Step 61500, Avg Loss: 0.6154221103796735
Step 62000, Avg Loss: 0.5633379127439111
Step 62500, Avg Loss: 0.5666078840531409
Step 63000, Avg Loss: 0.5588230301477015
Step 63500, Avg Loss: 0.5490658469777554
Step 64000, Avg Loss: 0.5891733847931028
Step 64500, Avg Loss: 0.5746885764971376
Step 65000, Avg Loss: 0.5232820598334074
-----Step 65000, F1 Score: 0.7419761172973245-----
Step 65500, Avg Loss: 0.5670915789743886
Step 66000, Avg Loss: 0.5729483902752399
Step 66500, Avg Loss: 0.5932147303540259
Step 67000, Avg Loss: 0.5547113285884261
Step 67500, Avg Loss: 0.5511506742872297
Step 68000, Avg Loss: 0.5303498809430748
Step 68500, Avg Loss: 0.5519404471999733
Step 69000, Avg Loss: 0.5526040571331978
Step 69500, Avg Loss: 0.580447638168931
Step 70000, Avg Loss: 0.5608569240476936
-----Step 70000, F1 Score: 0.7451794119135584-----
Step 70500, Avg Loss: 0.5701589281018823
Step 71000, Avg Loss: 0.5522154019959271
Step 71500, Avg Loss: 0.5567250043163076
Step 72000, Avg Loss: 0.5744741363949143
Step 72500, Avg Loss: 0.5858635842846707
Step 73000, Avg Loss: 0.5551728907227517
Step 73500, Avg Loss: 0.577981973838061
Step 74000, Avg Loss: 0.5516328630000353
Step 74500, Avg Loss: 0.5424397636074573
Step 75000, Avg Loss: 0.5480107505326159
-----Step 75000, F1 Score: 0.7493599718889614-----
Step 75500, Avg Loss: 0.5477599198606913
Step 76000, Avg Loss: 0.5432918876744807
Step 76500, Avg Loss: 0.5932426822334528
Step 77000, Avg Loss: 0.546561709061265
Step 77500, Avg Loss: 0.5880542842750438
Step 78000, Avg Loss: 0.5361718171574176
Step 78500, Avg Loss: 0.5520011161565781
Step 79000, Avg Loss: 0.5586217746399343
Step 79500, Avg Loss: 0.5452872438104823
Step 80000, Avg Loss: 0.572538449820131
-----Step 80000, F1 Score: 0.7520669489816495-----
Step 80500, Avg Loss: 0.5632914643418044
Step 81000, Avg Loss: 0.5434970598965884
Step 81500, Avg Loss: 0.5643947874624282
Step 82000, Avg Loss: 0.5098568926658481
Step 82500, Avg Loss: 0.5194878819324076
Step 83000, Avg Loss: 0.5528969521962572
Step 83500, Avg Loss: 0.5358203413328156
Step 84000, Avg Loss: 0.5425008974326775
Step 84500, Avg Loss: 0.5368243413977325
Step 85000, Avg Loss: 0.5681295931637287
-----Step 85000, F1 Score: 0.7544257830231502-----
Step 85500, Avg Loss: 0.5288577664904296
Step 86000, Avg Loss: 0.5406266112411394
Step 86500, Avg Loss: 0.5795674149598926
Step 87000, Avg Loss: 0.5405395314265042
Step 87500, Avg Loss: 0.5262410075450316
Step 88000, Avg Loss: 0.5219493375564925
Step 88500, Avg Loss: 0.5315007220311673
Step 89000, Avg Loss: 0.5465117100588978
Step 89500, Avg Loss: 0.583294184166938
Step 90000, Avg Loss: 0.5354255850965856
-----Step 90000, F1 Score: 0.7611556347086178-----
Step 90500, Avg Loss: 0.5145522235166281
Step 91000, Avg Loss: 0.5352775484041776
Step 91500, Avg Loss: 0.5294264820702373
Step 92000, Avg Loss: 0.5372491815630347
Step 92500, Avg Loss: 0.5667649318771437
Step 93000, Avg Loss: 0.5253807772435248
Step 93500, Avg Loss: 0.5087750413748435
Step 94000, Avg Loss: 0.5092923424807377
Step 94500, Avg Loss: 0.5359135207068175
Step 95000, Avg Loss: 0.48249003583565353
-----Step 95000, F1 Score: 0.7573666448395708-----
Step 95500, Avg Loss: 0.5103577266260981
Step 96000, Avg Loss: 0.5404877432967768
Step 96500, Avg Loss: 0.5332037443071603
Step 97000, Avg Loss: 0.514650900799781
Step 97500, Avg Loss: 0.5761540200805757
Step 98000, Avg Loss: 0.5131524346331134
Step 98500, Avg Loss: 0.539846892418398
Step 99000, Avg Loss: 0.5106143890293315
Step 99500, Avg Loss: 0.504854685387807
Step 100000, Avg Loss: 0.5301927975220606
-----Step 100000, F1 Score: 0.7623618819059047-----
Step 100500, Avg Loss: 0.5569210278298706
Step 101000, Avg Loss: 0.5022290190439671
Step 101500, Avg Loss: 0.4791781516224146
Step 102000, Avg Loss: 0.5286334001515061
Step 102500, Avg Loss: 0.5589035777561366
Step 103000, Avg Loss: 0.4831398998890072
Step 103500, Avg Loss: 0.5036672922130674
Step 104000, Avg Loss: 0.5257089538434521
Step 104500, Avg Loss: 0.5023469375576824
Step 105000, Avg Loss: 0.5190675606317818
-----Step 105000, F1 Score: 0.7661282330789813-----
Step 105500, Avg Loss: 0.5317880492910044
Step 106000, Avg Loss: 0.5122644354728982
Step 106500, Avg Loss: 0.5333802602849901
Step 107000, Avg Loss: 0.5247215140699409
Step 107500, Avg Loss: 0.5343364573121071
Step 108000, Avg Loss: 0.5047867552433163
Step 108500, Avg Loss: 0.5086337272934616
Step 109000, Avg Loss: 0.5210764278237184
Step 109500, Avg Loss: 0.4783309514280409
Step 110000, Avg Loss: 0.542989155460149
-----Step 110000, F1 Score: 0.76747077841333-----
Step 110500, Avg Loss: 0.5359455083723879
Step 111000, Avg Loss: 0.5136311421617865
Step 111500, Avg Loss: 0.5065100364969112
Step 112000, Avg Loss: 0.5295179833238944
Step 112500, Avg Loss: 0.5440076051539509
Step 113000, Avg Loss: 0.47725939711555837
Step 113500, Avg Loss: 0.48495365953911096
Step 114000, Avg Loss: 0.5210765095818788
Step 114500, Avg Loss: 0.5162099675359204
Step 115000, Avg Loss: 0.5578336863666773
-----Step 115000, F1 Score: 0.7679983979172925-----
Step 115500, Avg Loss: 0.491217000179342
Step 116000, Avg Loss: 0.49809187357872725
Step 116500, Avg Loss: 0.47571095848362893
Step 117000, Avg Loss: 0.5059452420659363
Step 117500, Avg Loss: 0.4891308188769035
Step 118000, Avg Loss: 0.5189061271530517
Step 118500, Avg Loss: 0.5279893633027531
Step 119000, Avg Loss: 0.47357772540301085
Step 119500, Avg Loss: 0.488141479649581
Step 120000, Avg Loss: 0.5023180956915021
-----Step 120000, F1 Score: 0.7685858585858586-----
Step 120500, Avg Loss: 0.4983636412416672
Step 121000, Avg Loss: 0.48148200927395374
Step 121500, Avg Loss: 0.4569945463985205
Step 122000, Avg Loss: 0.4814523929785937
Step 122500, Avg Loss: 0.498267377528362
Step 123000, Avg Loss: 0.5052165088048205
Step 123500, Avg Loss: 0.5122490064906887
Step 124000, Avg Loss: 0.5317719050124288
Step 124500, Avg Loss: 0.47137210973957555
Step 125000, Avg Loss: 0.48345059037249305
-----Step 125000, F1 Score: 0.7693858013805613-----
Step 125500, Avg Loss: 0.52331076418655
Step 126000, Avg Loss: 0.4629741127815505
Step 126500, Avg Loss: 0.4729263427092228
Step 127000, Avg Loss: 0.4807772878180258
Step 127500, Avg Loss: 0.5376947623938322
Step 128000, Avg Loss: 0.4977651421576738
Step 128500, Avg Loss: 0.49022689436376093
Step 129000, Avg Loss: 0.45363537475734483
Step 129500, Avg Loss: 0.472599930445198
Step 130000, Avg Loss: 0.4803470942303538
-----Step 130000, F1 Score: 0.7695892214962265-----
Step 130500, Avg Loss: 0.4512374498285353
Step 131000, Avg Loss: 0.4915230228132568
Step 131500, Avg Loss: 0.5227365228388225
Step 132000, Avg Loss: 0.49612380885565655
Step 132500, Avg Loss: 0.44822283200034874
Step 133000, Avg Loss: 0.5239826515233145
Step 133500, Avg Loss: 0.5000217297561467
Step 134000, Avg Loss: 0.49026159049523993
Step 134500, Avg Loss: 0.500833036372438
Step 135000, Avg Loss: 0.5597662575785071
-----Step 135000, F1 Score: 0.7750186613585469-----
Step 135500, Avg Loss: 0.42822061059810224
Step 136000, Avg Loss: 0.4800616074828431
Step 136500, Avg Loss: 0.46859175175847484
Step 137000, Avg Loss: 0.50126539182337
Step 137500, Avg Loss: 0.4693046882972121
Step 138000, Avg Loss: 0.47690504703391345
Step 138500, Avg Loss: 0.4837768633319065
Step 139000, Avg Loss: 0.4775322430199012
Step 139500, Avg Loss: 0.5197482374478132
Step 140000, Avg Loss: 0.503449065358378
-----Step 140000, F1 Score: 0.7751062234441389-----
Step 140500, Avg Loss: 0.4854326258208603
Step 141000, Avg Loss: 0.4726869757453824
Step 141500, Avg Loss: 0.5113800911467405
Step 142000, Avg Loss: 0.5350040038234147
Step 142500, Avg Loss: 0.4722761745499447
Step 143000, Avg Loss: 0.4763129500960931
Step 143500, Avg Loss: 0.5698310548905283
Step 144000, Avg Loss: 0.49744134228676556
Step 144500, Avg Loss: 0.5175417659431696
Step 145000, Avg Loss: 0.5086468996806071
-----Step 145000, F1 Score: 0.7739059967585089-----
Step 145500, Avg Loss: 0.5172941837152466
Step 146000, Avg Loss: 0.4770533720399253
Step 146500, Avg Loss: 0.47922450209915407
Step 147000, Avg Loss: 0.4952606174122775
Step 147500, Avg Loss: 0.4636481567490846
Step 148000, Avg Loss: 0.4672065413314849
Step 148500, Avg Loss: 0.4249592371145263
Step 149000, Avg Loss: 0.4795007121913368
Step 149500, Avg Loss: 0.45197629937250167
Step 150000, Avg Loss: 0.4911548930574208
-----Step 150000, F1 Score: 0.7785793006712755-----
Step 150500, Avg Loss: 0.4780516113527119
Step 151000, Avg Loss: 0.47287157468125224
Step 151500, Avg Loss: 0.49786735081020744
Step 152000, Avg Loss: 0.48871770829614253
Step 152500, Avg Loss: 0.4696113268820336
Step 153000, Avg Loss: 0.5089813158251345
Step 153500, Avg Loss: 0.47886024233931673
Step 154000, Avg Loss: 0.49699742305651307
Step 154500, Avg Loss: 0.5369307987765496
Step 155000, Avg Loss: 0.5009653136702255
-----Step 155000, F1 Score: 0.7822903784318576-----
Step 155500, Avg Loss: 0.48036166597902774
Step 156000, Avg Loss: 0.45645005945675077
Step 156500, Avg Loss: 0.44425817534443923
Step 157000, Avg Loss: 0.5433861659081886
Step 157500, Avg Loss: 0.48269653849909083
Step 158000, Avg Loss: 0.5008811746167776
Step 158500, Avg Loss: 0.48489194226218385
Step 159000, Avg Loss: 0.5106705771097914
Step 159500, Avg Loss: 0.48436303578229856
Step 160000, Avg Loss: 0.5333949066884816
-----Step 160000, F1 Score: 0.782838613120479-----
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="RegexpTokenizer" type="ABCMeta" qualifier="abc" value="%3Cclass %27nltk.tokenize.regexp.RegexpTokenizer%27&gt;" isContainer="True" />
<var name="avg_loss" type="float" qualifier="builtins" value="0.5333949066884816" />
<var name="batch_size" type="int" qualifier="builtins" value="1" />
<var name="best_f1" type="float64" qualifier="numpy" value="0.7829681274900399" shape="()" />
<var name="dev_data_path" type="str" qualifier="builtins" value="sentiment.dev.csv" />
<var name="dev_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x0000024A83FAB740&gt;" isContainer="True" shape="20000" />
<var name="dev_f1" type="float64" qualifier="numpy" value="0.782838613120479" shape="()" />
<var name="dev_list" type="list" qualifier="builtins" value="%5B%28%5B  96   96  193    4 7786   62   71   27  234   64  329   11   96    1%2C  631  314 7197 1822 1913 1025  177   96  104    4  105  106 4180   11%2C  264   11    1 1704  999   13 6634%5D%2C array%280%29%29%2C %28%5B   96   288     7  1167    71     3  1063   223   281    83 17240  2121%2C   268   609   140     7  1290    24 10395  1482    96 20524    98   958%2C 12462  2274   731   398     7    70    96     7 17240   785  2121   268%2C     7  3599    58    77  6634%5D%2C array%280%29%29%2C %28%5B  96    1   18 1829   27    0   21   96   15   88 5135   61    0   36%2C   61   96   13  104  190 4625   98   40   83 1723   61   96 1612   96%2C   96  552  200   81 1917    7   21  307   96 1050   96  776  200  172%2C   11  674 2299   96   96 1605   47   33 3251   15 3454  268    7  117%2C   24   33 5138%5D%2C array%281%29%29%2C %28%5B  96  193    4   18 1295   13  746 3874  866   96 3593 4996   77  128%2C 1243  889   96  128 3519  200  723 1766%5D%2C array%280%29%29%2C %28%5B  96  124  310  288    0   21   96    1  310   64   33 4357    6   96%2C  408  409 1706  151   62   11 ..." isContainer="True" shape="20000" />
<var name="dev_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x0000024A86243350&gt;" isContainer="True" shape="20000" />
<var name="device" type="device" qualifier="torch" value="device%28type=%27cpu%27%29" isContainer="True" />
<var name="embedding_size" type="int" qualifier="builtins" value="50" />
<var name="embeddings_file_path" type="str" qualifier="builtins" value="./models/1st_model_embeddings_state_dict.pt" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="1" />
<var name="f" type="BufferedReader" qualifier="_io" value="%3C_io.BufferedReader name=%27./models/1st_model_index_to_word.pkl%27&gt;" isContainer="True" />
<var name="f1_scores" type="list" qualifier="builtins" value="%5B0.5787778473651248%2C 0.5846965227496201%2C 0.580309936189608%2C 0.5872712373668396%2C 0.5667680131267796%2C 0.5571108343711083%2C 0.565273248344698%2C 0.5496804782519068%2C 0.5477993003706991%2C 0.5510268231349539%2C 0.5522269773339673%2C 0.5592008412197687%2C 0.5735347182844477%2C 0.5658881961323048%2C 0.5699899295065458%2C 0.572279228029135%2C 0.5791563012756047%2C 0.5985356612564641%2C 0.6015107137351626%2C 0.6074732914174718%2C 0.6114428622467207%2C 0.6146700015313153%2C 0.6181743673803913%2C 0.6118716689309228%2C 0.6145953303319024%2C 0.6096467826272542%2C 0.6152130032933177%2C 0.6215244740458822%2C 0.6249802454828004%2C 0.6352941176470588%2C 0.6390636279261628%2C 0.6465106857084979%2C 0.6466249554911236%2C 0.6437113402061856%2C 0.6537373737373737%2C 0.6576621811608757%2C 0.6595937248592116%2C 0.6551126516464472%2C 0.6530654408346677%2C 0.6543988421379097%2C 0.6683693910256411%2C 0.6680125164025437%2C 0.6729251972435834%2C 0.670071294938565%2C 0.6732204073401895%2C 0.6718885102487158%2C 0.6696143958868894%2C 0.6727421667007987%2C 0.6772697150430749%2C 0.678607879159012%2C 0.68..." isContainer="True" shape="320" />
<var name="index_to_word" type="dict" qualifier="builtins" value="%7B0%3A %27this%27%2C 1%3A %27was%27%2C 2%3A %27bought%27%2C 3%3A %27as%27%2C 4%3A %27a%27%2C 5%3A %27gift%27%2C 6%3A %27but%27%2C 7%3A %27the%27%2C 8%3A %27person%27%2C 9%3A %27who%27%2C 10%3A %27got%27%2C 11%3A %27it%27%2C 12%3A %27loved%27%2C 13%3A %27and%27%2C 14%3A %27they%27%2C 15%3A %27will%27%2C 16%3A %27use%27%2C 17%3A %27soon%27%2C 18%3A %27very%27%2C 19%3A %27well%27%2C 20%3A %27written%27%2C 21%3A %27book%27%2C 22%3A %27on%27%2C 23%3A %27period%27%2C 24%3A %27of%27%2C 25%3A %27world%27%2C 26%3A %27history%27%2C 27%3A %27with%27%2C 28%3A %27which%27%2C 29%3A %27i%27%2C 30%3A %27am%27%2C 31%3A %27familiar%27%2C 32%3A %27despite%27%2C 33%3A %27my%27%2C 34%3A %27familiarity%27%2C 35%3A %27subject%27%2C 36%3A %27area%27%2C 37%3A %27learned%27%2C 38%3A %27lot%27%2C 39%3A %27new%27%2C 40%3A %27information%27%2C 41%3A %27also%27%2C 42%3A %27one%27%2C 43%3A %27best%27%2C 44%3A %27concise%27%2C 45%3A %27descriptions%27%2C 46%3A %27wwii%27%2C 47%3A %27that%27%2C 48%3A %27have%27%2C 49%3A %27ever%27%2C 50%3A %27read%27%2C 51%3A %27thought%27%2C 52%3A %27provoking%27%2C 53%3A %27hot%27%2C 54%3A %27cross%27%2C 55%3A %27buns%27%2C 56%3A %27contains%27%2C 57%3A %27cast%27%2C 58%3A %27characters%27%2C 59%3A %27you%27%2C 60%3A %27fall%27%2C 61%3A %27in%27%2C 62%3A %27love%27%2C 63%3A %27want%27%2C 64%3A %27to%27%2C 65%3A %27hang%27%2C 66%3A %27out%27%2C 67%3A %27has%27%2C 68%3A %27few%27%2C 69%3A %27different%27%2C 70%3A %27plot%27%2C 71%3A %27story%27%2C 72%3A %27lines%27%2C 73%3A %27intersect%27%2C 74%3A %27unexpected%27%2C 75%3A %27ways%27%2C 76%3A %27stories%27%2C 77%3A %27are%27%2C 78%3A %27full%27%2C 79%3A %27h..." isContainer="True" shape="54767" />
<var name="index_to_word_file" type="str" qualifier="builtins" value="./models/1st_model_index_to_word.pkl" />
<var name="labels" type="Tensor" qualifier="torch" value="tensor%28%5B0.%5D%29" isContainer="True" shape="(1,)" />
<var name="learning_rate" type="float" qualifier="builtins" value="5e-05" />
<var name="loss" type="Tensor" qualifier="torch" value="tensor%280.0553%2C grad_fn=%3CBinaryCrossEntropyBackward0&gt;%29" isContainer="True" shape="()" />
<var name="loss_function" type="BCELoss" qualifier="torch.nn.modules.loss" value="BCELoss%28%29" isContainer="True" />
<var name="losses" type="list" qualifier="builtins" value="%5B0.73669300994277%2C 0.7272139386683703%2C 0.7045738829225302%2C 0.7268919925391674%2C 0.707372724443674%2C 0.709195172816515%2C 0.7224807038083673%2C 0.6997374452203512%2C 0.6910611676126719%2C 0.6965497943758965%2C 0.6808420803695917%2C 0.6910826377719641%2C 0.6919247459173202%2C 0.6650436120778322%2C 0.6807743585631252%2C 0.6795846753418445%2C 0.6971616626754403%2C 0.6954638570472598%2C 0.6666544487550855%2C 0.6598847579360009%2C 0.6903803596049547%2C 0.6605892859622836%2C 0.6646740023978055%2C 0.6692084616646171%2C 0.6481632799208165%2C 0.6528801095187664%2C 0.6643355054408312%2C 0.6659310574531555%2C 0.6951596502549946%2C 0.693802834674716%2C 0.675558649122715%2C 0.6713350623548031%2C 0.6574761789888144%2C 0.6686000064015388%2C 0.6691205003559589%2C 0.6782835072949529%2C 0.6560225018560887%2C 0.6576138383820653%2C 0.6604282735884189%2C 0.6421295806244016%2C 0.6648684914857149%2C 0.665509123776108%2C 0.6303441652953625%2C 0.6470306032747031%2C 0.658343497902155%2C 0.6397511106394231%2C 0.6582917216718197%2C 0.6372774410545826%2C 0.6484236812591553%2C 0.6227297076657414%2C 0.62775..." isContainer="True" shape="320" />
<var name="lr" type="float" qualifier="builtins" value="5e-05" />
<var name="max_steps" type="int" qualifier="builtins" value="1000000000" />
<var name="model" type="DocumentAttentionClassifier" qualifier="__main__" value="DocumentAttentionClassifier%28%0A  %28embeddings%29%3A Embedding%2854767%2C 50%29%0A  %28linear%29%3A Linear%28in_features=200%2C out_features=1%2C bias=True%29%0A%29" isContainer="True" />
<var name="num_heads" type="int" qualifier="builtins" value="4" />
<var name="optimizer" type="AdamW" qualifier="torch.optim.adamw" value="AdamW %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 5e-05%0A    maximize%3A False%0A    weight_decay%3A 0.01%0A%29" isContainer="True" />
<var name="patience" type="int" qualifier="builtins" value="100000" />
<var name="probability" type="Tensor" qualifier="torch" value="tensor%28%5B0.0538%5D%2C grad_fn=%3CSqueezeBackward1&gt;%29" isContainer="True" shape="(1,)" />
<var name="running_loss" type="float" qualifier="builtins" value="0.0" />
<var name="sent_dev_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0      Picturing Perfect is a sappy love story with l...      0%5D %5B1      Seems like the same story as any other series ...      0%5D %5B2      I was very pleased with this book. I will be t...      1%5D %5B3      It is a very light and rather silly novel. The...      0%5D %5B4      I did not like this book. It was not to my tas...      0%5D %5B...                                                  ...    ...%5D %5B19995  Great content%2C the story is fantastic%2C but sho...      1%5D %5B19996  Typical book club book... Incest%2C child abuse%2C...      0%5D %5B19997  Fascinating book. Shorter than most Russell Ba...      1%5D %5B19998  This book is not well-organized%2C which is impo...      0%5D %5B19999  CAN WE CALL THIS A CLASSIC OF THE GENRE%3F I THI...      1%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_test_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27text%27%5D %5B0            0  Really sad review as I absolutely loved the fi...%5D %5B1            1  Excellent content%2C perfect for Christians who ...%5D %5B2            2  This is an okay book if you need advice on bud...%5D %5B3            3  This is one book you can%27t put down%21 This book...%5D %5B4            4  There were to many names that I had no idenity...%5D %5B...        ...                                                ...%5D %5B19995    19995  I found this book to be a very entertaining an...%5D %5B19996    19996  Wow%2C what a Middle School%21 Read this book your...%5D %5B19997    19997  Not what I expected. Not enough about circular...%5D %5B19998    19998  I like Joanne Fluke%27s mystery series starring ...%5D %5B19999    19999  Grow some chickens%2C improve your land%2C make al...%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_train_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0       It was what I needed. There was no markings or...      1%5D %5B1       A cute little book. My wife gets the family wa...      1%5D %5B2       I bought these for 40.00 and for the price I j...      0%5D %5B3       It was interesting and enjoyable reading. Shor...      1%5D %5B4       A perfect ending to an amazing story. This was...      1%5D %5B...                                                   ...    ...%5D %5B159995  After reading every book Stephen King has to o...      1%5D %5B159996  Baby boomers who are experiencing %22aging eyeba...      0%5D %5B159997  Must read%2C must have%2C must read again. This bo...      1%5D %5B159998  Dr. Chopra%27s books are always enlightening and...      1%5D %5B159999  Boooring%21%21 Just enough to keep you intrigued f...      0%5D %5B%5D" isContainer="True" shape="(160000, 2)" />
<var name="step" type="int" qualifier="builtins" value="159999" />
<var name="steps_since_improvement" type="int" qualifier="builtins" value="500" />
<var name="test_data" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   890%2C   628%2C     3%2C    96%2C  1139%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4112%2C     4%2C  6634...8720%2C  7772%2C     7%2C%0A         2106%2C   766%2C     7%2C   731%2C    42%2C   335%2C   385%2C   515%2C    98%2C   631%2C%0A          102%2C    13%2C   451%5D%29%2C tensor%28%5B  96%2C 1680%2C 1434%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2683%2C  314%2C  808%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1900%2C   61%2C    7%2C  696%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  5005%2C    21%2C   335%2C    59%2C   663%2C   635%2C    22%2C%0A         8435%2C    13%2C  4026%2C    96%2C   360%2C   345...  723%2C    96%2C   309%2C   498%2C    94%2C   723%2C  1112%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 21503%2C    64%2C     7%2C  2917%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  507%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 9205%2C   59%2C   ...7%2C    4%2C  772%2C   13%2C%0A        3626%2C  680%2C   86%2C  681%2C   64%2C  287%2C  193%2C  538%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  682%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_data_path" type="str" qualifier="builtins" value="sentiment.test.csv" />
<var name="test_list" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   890%2C   628%2C     3%2C    96%2C  1139%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4112%2C     4%2C  6634...8720%2C  7772%2C     7%2C%0A         2106%2C   766%2C     7%2C   731%2C    42%2C   335%2C   385%2C   515%2C    98%2C   631%2C%0A          102%2C    13%2C   451%5D%29%2C tensor%28%5B  96%2C 1680%2C 1434%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2683%2C  314%2C  808%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1900%2C   61%2C    7%2C  696%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  5005%2C    21%2C   335%2C    59%2C   663%2C   635%2C    22%2C%0A         8435%2C    13%2C  4026%2C    96%2C   360%2C   345...  723%2C    96%2C   309%2C   498%2C    94%2C   723%2C  1112%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 21503%2C    64%2C     7%2C  2917%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  507%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 9205%2C   59%2C   ...7%2C    4%2C  772%2C   13%2C%0A        3626%2C  680%2C   86%2C  681%2C   64%2C  287%2C  193%2C  538%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  682%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="tokenizer" type="RegexpTokenizer" qualifier="nltk.tokenize.regexp" value="RegexpTokenizer%28pattern=%27%5C%5Cw%2B%27%2C gaps=False%2C discard_empty=True%2C flags=re.UNICODE%7Cre.MULTILINE%7Cre.DOTALL%29" isContainer="True" />
<var name="train_data_path" type="str" qualifier="builtins" value="sentiment.train.csv" />
<var name="train_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x0000024A85E166C0&gt;" isContainer="True" shape="160000" />
<var name="train_list" type="list" qualifier="builtins" value="%5B%28%5B   96     1   287    96   248    96     1   204 17795   291   110    24%2C    47    96   193   306    64    50    13    93%5D%2C array%281%29%29%2C %28%5B  96  889  234   21   96 1198 2498    7  696 1946 1941  268   96   96%2C   85 1699   96   21   67    7 1167  889 1589%5D%2C array%281%29%29%2C %28%5B  96    2  320   98 3619 7186   13   98    7  366   96  128 5856   14%2C  151   88  105 9931   96   88   96  123   59 1971  320   59  175   11%2C   15  128   88   96  140  105 9931 5948%5D%2C array%280%29%29%2C %28%5B  96    1  358   13  333  341   96   76 1786   83   96  241 1646 3108%2C   24   96   13 1203  733   61  305   95   96   76 1433   48    4  702%2C  117   96  115    4  487   24   96   96  356   76   96   62%5D%2C array%281%29%29%2C %28%5B  96 1434 1240   64  314  682   71   96    1  314 1138 1565 1566   98%2C  265 1920   96    1 7590   98 3244   87  310 1599  335   14  151  540%2C  232  287   14 1920  249    0    1    4  617  713   96   12   11%5D%2C array%281%29%29%2C %28%5B   96    64    88   314  1785  3235    24   345    24    96    26    96%2C    13  1185   341    ..." isContainer="True" shape="160000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x0000024A85E625A0&gt;" isContainer="True" shape="160000" />
<var name="vocab_size" type="int" qualifier="builtins" value="54767" />
<var name="word_ids" type="Tensor" qualifier="torch" value="tensor%28%5B%5B   96%2C   336%2C  5361%2C   268%2C    96%2C    13%2C    77%2C  3271%2C   263%2C    96%2C%0A            77%2C   310%2C    98%2C 30761%5D%5D%29" isContainer="True" shape="(1, 14)" />
<var name="word_to_index" type="dict" qualifier="builtins" value="%7B%270%27%3A 7011%2C %2700%27%3A 7186%2C %27000%27%3A 7903%2C %27001%27%3A 49428%2C %27003%27%3A 50032%2C %27005%27%3A 54582%2C %27007%27%3A 31481%2C %2700am%27%3A 28695%2C %2701%27%3A 27136%2C %2702%27%3A 33916%2C %2703%27%3A 11477%2C %27039%27%3A 47815%2C %2704%27%3A 8254%2C %27047%27%3A 52250%2C %2705%27%3A 37506%2C %2706%27%3A 34657%2C %2707%27%3A 7221%2C %2708%27%3A 26910%2C %2709%27%3A 24096%2C %270f%27%3A 47448%2C %271%27%3A 797%2C %2710%27%3A 1695%2C %27100%27%3A 874%2C %271000%27%3A 11350%2C %2710000%27%3A 35776%2C %27100000%27%3A 25201%2C %271000000%27%3A 29683%2C %271000s%27%3A 22349%2C %271000x%27%3A 40244%2C %271001%27%3A 8102%2C %27100k%27%3A 50021%2C %27100lbs%27%3A 20517%2C %27100s%27%3A 27884%2C %27100th%27%3A 884%2C %27100x%27%3A 38961%2C %27101%27%3A 7160%2C %27101st%27%3A 29946%2C %27102%27%3A 33226%2C %27103%27%3A 35688%2C %271031%27%3A 52765%2C %27104%27%3A 35687%2C %27105%27%3A 30396%2C %271050%27%3A 44952%2C %27106%27%3A 12373%2C %271066%27%3A 41729%2C %27107%27%3A 737%2C %27108%27%3A 16005%2C %27109%27%3A 41553%2C %271092%27%3A 53073%2C %2710am%27%3A 28786%2C %2710k%27%3A 17404%2C %2710min%27%3A 53254%2C %2710th%27%3A 26843%2C %2710x%27%3A 24694%2C %2710years%27%3A 37449%2C %2710yr%27%3A 32636%2C %2710yrs%27%3A 54312%2C %2711%27%3A 3883%2C %27110%27%3A 20178%2C %271100%27%3A 44794%2C %27111%27%3A 20179%2C %27112%27%3A 11541%2C %27113%27%3A 38280%2C %27114%27%3A 26876%2C %27115%27%3A 16004%2C %27116%27%3A 20922%2C %27117%27%3A 51318%2C %27118%27%3A 23682%2C %27119%27%3A 48641%2C %2711g%27%3A 42267%2C %2711pm%27%3A 284..." isContainer="True" shape="54767" />
<var name="word_to_index_file" type="str" qualifier="builtins" value="./models/1st_model_word_to_index.pkl" />
</xml>