
Step 1000, Avg Loss: 0.6820446678996086, F1 Score: 0.5725696154056568
Step 2000, Avg Loss: 0.6745714381933212, F1 Score: 0.6471112042323608
Step 3000, Avg Loss: 0.6611055792868137, F1 Score: 0.6298599407217993
Step 4000, Avg Loss: 0.6534568316936493, F1 Score: 0.6913994800693241
Step 5000, Avg Loss: 0.6414124419987202, F1 Score: 0.6668228404099561
Step 6000, Avg Loss: 0.6411472640037537, F1 Score: 0.7095882840563692
Step 7000, Avg Loss: 0.627735270023346, F1 Score: 0.7299018020934499
Step 8000, Avg Loss: 0.6169160022139549, F1 Score: 0.7489352752510647
Step 9000, Avg Loss: 0.6142969070672989, F1 Score: 0.7618950273419532
Step 10000, Avg Loss: 0.5991493299007415, F1 Score: 0.7617815943303502
Step 11000, Avg Loss: 0.5966821035444736, F1 Score: 0.7669907855215599
Step 12000, Avg Loss: 0.5906084209382534, F1 Score: 0.7725230908999142
Step 13000, Avg Loss: 0.5914995300918817, F1 Score: 0.7759127359665289
Step 14000, Avg Loss: 0.5847365988940001, F1 Score: 0.7793520261567423
Step 15000, Avg Loss: 0.5739159944280982, F1 Score: 0.7790831375293706
Step 16000, Avg Loss: 0.5666028791964054, F1 Score: 0.7854021608643458
Step 17000, Avg Loss: 0.550123063698411, F1 Score: 0.7847890903758737
Step 18000, Avg Loss: 0.558857355542481, F1 Score: 0.7850715444755864
Step 19000, Avg Loss: 0.5606109121665358, F1 Score: 0.783834113443052
Step 20000, Avg Loss: 0.5422005232349039, F1 Score: 0.7848690199251542
Step 21000, Avg Loss: 0.534015203371644, F1 Score: 0.7875292128685794
Step 22000, Avg Loss: 0.5291521888263524, F1 Score: 0.7882972382276972
Step 23000, Avg Loss: 0.523245140273124, F1 Score: 0.7900183195524088
Step 24000, Avg Loss: 0.5244341368824244, F1 Score: 0.7893317350913994
Step 25000, Avg Loss: 0.5241983389481902, F1 Score: 0.7917552271266868
Step 26000, Avg Loss: 0.5202200583145022, F1 Score: 0.791202229299363
Step 27000, Avg Loss: 0.5158303274475038, F1 Score: 0.7922310756972112
Step 28000, Avg Loss: 0.4879950784407556, F1 Score: 0.7928882725832013
Step 29000, Avg Loss: 0.5075902867875993, F1 Score: 0.792632263877071
Step 30000, Avg Loss: 0.4931945147998631, F1 Score: 0.7959945022580012
Step 31000, Avg Loss: 0.49301790561527015, F1 Score: 0.7943248238996853
Step 32000, Avg Loss: 0.492769370328635, F1 Score: 0.796031746031746
Step 33000, Avg Loss: 0.49501494386792183, F1 Score: 0.795247444998238
Step 34000, Avg Loss: 0.48642045274749396, F1 Score: 0.7958547137538988
Step 35000, Avg Loss: 0.47924443409219386, F1 Score: 0.7968483388537589
Step 36000, Avg Loss: 0.48660590074956417, F1 Score: 0.7964736282109743
Step 37000, Avg Loss: 0.4683386192433536, F1 Score: 0.7959914101646385
Step 38000, Avg Loss: 0.4693984793052077, F1 Score: 0.79669573198715
Step 39000, Avg Loss: 0.488632571878843, F1 Score: 0.7949087910955375
Step 40000, Avg Loss: 0.4537379213832319, F1 Score: 0.7972027972027972
Step 41000, Avg Loss: 0.4784276036974043, F1 Score: 0.7995755003032141
Step 42000, Avg Loss: 0.46832882188819347, F1 Score: 0.8008446880185027
Step 43000, Avg Loss: 0.49385901968926194, F1 Score: 0.8011457862204131
Step 44000, Avg Loss: 0.4422633355427533, F1 Score: 0.8011269306233335
Step 45000, Avg Loss: 0.4653039186708629, F1 Score: 0.8035661218424963
Step 46000, Avg Loss: 0.45360543571040035, F1 Score: 0.8040007883326764
Step 47000, Avg Loss: 0.43613642949517817, F1 Score: 0.8042437700468789
Step 48000, Avg Loss: 0.46317732937913386, F1 Score: 0.804920049200492
Step 49000, Avg Loss: 0.46063920313771817, F1 Score: 0.8038044019519968
Step 50000, Avg Loss: 0.45593659341707826, F1 Score: 0.8031020243730487
Step 51000, Avg Loss: 0.448361409123987, F1 Score: 0.8040074510396215
Step 52000, Avg Loss: 0.453975021019578, F1 Score: 0.8048572432134076
Step 53000, Avg Loss: 0.44395299794338644, F1 Score: 0.8060483273212433
Step 54000, Avg Loss: 0.4540671671051532, F1 Score: 0.8060698325394097
Step 55000, Avg Loss: 0.46266409585438667, F1 Score: 0.8068809148264984
Step 56000, Avg Loss: 0.4539056380372494, F1 Score: 0.8068517424689899
Step 57000, Avg Loss: 0.4550024580173194, F1 Score: 0.8065446588422518
Step 58000, Avg Loss: 0.45240795651823285, F1 Score: 0.8063731572938899
Step 59000, Avg Loss: 0.46761086700391025, F1 Score: 0.8071136793862708
Step 60000, Avg Loss: 0.4150656867949292, F1 Score: 0.8070105754431259
Step 61000, Avg Loss: 0.4478911489793099, F1 Score: 0.8079594148647983
Step 62000, Avg Loss: 0.44732986418157816, F1 Score: 0.8081821310911147
Step 63000, Avg Loss: 0.45136154372151943, F1 Score: 0.8074107276258777
Step 64000, Avg Loss: 0.4227735818363726, F1 Score: 0.8081389578163771
Step 65000, Avg Loss: 0.42695315736252815, F1 Score: 0.8089798744004352
Step 66000, Avg Loss: 0.421881951729767, F1 Score: 0.8092542945089862
Step 67000, Avg Loss: 0.4380206426519435, F1 Score: 0.8091238880882572
Step 68000, Avg Loss: 0.445167234512046, F1 Score: 0.8076088601846713
Step 69000, Avg Loss: 0.45467839416116473, F1 Score: 0.807003339742941
Step 70000, Avg Loss: 0.4176479277778417, F1 Score: 0.8088955522238881
Step 71000, Avg Loss: 0.424657632409595, F1 Score: 0.809350182308576
Step 72000, Avg Loss: 0.40371915776282546, F1 Score: 0.810126582278481
Step 73000, Avg Loss: 0.43189715262898243, F1 Score: 0.808943700807139
Step 74000, Avg Loss: 0.46102594952657816, F1 Score: 0.810824230387289
Step 75000, Avg Loss: 0.41569382931897414, F1 Score: 0.8105121696281917
Step 76000, Avg Loss: 0.4257513266829774, F1 Score: 0.8092752459345512
Step 77000, Avg Loss: 0.44782669286523014, F1 Score: 0.809535708930981
Step 78000, Avg Loss: 0.4176122328380588, F1 Score: 0.80950233047662
Step 79000, Avg Loss: 0.4462777014141902, F1 Score: 0.8116671602013622
Step 80000, Avg Loss: 0.449023348595947, F1 Score: 0.8118773197890213
Step 81000, Avg Loss: 0.41038330471236256, F1 Score: 0.8121872240211951
Step 82000, Avg Loss: 0.42801858090842143, F1 Score: 0.8131472747605387
Step 83000, Avg Loss: 0.45485672931093724, F1 Score: 0.8125337655321447
Step 84000, Avg Loss: 0.42799982421286403, F1 Score: 0.8123423824358403
Step 85000, Avg Loss: 0.4315583623219281, F1 Score: 0.8123107470836436
Step 86000, Avg Loss: 0.4321510507483035, F1 Score: 0.8120838716088642
Step 87000, Avg Loss: 0.40040799023979345, F1 Score: 0.8114451213422551
Step 88000, Avg Loss: 0.4094039033050649, F1 Score: 0.8085063291139241
Step 89000, Avg Loss: 0.4145785451484844, F1 Score: 0.8088123575588757
Step 90000, Avg Loss: 0.40309632471716034, F1 Score: 0.8115128115128115
Step 91000, Avg Loss: 0.41666597116948106, F1 Score: 0.8110335125357986
Step 92000, Avg Loss: 0.4149512883094139, F1 Score: 0.812770401312845
Step 93000, Avg Loss: 0.3829736956343986, F1 Score: 0.8132631787947979
Step 94000, Avg Loss: 0.4460358842937276, F1 Score: 0.8125935535375711
Step 95000, Avg Loss: 0.42077874167775736, F1 Score: 0.8126783836563016
Step 96000, Avg Loss: 0.4364001607047394, F1 Score: 0.81218886709911
Step 97000, Avg Loss: 0.42909441640088336, F1 Score: 0.8107451813628775
Step 98000, Avg Loss: 0.4301024313326925, F1 Score: 0.8117332256273034
Step 99000, Avg Loss: 0.4340215584379621, F1 Score: 0.8125031459203704
Step 100000, Avg Loss: 0.4144230754226446, F1 Score: 0.8127510040160643
Step 101000, Avg Loss: 0.43427587593183853, F1 Score: 0.8144468180237469
Step 102000, Avg Loss: 0.4038028998093214, F1 Score: 0.8143718493624592
Step 103000, Avg Loss: 0.4182945268652402, F1 Score: 0.815385365378053
Step 104000, Avg Loss: 0.4268730369457044, F1 Score: 0.814935064935065
Step 105000, Avg Loss: 0.4170024997410364, F1 Score: 0.8145209283872248
Step 106000, Avg Loss: 0.4207205444308929, F1 Score: 0.8149836487959567
Step 107000, Avg Loss: 0.3907902045736555, F1 Score: 0.8148951911220715
Step 108000, Avg Loss: 0.3793149694120511, F1 Score: 0.8153868857297457
Step 109000, Avg Loss: 0.4197307408405468, F1 Score: 0.8153392330383481
Step 110000, Avg Loss: 0.4145741195457522, F1 Score: 0.8155839019084347
Step 111000, Avg Loss: 0.42098619653098285, F1 Score: 0.8152512998266898
Step 112000, Avg Loss: 0.39139946988923474, F1 Score: 0.8158517350157729
Step 113000, Avg Loss: 0.41852587270364167, F1 Score: 0.8162286497332746
Step 114000, Avg Loss: 0.4174861578042619, F1 Score: 0.8159409594095941
Step 115000, Avg Loss: 0.4166410105833784, F1 Score: 0.81514985834286
Step 116000, Avg Loss: 0.38946738849626855, F1 Score: 0.8148818501759678
Step 117000, Avg Loss: 0.4016987908831798, F1 Score: 0.8148148148148148
Step 118000, Avg Loss: 0.3942900126602035, F1 Score: 0.8148522354129831
Step 119000, Avg Loss: 0.3876293498221785, F1 Score: 0.815315992582569
Step 120000, Avg Loss: 0.4120418070312589, F1 Score: 0.8152293394623763
Step 121000, Avg Loss: 0.3975745602103416, F1 Score: 0.8155554439029292
Step 122000, Avg Loss: 0.39423878248245453, F1 Score: 0.8155651389028181
Step 123000, Avg Loss: 0.4243798797547352, F1 Score: 0.8164364434249239
Step 124000, Avg Loss: 0.3938731332258321, F1 Score: 0.8165137614678899
Step 125000, Avg Loss: 0.41753941868990657, F1 Score: 0.8162634073334997
Step 126000, Avg Loss: 0.4136363224121742, F1 Score: 0.8165219994013768
Step 127000, Avg Loss: 0.4101855262090685, F1 Score: 0.8160056443078164
Step 128000, Avg Loss: 0.4341512423425447, F1 Score: 0.8163306148996848
Step 129000, Avg Loss: 0.39456004281085916, F1 Score: 0.8165418823174715
Step 130000, Avg Loss: 0.44114609483326783, F1 Score: 0.8166364916515791
Step 131000, Avg Loss: 0.4310575026955921, F1 Score: 0.8169595649327761
Step 132000, Avg Loss: 0.38810436122422104, F1 Score: 0.8175769612711022
Step 133000, Avg Loss: 0.4254876928250305, F1 Score: 0.8174957799622679
Step 134000, Avg Loss: 0.41362008830928243, F1 Score: 0.8177432990203392
Step 135000, Avg Loss: 0.3968035352912266, F1 Score: 0.8178383215670677
Step 136000, Avg Loss: 0.39412585069891065, F1 Score: 0.8180280569097602
Step 137000, Avg Loss: 0.41737748982547784, F1 Score: 0.8179603202227637
Step 138000, Avg Loss: 0.40140108672948555, F1 Score: 0.81840746624305
Step 139000, Avg Loss: 0.4126106765011791, F1 Score: 0.8190693295009663
Step 140000, Avg Loss: 0.41741174304101153, F1 Score: 0.8190223271368152
Step 141000, Avg Loss: 0.41045591795025393, F1 Score: 0.8193152582624184
Step 142000, Avg Loss: 0.42855158695043066, F1 Score: 0.8189359694004272
Step 143000, Avg Loss: 0.38129126735171304, F1 Score: 0.818831525813384
Step 144000, Avg Loss: 0.3924895665720105, F1 Score: 0.8187731411988391
Step 145000, Avg Loss: 0.39653477304731494, F1 Score: 0.817526344980588
Step 146000, Avg Loss: 0.4061966287138639, F1 Score: 0.8178763103776897
Step 147000, Avg Loss: 0.3919255941924639, F1 Score: 0.8195136535778353
Step 148000, Avg Loss: 0.39451224635727705, F1 Score: 0.8187240014032977
Step 149000, Avg Loss: 0.414035113380407, F1 Score: 0.8182047022101394
Step 150000, Avg Loss: 0.408202667590871, F1 Score: 0.8182414367149271
Step 151000, Avg Loss: 0.3877079686371144, F1 Score: 0.8182826043136464
Step 152000, Avg Loss: 0.41051732096401977, F1 Score: 0.8186309103740297
Step 153000, Avg Loss: 0.39206058146501893, F1 Score: 0.819225179171052
Step 154000, Avg Loss: 0.3895658667460084, F1 Score: 0.8200239568776203
Step 155000, Avg Loss: 0.4286286141332239, F1 Score: 0.8205128205128205
Step 156000, Avg Loss: 0.39680513155367225, F1 Score: 0.8202410120506025
Step 157000, Avg Loss: 0.39746094757050743, F1 Score: 0.8205256320575597
Step 158000, Avg Loss: 0.37767578452359885, F1 Score: 0.8207095874028304
Step 159000, Avg Loss: 0.41721389003796505, F1 Score: 0.821120797011208
Step 160000, Avg Loss: 0.3766139884793665, F1 Score: 0.8210097396143908
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="RegexpTokenizer" type="ABCMeta" qualifier="abc" value="%3Cclass %27nltk.tokenize.regexp.RegexpTokenizer%27&gt;" isContainer="True" />
<var name="avg_loss" type="float" qualifier="builtins" value="0.3766139884793665" />
<var name="batch_size" type="int" qualifier="builtins" value="1" />
<var name="best_f1" type="float64" qualifier="numpy" value="0.821120797011208" shape="()" />
<var name="dev_data_path" type="str" qualifier="builtins" value="sentiment.dev.csv" />
<var name="dev_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x000001DFDCF076B0&gt;" isContainer="True" shape="20000" />
<var name="dev_f1" type="float64" qualifier="numpy" value="0.8210097396143908" shape="()" />
<var name="dev_list" type="list" qualifier="builtins" value="%5B%28%5B  96   96  193    4 7609   62   71   27  234   64  329   11   96    1%2C  629  314 7041 1811 1900 1018  177   96  104    4  105  106 4116   11%2C  264   11    1 1693  992   13 6499%5D%2C array%280%29%29%2C %28%5B   96   288     7  1160    71     3  1056   223   281    83 16366  2101%2C   268   607   140     7  1283    24 10093  1475    96 19270    98   951%2C 11996  2253   727   398     7    70    96     7 16366   781  2101   268%2C     7  3551    58    77  6499%5D%2C array%280%29%29%2C %28%5B  96    1   18 1818   27    0   21   96   15   88 5047   61    0   36%2C   61   96   13  104  190 4549   98   40   83 1712   61   96 1601   96%2C   96  551  200   81 1903    7   21  307   96 1043   96  772  200  172%2C   11  671 2278   96   96 1594   47   33 3211   15 3408  268    7  117%2C   24   33 5050%5D%2C array%281%29%29%2C %28%5B  96  193    4   18 1288   13  742 3822  862   96 3545 4912   77  128%2C 1236  884   96  128 3472  200  719 1755%5D%2C array%280%29%29%2C %28%5B  96  124  310  288    0   21   96    1  310   64   33 4288    6   96%2C  408  409 1695  151   62   11 ..." isContainer="True" shape="20000" />
<var name="dev_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000001DFC351FD70&gt;" isContainer="True" shape="20000" />
<var name="device" type="device" qualifier="torch" value="device%28type=%27cpu%27%29" isContainer="True" />
<var name="embedding_size" type="int" qualifier="builtins" value="50" />
<var name="embeddings_file_path" type="str" qualifier="builtins" value="./models/1st_model_embeddings_state_dict.pt" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="1" />
<var name="f" type="BufferedReader" qualifier="_io" value="%3C_io.BufferedReader name=%27./models/1st_model_index_to_word.pkl%27&gt;" isContainer="True" />
<var name="f1_scores" type="list" qualifier="builtins" value="%5B0.5725696154056568%2C 0.6471112042323608%2C 0.6298599407217993%2C 0.6913994800693241%2C 0.6668228404099561%2C 0.7095882840563692%2C 0.7299018020934499%2C 0.7489352752510647%2C 0.7618950273419532%2C 0.7617815943303502%2C 0.7669907855215599%2C 0.7725230908999142%2C 0.7759127359665289%2C 0.7793520261567423%2C 0.7790831375293706%2C 0.7854021608643458%2C 0.7847890903758737%2C 0.7850715444755864%2C 0.783834113443052%2C 0.7848690199251542%2C 0.7875292128685794%2C 0.7882972382276972%2C 0.7900183195524088%2C 0.7893317350913994%2C 0.7917552271266868%2C 0.791202229299363%2C 0.7922310756972112%2C 0.7928882725832013%2C 0.792632263877071%2C 0.7959945022580012%2C 0.7943248238996853%2C 0.796031746031746%2C 0.795247444998238%2C 0.7958547137538988%2C 0.7968483388537589%2C 0.7964736282109743%2C 0.7959914101646385%2C 0.79669573198715%2C 0.7949087910955375%2C 0.7972027972027972%2C 0.7995755003032141%2C 0.8008446880185027%2C 0.8011457862204131%2C 0.8011269306233335%2C 0.8035661218424963%2C 0.8040007883326764%2C 0.8042437700468789%2C 0.804920049200492%2C 0.8038044019519968%2C 0.8031020243730487%2C 0.80400..." isContainer="True" shape="160" />
<var name="index_to_word" type="dict" qualifier="builtins" value="%7B0%3A %27this%27%2C 1%3A %27was%27%2C 2%3A %27bought%27%2C 3%3A %27as%27%2C 4%3A %27a%27%2C 5%3A %27gift%27%2C 6%3A %27but%27%2C 7%3A %27the%27%2C 8%3A %27person%27%2C 9%3A %27who%27%2C 10%3A %27got%27%2C 11%3A %27it%27%2C 12%3A %27loved%27%2C 13%3A %27and%27%2C 14%3A %27they%27%2C 15%3A %27will%27%2C 16%3A %27use%27%2C 17%3A %27soon%27%2C 18%3A %27very%27%2C 19%3A %27well%27%2C 20%3A %27written%27%2C 21%3A %27book%27%2C 22%3A %27on%27%2C 23%3A %27period%27%2C 24%3A %27of%27%2C 25%3A %27world%27%2C 26%3A %27history%27%2C 27%3A %27with%27%2C 28%3A %27which%27%2C 29%3A %27i%27%2C 30%3A %27am%27%2C 31%3A %27familiar%27%2C 32%3A %27despite%27%2C 33%3A %27my%27%2C 34%3A %27familiarity%27%2C 35%3A %27subject%27%2C 36%3A %27area%27%2C 37%3A %27learned%27%2C 38%3A %27lot%27%2C 39%3A %27new%27%2C 40%3A %27information%27%2C 41%3A %27also%27%2C 42%3A %27one%27%2C 43%3A %27best%27%2C 44%3A %27concise%27%2C 45%3A %27descriptions%27%2C 46%3A %27wwii%27%2C 47%3A %27that%27%2C 48%3A %27have%27%2C 49%3A %27ever%27%2C 50%3A %27read%27%2C 51%3A %27thought%27%2C 52%3A %27provoking%27%2C 53%3A %27hot%27%2C 54%3A %27cross%27%2C 55%3A %27buns%27%2C 56%3A %27contains%27%2C 57%3A %27cast%27%2C 58%3A %27characters%27%2C 59%3A %27you%27%2C 60%3A %27fall%27%2C 61%3A %27in%27%2C 62%3A %27love%27%2C 63%3A %27want%27%2C 64%3A %27to%27%2C 65%3A %27hang%27%2C 66%3A %27out%27%2C 67%3A %27has%27%2C 68%3A %27few%27%2C 69%3A %27different%27%2C 70%3A %27plot%27%2C 71%3A %27story%27%2C 72%3A %27lines%27%2C 73%3A %27intersect%27%2C 74%3A %27unexpected%27%2C 75%3A %27ways%27%2C 76%3A %27stories%27%2C 77%3A %27are%27%2C 78%3A %27full%27%2C 79%3A %27h..." isContainer="True" shape="40547" />
<var name="index_to_word_file" type="str" qualifier="builtins" value="./models/1st_model_index_to_word.pkl" />
<var name="labels" type="Tensor" qualifier="torch" value="tensor%28%5B0.%5D%29" isContainer="True" shape="(1,)" />
<var name="learning_rate" type="float" qualifier="builtins" value="5e-05" />
<var name="loss" type="Tensor" qualifier="torch" value="tensor%280.1218%2C grad_fn=%3CBinaryCrossEntropyBackward0&gt;%29" isContainer="True" shape="()" />
<var name="loss_function" type="BCELoss" qualifier="torch.nn.modules.loss" value="BCELoss%28%29" isContainer="True" />
<var name="losses" type="list" qualifier="builtins" value="%5B0.6820446678996086%2C 0.6745714381933212%2C 0.6611055792868137%2C 0.6534568316936493%2C 0.6414124419987202%2C 0.6411472640037537%2C 0.627735270023346%2C 0.6169160022139549%2C 0.6142969070672989%2C 0.5991493299007415%2C 0.5966821035444736%2C 0.5906084209382534%2C 0.5914995300918817%2C 0.5847365988940001%2C 0.5739159944280982%2C 0.5666028791964054%2C 0.550123063698411%2C 0.558857355542481%2C 0.5606109121665358%2C 0.5422005232349039%2C 0.534015203371644%2C 0.5291521888263524%2C 0.523245140273124%2C 0.5244341368824244%2C 0.5241983389481902%2C 0.5202200583145022%2C 0.5158303274475038%2C 0.4879950784407556%2C 0.5075902867875993%2C 0.4931945147998631%2C 0.49301790561527015%2C 0.492769370328635%2C 0.49501494386792183%2C 0.48642045274749396%2C 0.47924443409219386%2C 0.48660590074956417%2C 0.4683386192433536%2C 0.4693984793052077%2C 0.488632571878843%2C 0.4537379213832319%2C 0.4784276036974043%2C 0.46832882188819347%2C 0.49385901968926194%2C 0.4422633355427533%2C 0.4653039186708629%2C 0.45360543571040035%2C 0.43613642949517817%2C 0.46317732937913386%2C 0.46063920313771817%2C 0.4559365934170..." isContainer="True" shape="160" />
<var name="max_steps" type="int" qualifier="builtins" value="1000000000" />
<var name="model" type="DocumentAttentionClassifier" qualifier="__main__" value="DocumentAttentionClassifier%28%0A  %28embeddings%29%3A Embedding%2840547%2C 50%29%0A  %28linear%29%3A Linear%28in_features=200%2C out_features=1%2C bias=True%29%0A%29" isContainer="True" />
<var name="num_heads" type="int" qualifier="builtins" value="4" />
<var name="optimizer" type="AdamW" qualifier="torch.optim.adamw" value="AdamW %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 5e-05%0A    maximize%3A False%0A    weight_decay%3A 0.01%0A%29" isContainer="True" />
<var name="patience" type="int" qualifier="builtins" value="100000" />
<var name="probability" type="Tensor" qualifier="torch" value="tensor%28%5B0.1147%5D%2C grad_fn=%3CSqueezeBackward1&gt;%29" isContainer="True" shape="(1,)" />
<var name="running_loss" type="float" qualifier="builtins" value="0.0" />
<var name="sent_dev_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0      Picturing Perfect is a sappy love story with l...      0%5D %5B1      Seems like the same story as any other series ...      0%5D %5B2      I was very pleased with this book. I will be t...      1%5D %5B3      It is a very light and rather silly novel. The...      0%5D %5B4      I did not like this book. It was not to my tas...      0%5D %5B...                                                  ...    ...%5D %5B19995  Great content%2C the story is fantastic%2C but sho...      1%5D %5B19996  Typical book club book... Incest%2C child abuse%2C...      0%5D %5B19997  Fascinating book. Shorter than most Russell Ba...      1%5D %5B19998  This book is not well-organized%2C which is impo...      0%5D %5B19999  CAN WE CALL THIS A CLASSIC OF THE GENRE%3F I THI...      1%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_test_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27text%27%5D %5B0            0  Really sad review as I absolutely loved the fi...%5D %5B1            1  Excellent content%2C perfect for Christians who ...%5D %5B2            2  This is an okay book if you need advice on bud...%5D %5B3            3  This is one book you can%27t put down%21 This book...%5D %5B4            4  There were to many names that I had no idenity...%5D %5B...        ...                                                ...%5D %5B19995    19995  I found this book to be a very entertaining an...%5D %5B19996    19996  Wow%2C what a Middle School%21 Read this book your...%5D %5B19997    19997  Not what I expected. Not enough about circular...%5D %5B19998    19998  I like Joanne Fluke%27s mystery series starring ...%5D %5B19999    19999  Grow some chickens%2C improve your land%2C make al...%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_train_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0       It was what I needed. There was no markings or...      1%5D %5B1       A cute little book. My wife gets the family wa...      1%5D %5B2       I bought these for 40.00 and for the price I j...      0%5D %5B3       It was interesting and enjoyable reading. Shor...      1%5D %5B4       A perfect ending to an amazing story. This was...      1%5D %5B...                                                   ...    ...%5D %5B159995  After reading every book Stephen King has to o...      1%5D %5B159996  Baby boomers who are experiencing %22aging eyeba...      0%5D %5B159997  Must read%2C must have%2C must read again. This bo...      1%5D %5B159998  Dr. Chopra%27s books are always enlightening and...      1%5D %5B159999  Boooring%21%21 Just enough to keep you intrigued f...      0%5D %5B%5D" isContainer="True" shape="(160000, 2)" />
<var name="step" type="int" qualifier="builtins" value="159999" />
<var name="steps_since_improvement" type="int" qualifier="builtins" value="1000" />
<var name="test_data" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   885%2C   626%2C     3%2C    96%2C  1132%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4050%2C     4%2C  6499...8504%2C  7595%2C     7%2C%0A         2087%2C   762%2C     7%2C   727%2C    42%2C   335%2C   385%2C   514%2C    98%2C   629%2C%0A          102%2C    13%2C   450%5D%29%2C tensor%28%5B  96%2C 1669%2C 1427%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2651%2C  314%2C  804%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1887%2C   61%2C    7%2C  693%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  4921%2C    21%2C   335%2C    59%2C   661%2C   633%2C    22%2C%0A         8229%2C    13%2C  3966%2C    96%2C   360%2C   345...  719%2C    96%2C   309%2C   497%2C    94%2C   719%2C  1105%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 20119%2C    64%2C     7%2C  2881%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  506%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 8965%2C   59%2C   ...7%2C    4%2C  768%2C   13%2C%0A        3576%2C  677%2C   86%2C  678%2C   64%2C  287%2C  193%2C  537%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  679%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_data_path" type="str" qualifier="builtins" value="sentiment.test.csv" />
<var name="test_list" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   885%2C   626%2C     3%2C    96%2C  1132%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4050%2C     4%2C  6499...8504%2C  7595%2C     7%2C%0A         2087%2C   762%2C     7%2C   727%2C    42%2C   335%2C   385%2C   514%2C    98%2C   629%2C%0A          102%2C    13%2C   450%5D%29%2C tensor%28%5B  96%2C 1669%2C 1427%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2651%2C  314%2C  804%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1887%2C   61%2C    7%2C  693%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  4921%2C    21%2C   335%2C    59%2C   661%2C   633%2C    22%2C%0A         8229%2C    13%2C  3966%2C    96%2C   360%2C   345...  719%2C    96%2C   309%2C   497%2C    94%2C   719%2C  1105%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 20119%2C    64%2C     7%2C  2881%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  506%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 8965%2C   59%2C   ...7%2C    4%2C  768%2C   13%2C%0A        3576%2C  677%2C   86%2C  678%2C   64%2C  287%2C  193%2C  537%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  679%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="tokenizer" type="RegexpTokenizer" qualifier="nltk.tokenize.regexp" value="RegexpTokenizer%28pattern=%27%5C%5Cw%2B%27%2C gaps=False%2C discard_empty=True%2C flags=re.UNICODE%7Cre.MULTILINE%7Cre.DOTALL%29" isContainer="True" />
<var name="train_data_path" type="str" qualifier="builtins" value="sentiment.train.csv" />
<var name="train_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x000001DF8112E120&gt;" isContainer="True" shape="160000" />
<var name="train_list" type="list" qualifier="builtins" value="%5B%28%5B   96     1   287    96   248    96     1   204 16850   291   110    24%2C    47    96   193   306    64    50    13    93%5D%2C array%281%29%29%2C %28%5B  96  884  234   21   96 1191 2472    7  693 1931 1926  268   96   96%2C   85 1688   96   21   67    7 1160  884 1578%5D%2C array%281%29%29%2C %28%5B  96    2  320   98 3569 7030   13   98    7  366   96  128 5745   14%2C  151   88  105 9655   96   88   96  123   59 1956  320   59  175   11%2C   15  128   88   96  140  105 9655 5834%5D%2C array%280%29%29%2C %28%5B  96    1  358   13  333  341   96   76 1775   83   96  241 1635 3070%2C   24   96   13 1196  729   61  305   95   96   76 1426   48    4  699%2C  117   96  115    4  486   24   96   96  356   76   96   62%5D%2C array%281%29%29%2C %28%5B  96 1427 1233   64  314  679   71   96    1  314 1131 1554 1555   98%2C  265 1905   96    1 7419   98 3204   87  310 1588  335   14  151  539%2C  232  287   14 1905  249    0    1    4  615  710   96   12   11%5D%2C array%281%29%29%2C %28%5B   96    64    88   314  1774  3195    24   345    24    96    26    96%2C    13  1178   341    ..." isContainer="True" shape="160000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000001DFDDC6E720&gt;" isContainer="True" shape="160000" />
<var name="vocab_size" type="int" qualifier="builtins" value="40547" />
<var name="word_ids" type="Tensor" qualifier="torch" value="tensor%28%5B%5B   96%2C   193%2C   966%2C    13%2C   310%2C   140%2C   160%2C   358%2C    96%2C   193%2C%0A           204%2C  2124%2C   204%2C   868%2C    70%2C   619%2C    13%2C   160%2C    24%2C     7%2C%0A            58%2C    77%2C  2552%2C  1056%2C   975%2C 14330%5D%5D%29" isContainer="True" shape="(1, 26)" />
<var name="word_to_index" type="dict" qualifier="builtins" value="%7B%270%27%3A 6860%2C %2700%27%3A 7030%2C %27000%27%3A 7717%2C %27001%27%3A 38825%2C %27007%27%3A 28099%2C %2700am%27%3A 25993%2C %2701%27%3A 24762%2C %2702%27%3A 29846%2C %2703%27%3A 11089%2C %27039%27%3A 38140%2C %2704%27%3A 8057%2C %2705%27%3A 32280%2C %2706%27%3A 30353%2C %2707%27%3A 7063%2C %2708%27%3A 24586%2C %2709%27%3A 22275%2C %270f%27%3A 37971%2C %271%27%3A 793%2C %2710%27%3A 1684%2C %27100%27%3A 870%2C %271000%27%3A 10968%2C %2710000%27%3A 31123%2C %271000s%27%3A 20833%2C %271000x%27%3A 33996%2C %271001%27%3A 7910%2C %27100k%27%3A 39109%2C %27100s%27%3A 25367%2C %27100th%27%3A 879%2C %27101%27%3A 7005%2C %27101st%27%3A 26963%2C %27102%27%3A 29336%2C %27103%27%3A 31073%2C %27104%27%3A 31072%2C %27105%27%3A 27289%2C %27106%27%3A 11911%2C %271066%27%3A 34895%2C %27107%27%3A 733%2C %27108%27%3A 15245%2C %27109%27%3A 34772%2C %2710k%27%3A 16506%2C %2710th%27%3A 24530%2C %2710x%27%3A 22765%2C %2710yr%27%3A 28921%2C %2711%27%3A 3831%2C %27110%27%3A 18961%2C %271100%27%3A 36651%2C %27111%27%3A 18962%2C %27112%27%3A 11145%2C %27113%27%3A 32771%2C %27114%27%3A 24557%2C %27115%27%3A 15244%2C %27116%27%3A 19634%2C %27117%27%3A 39597%2C %27118%27%3A 21937%2C %27119%27%3A 38487%2C %2711pm%27%3A 25792%2C %2711th%27%3A 10687%2C %2712%27%3A 1681%2C %27120%27%3A 14299%2C %271200%27%3A 13958%2C %27121%27%3A 33184%2C %27122%27%3A 38431%2C %27123%27%3A 18402%2C %27124%27%3A 39041%2C %27125%27%3A 25024%2C %27126%27%3A 39242%2C %27127%27%3A 35211%2C %27128%27%3A 37280%2C %27129%27%3A 39683%2C %2712th%27%3A 10890%2C %2712yr%27%3A 17641%2C %2713%27%3A 3671%2C %27130%27..." isContainer="True" shape="40547" />
<var name="word_to_index_file" type="str" qualifier="builtins" value="./models/1st_model_word_to_index.pkl" />
</xml>
   inst_id                                               text  predicted_label
0        0  Really sad review as I absolutely loved the fi...              1.0
1        1  Excellent content, perfect for Christians who ...              1.0
2        2  This is an okay book if you need advice on bud...              1.0
3        3  This is one book you can't put down! This book...              1.0
4        4  There were to many names that I had no idenity...              1.0
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="RegexpTokenizer" type="ABCMeta" qualifier="abc" value="%3Cclass %27nltk.tokenize.regexp.RegexpTokenizer%27&gt;" isContainer="True" />
<var name="avg_loss" type="float" qualifier="builtins" value="0.3766139884793665" />
<var name="batch_size" type="int" qualifier="builtins" value="1" />
<var name="best_f1" type="float64" qualifier="numpy" value="0.821120797011208" shape="()" />
<var name="dev_data_path" type="str" qualifier="builtins" value="sentiment.dev.csv" />
<var name="dev_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x000001DFDCF076B0&gt;" isContainer="True" shape="20000" />
<var name="dev_f1" type="float64" qualifier="numpy" value="0.8210097396143908" shape="()" />
<var name="dev_list" type="list" qualifier="builtins" value="%5B%28%5B  96   96  193    4 7609   62   71   27  234   64  329   11   96    1%2C  629  314 7041 1811 1900 1018  177   96  104    4  105  106 4116   11%2C  264   11    1 1693  992   13 6499%5D%2C array%280%29%29%2C %28%5B   96   288     7  1160    71     3  1056   223   281    83 16366  2101%2C   268   607   140     7  1283    24 10093  1475    96 19270    98   951%2C 11996  2253   727   398     7    70    96     7 16366   781  2101   268%2C     7  3551    58    77  6499%5D%2C array%280%29%29%2C %28%5B  96    1   18 1818   27    0   21   96   15   88 5047   61    0   36%2C   61   96   13  104  190 4549   98   40   83 1712   61   96 1601   96%2C   96  551  200   81 1903    7   21  307   96 1043   96  772  200  172%2C   11  671 2278   96   96 1594   47   33 3211   15 3408  268    7  117%2C   24   33 5050%5D%2C array%281%29%29%2C %28%5B  96  193    4   18 1288   13  742 3822  862   96 3545 4912   77  128%2C 1236  884   96  128 3472  200  719 1755%5D%2C array%280%29%29%2C %28%5B  96  124  310  288    0   21   96    1  310   64   33 4288    6   96%2C  408  409 1695  151   62   11 ..." isContainer="True" shape="20000" />
<var name="dev_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000001DFC351FD70&gt;" isContainer="True" shape="20000" />
<var name="device" type="device" qualifier="torch" value="device%28type=%27cpu%27%29" isContainer="True" />
<var name="embedding_size" type="int" qualifier="builtins" value="50" />
<var name="embeddings_file_path" type="str" qualifier="builtins" value="./models/1st_model_embeddings_state_dict.pt" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="1" />
<var name="f" type="BufferedReader" qualifier="_io" value="%3C_io.BufferedReader name=%27./models/1st_model_index_to_word.pkl%27&gt;" isContainer="True" />
<var name="f1_scores" type="list" qualifier="builtins" value="%5B0.5725696154056568%2C 0.6471112042323608%2C 0.6298599407217993%2C 0.6913994800693241%2C 0.6668228404099561%2C 0.7095882840563692%2C 0.7299018020934499%2C 0.7489352752510647%2C 0.7618950273419532%2C 0.7617815943303502%2C 0.7669907855215599%2C 0.7725230908999142%2C 0.7759127359665289%2C 0.7793520261567423%2C 0.7790831375293706%2C 0.7854021608643458%2C 0.7847890903758737%2C 0.7850715444755864%2C 0.783834113443052%2C 0.7848690199251542%2C 0.7875292128685794%2C 0.7882972382276972%2C 0.7900183195524088%2C 0.7893317350913994%2C 0.7917552271266868%2C 0.791202229299363%2C 0.7922310756972112%2C 0.7928882725832013%2C 0.792632263877071%2C 0.7959945022580012%2C 0.7943248238996853%2C 0.796031746031746%2C 0.795247444998238%2C 0.7958547137538988%2C 0.7968483388537589%2C 0.7964736282109743%2C 0.7959914101646385%2C 0.79669573198715%2C 0.7949087910955375%2C 0.7972027972027972%2C 0.7995755003032141%2C 0.8008446880185027%2C 0.8011457862204131%2C 0.8011269306233335%2C 0.8035661218424963%2C 0.8040007883326764%2C 0.8042437700468789%2C 0.804920049200492%2C 0.8038044019519968%2C 0.8031020243730487%2C 0.80400..." isContainer="True" shape="160" />
<var name="index_to_word" type="dict" qualifier="builtins" value="%7B0%3A %27this%27%2C 1%3A %27was%27%2C 2%3A %27bought%27%2C 3%3A %27as%27%2C 4%3A %27a%27%2C 5%3A %27gift%27%2C 6%3A %27but%27%2C 7%3A %27the%27%2C 8%3A %27person%27%2C 9%3A %27who%27%2C 10%3A %27got%27%2C 11%3A %27it%27%2C 12%3A %27loved%27%2C 13%3A %27and%27%2C 14%3A %27they%27%2C 15%3A %27will%27%2C 16%3A %27use%27%2C 17%3A %27soon%27%2C 18%3A %27very%27%2C 19%3A %27well%27%2C 20%3A %27written%27%2C 21%3A %27book%27%2C 22%3A %27on%27%2C 23%3A %27period%27%2C 24%3A %27of%27%2C 25%3A %27world%27%2C 26%3A %27history%27%2C 27%3A %27with%27%2C 28%3A %27which%27%2C 29%3A %27i%27%2C 30%3A %27am%27%2C 31%3A %27familiar%27%2C 32%3A %27despite%27%2C 33%3A %27my%27%2C 34%3A %27familiarity%27%2C 35%3A %27subject%27%2C 36%3A %27area%27%2C 37%3A %27learned%27%2C 38%3A %27lot%27%2C 39%3A %27new%27%2C 40%3A %27information%27%2C 41%3A %27also%27%2C 42%3A %27one%27%2C 43%3A %27best%27%2C 44%3A %27concise%27%2C 45%3A %27descriptions%27%2C 46%3A %27wwii%27%2C 47%3A %27that%27%2C 48%3A %27have%27%2C 49%3A %27ever%27%2C 50%3A %27read%27%2C 51%3A %27thought%27%2C 52%3A %27provoking%27%2C 53%3A %27hot%27%2C 54%3A %27cross%27%2C 55%3A %27buns%27%2C 56%3A %27contains%27%2C 57%3A %27cast%27%2C 58%3A %27characters%27%2C 59%3A %27you%27%2C 60%3A %27fall%27%2C 61%3A %27in%27%2C 62%3A %27love%27%2C 63%3A %27want%27%2C 64%3A %27to%27%2C 65%3A %27hang%27%2C 66%3A %27out%27%2C 67%3A %27has%27%2C 68%3A %27few%27%2C 69%3A %27different%27%2C 70%3A %27plot%27%2C 71%3A %27story%27%2C 72%3A %27lines%27%2C 73%3A %27intersect%27%2C 74%3A %27unexpected%27%2C 75%3A %27ways%27%2C 76%3A %27stories%27%2C 77%3A %27are%27%2C 78%3A %27full%27%2C 79%3A %27h..." isContainer="True" shape="40547" />
<var name="index_to_word_file" type="str" qualifier="builtins" value="./models/1st_model_index_to_word.pkl" />
<var name="labels" type="Tensor" qualifier="torch" value="tensor%28%5B0.%5D%29" isContainer="True" shape="(1,)" />
<var name="learning_rate" type="float" qualifier="builtins" value="5e-05" />
<var name="loss" type="Tensor" qualifier="torch" value="tensor%280.1218%2C grad_fn=%3CBinaryCrossEntropyBackward0&gt;%29" isContainer="True" shape="()" />
<var name="loss_function" type="BCELoss" qualifier="torch.nn.modules.loss" value="BCELoss%28%29" isContainer="True" />
<var name="losses" type="list" qualifier="builtins" value="%5B0.6820446678996086%2C 0.6745714381933212%2C 0.6611055792868137%2C 0.6534568316936493%2C 0.6414124419987202%2C 0.6411472640037537%2C 0.627735270023346%2C 0.6169160022139549%2C 0.6142969070672989%2C 0.5991493299007415%2C 0.5966821035444736%2C 0.5906084209382534%2C 0.5914995300918817%2C 0.5847365988940001%2C 0.5739159944280982%2C 0.5666028791964054%2C 0.550123063698411%2C 0.558857355542481%2C 0.5606109121665358%2C 0.5422005232349039%2C 0.534015203371644%2C 0.5291521888263524%2C 0.523245140273124%2C 0.5244341368824244%2C 0.5241983389481902%2C 0.5202200583145022%2C 0.5158303274475038%2C 0.4879950784407556%2C 0.5075902867875993%2C 0.4931945147998631%2C 0.49301790561527015%2C 0.492769370328635%2C 0.49501494386792183%2C 0.48642045274749396%2C 0.47924443409219386%2C 0.48660590074956417%2C 0.4683386192433536%2C 0.4693984793052077%2C 0.488632571878843%2C 0.4537379213832319%2C 0.4784276036974043%2C 0.46832882188819347%2C 0.49385901968926194%2C 0.4422633355427533%2C 0.4653039186708629%2C 0.45360543571040035%2C 0.43613642949517817%2C 0.46317732937913386%2C 0.46063920313771817%2C 0.4559365934170..." isContainer="True" shape="160" />
<var name="max_steps" type="int" qualifier="builtins" value="1000000000" />
<var name="model" type="DocumentAttentionClassifier" qualifier="__main__" value="DocumentAttentionClassifier%28%0A  %28embeddings%29%3A Embedding%2840547%2C 50%29%0A  %28linear%29%3A Linear%28in_features=200%2C out_features=1%2C bias=True%29%0A%29" isContainer="True" />
<var name="num_heads" type="int" qualifier="builtins" value="4" />
<var name="optimizer" type="AdamW" qualifier="torch.optim.adamw" value="AdamW %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 5e-05%0A    maximize%3A False%0A    weight_decay%3A 0.01%0A%29" isContainer="True" />
<var name="output" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.2377%5D%5D%29" isContainer="True" shape="(1, 1)" />
<var name="patience" type="int" qualifier="builtins" value="100000" />
<var name="prediction" type="float" qualifier="builtins" value="1.0" />
<var name="predictions" type="list" qualifier="builtins" value="%5B1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C 1.0%2C..." isContainer="True" shape="20000" />
<var name="probability" type="Tensor" qualifier="torch" value="tensor%28%5B0.1147%5D%2C grad_fn=%3CSqueezeBackward1&gt;%29" isContainer="True" shape="(1,)" />
<var name="running_loss" type="float" qualifier="builtins" value="0.0" />
<var name="sent_dev_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0      Picturing Perfect is a sappy love story with l...      0%5D %5B1      Seems like the same story as any other series ...      0%5D %5B2      I was very pleased with this book. I will be t...      1%5D %5B3      It is a very light and rather silly novel. The...      0%5D %5B4      I did not like this book. It was not to my tas...      0%5D %5B...                                                  ...    ...%5D %5B19995  Great content%2C the story is fantastic%2C but sho...      1%5D %5B19996  Typical book club book... Incest%2C child abuse%2C...      0%5D %5B19997  Fascinating book. Shorter than most Russell Ba...      1%5D %5B19998  This book is not well-organized%2C which is impo...      0%5D %5B19999  CAN WE CALL THIS A CLASSIC OF THE GENRE%3F I THI...      1%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="sent_test_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27text%27%2C %27predicted_label%27%5D %5B0            0  Really sad review as I absolutely loved the fi...   %5D %5B1            1  Excellent content%2C perfect for Christians who ...   %5D %5B2            2  This is an okay book if you need advice on bud...   %5D %5B3            3  This is one book you can%27t put down%21 This book...   %5D %5B4            4  There were to many names that I had no idenity...   %5D %5B...        ...                                                ...   %5D %5B19995    19995  I found this book to be a very entertaining an...   %5D %5B19996    19996  Wow%2C what a Middle School%21 Read this book your...   %5D %5B19997    19997  Not what I expected. Not enough about circular...   %5D %5B19998    19998  I like Joanne Fluke%27s mystery series starring ...   %5D %5B19999    19999  Grow some chickens%2C improve your land%2C make al...   %5D %5B%5D %5B0                  1.0  %5D %5B1                  1.0  %5D %5B2                  1.0  %5D %5B3                  1.0  %5D %5B4                  1.0  %5D %5B...                ...  %5D %5B19995         ..." isContainer="True" shape="(20000, 3)" />
<var name="sent_train_df" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27text%27%2C %27label%27%5D %5B0       It was what I needed. There was no markings or...      1%5D %5B1       A cute little book. My wife gets the family wa...      1%5D %5B2       I bought these for 40.00 and for the price I j...      0%5D %5B3       It was interesting and enjoyable reading. Shor...      1%5D %5B4       A perfect ending to an amazing story. This was...      1%5D %5B...                                                   ...    ...%5D %5B159995  After reading every book Stephen King has to o...      1%5D %5B159996  Baby boomers who are experiencing %22aging eyeba...      0%5D %5B159997  Must read%2C must have%2C must read again. This bo...      1%5D %5B159998  Dr. Chopra%27s books are always enlightening and...      1%5D %5B159999  Boooring%21%21 Just enough to keep you intrigued f...      0%5D %5B%5D" isContainer="True" shape="(160000, 2)" />
<var name="step" type="int" qualifier="builtins" value="159999" />
<var name="steps_since_improvement" type="int" qualifier="builtins" value="1000" />
<var name="test_data" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   885%2C   626%2C     3%2C    96%2C  1132%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4050%2C     4%2C  6499...8504%2C  7595%2C     7%2C%0A         2087%2C   762%2C     7%2C   727%2C    42%2C   335%2C   385%2C   514%2C    98%2C   629%2C%0A          102%2C    13%2C   450%5D%29%2C tensor%28%5B  96%2C 1669%2C 1427%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2651%2C  314%2C  804%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1887%2C   61%2C    7%2C  693%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  4921%2C    21%2C   335%2C    59%2C   661%2C   633%2C    22%2C%0A         8229%2C    13%2C  3966%2C    96%2C   360%2C   345...  719%2C    96%2C   309%2C   497%2C    94%2C   719%2C  1105%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 20119%2C    64%2C     7%2C  2881%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  506%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 8965%2C   59%2C   ...7%2C    4%2C  768%2C   13%2C%0A        3576%2C  677%2C   86%2C  678%2C   64%2C  287%2C  193%2C  537%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  679%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_data_path" type="str" qualifier="builtins" value="sentiment.test.csv" />
<var name="test_list" type="list" qualifier="builtins" value="%5Btensor%28%5B   96%2C   885%2C   626%2C     3%2C    96%2C  1132%2C    12%2C     7%2C   285%2C    21%2C%0A           96%2C    21%2C     1%2C  4050%2C     4%2C  6499...8504%2C  7595%2C     7%2C%0A         2087%2C   762%2C     7%2C   727%2C    42%2C   335%2C   385%2C   514%2C    98%2C   629%2C%0A          102%2C    13%2C   450%5D%29%2C tensor%28%5B  96%2C 1669%2C 1427%2C   98%2C   96%2C    9%2C   77%2C  207%2C 2651%2C  314%2C  804%2C   24%2C%0A           7%2C   96%2C   13%2C   86%2C 1887%2C   61%2C    7%2C  693%5D%29%2C tensor%28%5B   96%2C   193%2C   314%2C  4921%2C    21%2C   335%2C    59%2C   661%2C   633%2C    22%2C%0A         8229%2C    13%2C  3966%2C    96%2C   360%2C   345...  719%2C    96%2C   309%2C   497%2C    94%2C   719%2C  1105%2C%0A           59%2C    80%2C    32%2C     7%2C   269%2C   154%2C 20119%2C    64%2C     7%2C  2881%5D%29%2C tensor%28%5B  96%2C  193%2C   42%2C   21%2C   59%2C   92%2C  200%2C  506%2C  108%2C   96%2C   21%2C   15%2C%0A          80%2C   59%2C  218%2C   13%2C 8965%2C   59%2C   ...7%2C    4%2C  768%2C   13%2C%0A        3576%2C  677%2C   86%2C  678%2C   64%2C  287%2C  193%2C  537%2C  265%2C   96%2C  151%2C  329%2C%0A           0%2C  679%2C   21%5D%29%2C tensor%28%5B  96%2C  336%2C   64%2C  141%2C  352%2C   47%2C   96%2C  104%2C  204%2C   96%2C   27%2C   61%2C%0A       ..." isContainer="True" shape="20000" />
<var name="test_output" type="DataFrame" qualifier="pandas.core.frame" value="%5B%27inst_id%27%2C %27predicted_label%27%5D %5B0            0              1.0%5D %5B1            1              1.0%5D %5B2            2              1.0%5D %5B3            3              1.0%5D %5B4            4              1.0%5D %5B...        ...              ...%5D %5B19995    19995              1.0%5D %5B19996    19996              1.0%5D %5B19997    19997              1.0%5D %5B19998    19998              1.0%5D %5B19999    19999              1.0%5D %5B%5D" isContainer="True" shape="(20000, 2)" />
<var name="tokenizer" type="RegexpTokenizer" qualifier="nltk.tokenize.regexp" value="RegexpTokenizer%28pattern=%27%5C%5Cw%2B%27%2C gaps=False%2C discard_empty=True%2C flags=re.UNICODE%7Cre.MULTILINE%7Cre.DOTALL%29" isContainer="True" />
<var name="train_data_path" type="str" qualifier="builtins" value="sentiment.train.csv" />
<var name="train_dataset" type="TextDataset" qualifier="__main__" value="%3C__main__.TextDataset object at 0x000001DF8112E120&gt;" isContainer="True" shape="160000" />
<var name="train_list" type="list" qualifier="builtins" value="%5B%28%5B   96     1   287    96   248    96     1   204 16850   291   110    24%2C    47    96   193   306    64    50    13    93%5D%2C array%281%29%29%2C %28%5B  96  884  234   21   96 1191 2472    7  693 1931 1926  268   96   96%2C   85 1688   96   21   67    7 1160  884 1578%5D%2C array%281%29%29%2C %28%5B  96    2  320   98 3569 7030   13   98    7  366   96  128 5745   14%2C  151   88  105 9655   96   88   96  123   59 1956  320   59  175   11%2C   15  128   88   96  140  105 9655 5834%5D%2C array%280%29%29%2C %28%5B  96    1  358   13  333  341   96   76 1775   83   96  241 1635 3070%2C   24   96   13 1196  729   61  305   95   96   76 1426   48    4  699%2C  117   96  115    4  486   24   96   96  356   76   96   62%5D%2C array%281%29%29%2C %28%5B  96 1427 1233   64  314  679   71   96    1  314 1131 1554 1555   98%2C  265 1905   96    1 7419   98 3204   87  310 1588  335   14  151  539%2C  232  287   14 1905  249    0    1    4  615  710   96   12   11%5D%2C array%281%29%29%2C %28%5B   96    64    88   314  1774  3195    24   345    24    96    26    96%2C    13  1178   341    ..." isContainer="True" shape="160000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000001DFDDC6E720&gt;" isContainer="True" shape="160000" />
<var name="vocab_size" type="int" qualifier="builtins" value="40547" />
<var name="word_ids" type="Tensor" qualifier="torch" value="tensor%28%5B  96%2C  345%2C 8879%2C 1977%2C  385%2C 1364%2C   80%2C 4134%2C   24%2C 1105%2C  680%2C   19%2C%0A          13%2C   96%2C   96%2C   96%2C   94%2C   61%2C    0%2C   21%2C   13%2C   61%2C   96%2C   24%2C%0A          96%2C   96%2C  180%5D%29" isContainer="True" shape="(27,)" />
<var name="word_to_index" type="dict" qualifier="builtins" value="%7B%270%27%3A 6860%2C %2700%27%3A 7030%2C %27000%27%3A 7717%2C %27001%27%3A 38825%2C %27007%27%3A 28099%2C %2700am%27%3A 25993%2C %2701%27%3A 24762%2C %2702%27%3A 29846%2C %2703%27%3A 11089%2C %27039%27%3A 38140%2C %2704%27%3A 8057%2C %2705%27%3A 32280%2C %2706%27%3A 30353%2C %2707%27%3A 7063%2C %2708%27%3A 24586%2C %2709%27%3A 22275%2C %270f%27%3A 37971%2C %271%27%3A 793%2C %2710%27%3A 1684%2C %27100%27%3A 870%2C %271000%27%3A 10968%2C %2710000%27%3A 31123%2C %271000s%27%3A 20833%2C %271000x%27%3A 33996%2C %271001%27%3A 7910%2C %27100k%27%3A 39109%2C %27100s%27%3A 25367%2C %27100th%27%3A 879%2C %27101%27%3A 7005%2C %27101st%27%3A 26963%2C %27102%27%3A 29336%2C %27103%27%3A 31073%2C %27104%27%3A 31072%2C %27105%27%3A 27289%2C %27106%27%3A 11911%2C %271066%27%3A 34895%2C %27107%27%3A 733%2C %27108%27%3A 15245%2C %27109%27%3A 34772%2C %2710k%27%3A 16506%2C %2710th%27%3A 24530%2C %2710x%27%3A 22765%2C %2710yr%27%3A 28921%2C %2711%27%3A 3831%2C %27110%27%3A 18961%2C %271100%27%3A 36651%2C %27111%27%3A 18962%2C %27112%27%3A 11145%2C %27113%27%3A 32771%2C %27114%27%3A 24557%2C %27115%27%3A 15244%2C %27116%27%3A 19634%2C %27117%27%3A 39597%2C %27118%27%3A 21937%2C %27119%27%3A 38487%2C %2711pm%27%3A 25792%2C %2711th%27%3A 10687%2C %2712%27%3A 1681%2C %27120%27%3A 14299%2C %271200%27%3A 13958%2C %27121%27%3A 33184%2C %27122%27%3A 38431%2C %27123%27%3A 18402%2C %27124%27%3A 39041%2C %27125%27%3A 25024%2C %27126%27%3A 39242%2C %27127%27%3A 35211%2C %27128%27%3A 37280%2C %27129%27%3A 39683%2C %2712th%27%3A 10890%2C %2712yr%27%3A 17641%2C %2713%27%3A 3671%2C %27130%27..." isContainer="True" shape="40547" />
<var name="word_to_index_file" type="str" qualifier="builtins" value="./models/1st_model_word_to_index.pkl" />
</xml>