{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Part 4: Attention-based classification\n",
    "\n",
    "This last part of homework 2 will have you _using_ the vectors we learned from your word2vec implementation to do classification. You should complete the initial word2vec part before before starting on this.\n",
    "\n",
    "Broadly, this last part of the homework consists of a few major steps:\n",
    "1. Load in the data, word vectors, and word-indexing\n",
    "2. Define the attention-based classification network\n",
    "3. Train your model at least one epoch (2+ is recommended though).\n",
    "4. Perform exploratory analyses on attention\n",
    "5. Test the effects of freezing the pre-trained word vectors (see homework PDF for details)\n",
    "\n",
    "After Step 2, you should be able to train your classifier implementation on a small percent of the dataset and verify that it's learning correctly. **Please note that this list is a general sketch and the homework PDF has the full list/description of to-dos and all your deliverables.**\n",
    "\n",
    "\n",
    "### Estimated performance times\n",
    "\n",
    "We designed this homework to be run on a laptop-grade CPU, so no GPU is required. If your primary computing device is a tablet or similar device, this homework can also be _developed_ on that device but then run on a more powerful machine in the Great Lakes cluster (for free). Such cases are the exception though. Following, we report on the estimated times from our reference implementation for longer-running or data-intensive pieces of the homework. Your timing may vary based on implementation design; major differences in time (e.g., 10x longer) usually point to a performance bug.\n",
    "\n",
    "* Reading data, tokenizing, and converting to ids: ~20 seconds\n",
    "* Training one epoch: ~18 minutes\n",
    "* Training one epoch using frozen embeddings: ~3 minutes\n",
    "* Evaluating on dev/test set: ~5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Attention plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the necessary parameters from the word2vec code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word-to-index mapping we used for word2vec and use the same type\n",
    "# of tokenizer. We'll need to use this to tokenize in the same way and keep \n",
    "# the same word-to-id mapping\n",
    "\n",
    "tokenizer = None\n",
    "word_to_index = None\n",
    "index_to_word = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Classifier Model\n",
    "\n",
    "Just like we did for word2vec, let's define a PyTorch `nn.Module` class here that will contain our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, embeddings_fname):\n",
    "        '''\n",
    "        Creates the new classifier model. embeddings_fname is a string containing the\n",
    "        filename with the saved pytorch parameters (the state dict) for the Embedding\n",
    "        object that should be used to initialize this class's word Embedding parameters\n",
    "        '''\n",
    "        super(DocumentAttentionClassifier, self).__init__()\n",
    "        \n",
    "        # Save the input arguments to the state\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create the Embedding object that will hold our word embeddings that we\n",
    "        # learned in word2vec. This embedding object should have the same size\n",
    "        # as what we learned before. However, we don't to start from scratch! \n",
    "        # Once created, load the saved (word2vec-based) parameters into the object\n",
    "        # using load_state_dict.\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Define the attention heads. You have two options:\n",
    "        # \n",
    "        # 1) the worse way to implement this is to define your heads using an Embedding\n",
    "        #    and then access them individually later in forward(). This will be slower\n",
    "        #    but will probably still work \n",
    "        #\n",
    "        # 2) the ideal way is to think of your attention heads as rows in a matrix--\n",
    "        #    just like we do for word2vec. While this is kind of the same as how\n",
    "        #    we represent things like in an Embedding, the key difference is that we\n",
    "        #    can now use **matrix operations** to calculate the different r and a\n",
    "        #    vectors, which will be much faster (and less code). To do this, you'll\n",
    "        #    need to represent the attention heads as a Tensor directly (not a layer)\n",
    "        #    and make sure pytorch runs gradient descent on these parameters.\n",
    "        #\n",
    "        #  It's up to you which to use, but try option 2 first and see what you do \n",
    "        #  in the forward() function\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Define the layer that goes from the concatenated attention heads' outputs\n",
    "        # to the single output value. We'll push this output value through the sigmoid\n",
    "        # to get our prediction\n",
    "\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def forward(self, word_ids):\n",
    "        \n",
    "        # Pro Tipâ„¢: when implementing this forward pass, try playing around with pytorch\n",
    "        # tensors in a jupyter notebook by making \"fake\" versions of them. For example:\n",
    "        #\n",
    "        # word_embeds = torch.Tensor([[1,6,2], [9,1,7]])\n",
    "        #\n",
    "        # If you have two word embeddings of length 3, how can you define the attention\n",
    "        # heads to get the 'r' vector? Trying things out in the simple case will let you\n",
    "        # quickly verify the sequence of operations you want to run, e.g., that you can take\n",
    "        # the softmax of the 'r' vector to get the 'a' vector and it has the right shape\n",
    "        # and values\n",
    "        \n",
    "        # Hint 1: If you're representing attention using Option 2, most of this code is just \n",
    "        #         matrix multiplications\n",
    "\n",
    "        # Hint 2: Most of your time is going to be spent figuring out shape errors and what\n",
    "        #         operations you need to do to get the right outputs. This is normal.\n",
    "        \n",
    "        # Hint 3: This is the hardest part of this last part of the homework.\n",
    "\n",
    "        \n",
    "        # Get the word embeddings for the ids\n",
    "\n",
    "\n",
    "        # Calcuate the 'r' vectors which are the dot product of each attention head\n",
    "        # with each word embedding. You should be getting a tensor that has this\n",
    "        # dot product back out---remember this vector is capturing how much the \n",
    "        # head thinks the vector is relevant for the task\n",
    "\n",
    "\n",
    "        # Calcuate the softmax of the 'r' vector, which call 'a'. This will give us\n",
    "        # a probability distribution over the tokens for each head. Be sure to check\n",
    "        # that the softmax is being calculated over the right axis/dimension of the \n",
    "        # data (You should see probability values that sum to 1 for each head's \n",
    "        # ratings across all the tokens)        \n",
    "\n",
    "\n",
    "        # Calculate the re-weighting of the word embeddings for each head's attention\n",
    "        # weight and sum the reweighted sequence for each head into a single vector.\n",
    "        # This should give you n_heads vectors that each have embedding_size length.\n",
    "        # Note again that each head should give you a different weighting of the\n",
    "        # input word embeddings\n",
    "\n",
    "        \n",
    "        # Create a single vector that has all n_heads' attention-weighted vectors\n",
    "        # as one single vector. We need this one-long-vector shape so that we \n",
    "        # can pass all these vectors as input into a layer.\n",
    "        #\n",
    "        # NOTE: if you're doing Option 2 for representing attention, you don't \n",
    "        # actually need to create a new vector (which is very inefficient).\n",
    "        # Instead, you can create a new *view* of the same data that reshapes the\n",
    "        # different heads' vectors so it looks like one long vector. \n",
    "\n",
    "\n",
    "        # Pass the side-by-side attention-weighted vectors through your linear\n",
    "        # layer to get some output activation.\n",
    "        #\n",
    "        # NOTE: if you're feeling adventurous, try adding an extra layer here\n",
    "        # which will allow you different attention-weighted vectors to interact\n",
    "        # in making the model decision\n",
    "\n",
    "        \n",
    "        \n",
    "        # Return the sigmoid of the output activation *and* the attention \n",
    "        # weights for each head. We'll need these later for visualization\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the datasets \n",
    "\n",
    "You can keep these as pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_train_df = pd.DataFrame()\n",
    "sent_dev_df = pd.DataFrame()\n",
    "sent_test_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each dataset into a list of tuples of the form `([word-ids,...], label)`. Both the word ids and the label should be numpy arrays so they will get converted into Tensors by our data loader. Note that you did something very similar for creating the word2vec training data. This process will require tokenizing the data in the same way as you did for word2vec and using the same word-to-id mapping (both of which you loaded/created above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "dev_list = []\n",
    "test_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this worked you should see XXXX train, XXXX dev, and XXX test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_list), len(dev_list), len(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the code training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll evaluate periodically so before we start training, let's define a function that takes in some evaluation data (e.g., the dev or test sets) and computes the F1 score on that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(model, eval_data):\n",
    "    '''\n",
    "    Scores the model on the evaluation data and returns the F1\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The training code will look surprisingly similar to your word2vec code. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. **see below:** Initializes Weights & Biases and periodically write our running-sum of the loss \n",
    "5. Train your model \n",
    "\n",
    "For step 4, in addition to writing the loss, you should write the F1 score on the dev set to the `wandb` as well, using the specified number of steps.\n",
    "\n",
    "**NOTE:** In this training, you'll use a batch size of 1, which will make your life _much_ simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set your training stuff, hyperparameters, models, etc. here\n",
    "\n",
    "# TODO: initialize weights and biases (wandb) here\n",
    "\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "for epoch in []:\n",
    "\n",
    "    loss_sum = 0\n",
    "    \n",
    "    # TODO: use your DataLoader to iterate over the data\n",
    "    for step, data in enumerate([]):\n",
    "\n",
    "        # NOTE: since you created the data np.array instances,\n",
    "        # these have now been converted to Tensor objects for us\n",
    "        word_ids, label = data    \n",
    "        \n",
    "        # TODO: Fill in all the training details here\n",
    "\n",
    "        \n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to weights and biases. Be sure\n",
    "        # to reset the running sum after reporting it.\n",
    "        # \n",
    "        # Optional TODO: periodically evaluate the model on the dev set and \n",
    "        # report it to weights and biases\n",
    "        \n",
    "        \n",
    "        # TODO: it can be helpful to add some early stopping here after\n",
    "        # a fixed number of steps (e.g., if step > max_steps)\n",
    "        \n",
    "        \n",
    "\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting what the model learned\n",
    "\n",
    "In this last bit of the homework you should look at the model's attention weights. We've written a visualization helper function below that will plot the attention weights. You'll need to fill in the `get_label_and_weights` method that uses the model to classify some new text and structures the attention output in a way that's specified. \n",
    "\n",
    "**NOTE:** most of the code for `get_label_and_weights` is code you've already written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_and_weights(text):\n",
    "    '''\n",
    "    Classifies the text (requires tokenizing, etc.) and returns (1) the classification label, \n",
    "    (2) the tokenized words in the model's vocabulary, \n",
    "    and (3) the attention weights over the in-vocab tokens as a numpy array. Note that the\n",
    "    attention weights will be a matrix, depending on how many heads were used in training.\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(words, attention_weights):\n",
    "    '''\n",
    "    Makes a heatmap figure that visualizes the attention weights for an item.\n",
    "    Attention weights should be a numpy array that has the shape (num_words, num_heads)\n",
    "    '''\n",
    "    fig, ax = plt.subplots() \n",
    "    # Rescale image size based on the input length\n",
    "    fig.set_size_inches((len(words), 4))    \n",
    "    im = ax.imshow(attention_weights.T)\n",
    "\n",
    "    head_labels = [ 'head-%d' % h for h in range(attention_weights.shape[1])]\n",
    "    ax.set_xticks(np.arange(len(words))) # , labels=words)\n",
    "    ax.set_yticks(np.arange(len(head_labels))) #, labels=head_labels)\n",
    "\n",
    "    # Rotate the word labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add the words and axis labels\n",
    "    ax.set_yticklabels(labels=range(attention_weights.shape[1]), fontsize=16)\n",
    "    ax.set_ylabel('Attention Head', fontsize=16)\n",
    "    ax.set_xticklabels(labels=words, fontsize=16)\n",
    "\n",
    "    # Add a color bar to show probability scaling\n",
    "    cb = fig.colorbar(im, ax=ax, label='Probability', pad = 0.01)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    cb.set_label(label='Probability',size=16)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example messages to try visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Just as I remembered it, one of my favorites from childhood! Great condition, very happy to have this to share with my daughter. Packaging was so nice and was received quickly.'\n",
    "pred, tokens, attn = get_label_and_weights(s)\n",
    "visualize_attention(tokens, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''\n",
    "I'm a big fan of his, and I have to say that this was a BIG letdown. It features: Stilted dialogue, no character development, no suspense, no description of Indian tradition and poor editing.\\n\\nAvoid at all costs.\n",
    "'''\n",
    "pred, tokens, attn = get_label_and_weights(s)\n",
    "visualize_attention(tokens, attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional TODOs:\n",
    "\n",
    "### How many instances do we need to learn?\n",
    "\n",
    "Since the word2vec vectors capture word meaning, do we need to see a lot of examples to train an effective classifier? Maybe we can get away with fewer (or not?). Try making a plot that shows the performance of training on 1 epoch with varying numbers of training examples. What if we just had 10 examples? 100? 1000? \n",
    "\n",
    "### Make the \"important word\" vectors learn different attentions\n",
    "\n",
    "If your attention vectors are learning to look at similar words, we could try to add some structure to how we learn so these vectors become dissimilar. One idea is to penalize the model according to how similar each head's vector is to the others. You could use cosine similarity or MSE or any weighting function. For example, you could add the cosine similarities of each pair to the `loss` so that the model benefits from learning orthoginal or dissimlar vectors. Would this help? \n",
    "\n",
    "### Add more layers to the network\n",
    "\n",
    "This is the second easiest one, but can be fun. What if you add more layers after you aggregate? Does letting the different attention heads' representations interact give better performance? Find out!\n",
    "\n",
    "### Change the learning rate dynamically\n",
    "\n",
    "We have a fixed learning rate, but what if we wanted to decrease the learning rate as the model starts to converge? In many cases, this can help the model take smaller but more precise steps towards the best possible parameters. PyTorch supports this with _learning rate schedulers_ that tell pytorch how and when to change the learning rate. See if you can get a better performance using a scheduler!\n",
    "\n",
    "### Add support for batch sizes > 1\n",
    "\n",
    "This is non-trivial but will increase training speed _a lot_. The main issue with increasing batch sizes is that our input sequences (the word ids) in a batch will have different lengths. Under the hood, pytorch is turning your code into a series of very fast matrix operations. However, if those matrices suddenly have difference sizes, the math no longer works. As a result developers (like us) have to do a few things:\n",
    "\n",
    "* We need to _pad_ the sequences with empty values so that all sequences have the same length. You could do this by  adding an extra word ID that is the \"empty token\" and make sure its values are set to 0 (so it won't interact with anything)\n",
    "\n",
    "* Set up a collate function in our DataLoader that automatically pads each batch's data based on the longest length in the batch\n",
    "\n",
    "* At inference time, it's also efficient to mask part of the sequence that's the padded part so we ignore the computations for that part in anything downstream. Depending on how you set it up, you may be able to avoid this step.\n",
    "\n",
    "If you want to dig into this, you might see some of the documentation around packed and padded sequences in pytorch. You won't need to use these functions but they can provide more context for what's happening and why.\n",
    "\n",
    "\n",
    "### Add positional information to the word embeddings\n",
    "\n",
    "Right now our model doesn't know much about which order the words are in. What if we helped the model in this? One way that people have done this is to _add_ some positional embedding to the word embedding, where the positional embedding represents which position in the sequence is in. There are many complicated schemes for this, but one potential idea is to _learn the positional embeddings_. You would keep a separate `Embedding` object for positions with the number of positions up to the length of your longest sequence in the data. Then for the word in the first position, you add `position_embeddings(0)` to it. You can definitely speed that up by passing in a sequence of the positions in the current input and, conveniently, pytorch will let you easily add all the position embeddings to the word embeddings easily (no for loop required).\n",
    "\n",
    "Will it help here? I have no idea but I'm curious.\n",
    "\n",
    "\n",
    "**No extra credit is given for these; they're just for folks who want to explore more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
