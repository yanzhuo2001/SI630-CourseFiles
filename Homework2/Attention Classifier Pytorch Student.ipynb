{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Part 4: Attention-based classification\n",
    "\n",
    "This last part of homework 2 will have you _using_ the vectors we learned from your word2vec implementation to do classification. You should complete the initial word2vec part before before starting on this.\n",
    "\n",
    "Broadly, this last part of the homework consists of a few major steps:\n",
    "1. Load in the data, word vectors, and word-indexing\n",
    "2. Define the attention-based classification network\n",
    "3. Train your model at least one epoch (2+ is recommended though).\n",
    "4. Perform exploratory analyses on attention\n",
    "5. Test the effects of freezing the pre-trained word vectors (see homework PDF for details)\n",
    "\n",
    "After Step 2, you should be able to train your classifier implementation on a small percent of the dataset and verify that it's learning correctly. **Please note that this list is a general sketch and the homework PDF has the full list/description of to-dos and all your deliverables.**\n",
    "\n",
    "\n",
    "### Estimated performance times\n",
    "\n",
    "We designed this homework to be run on a laptop-grade CPU, so no GPU is required. If your primary computing device is a tablet or similar device, this homework can also be _developed_ on that device but then run on a more powerful machine in the Great Lakes cluster (for free). Such cases are the exception though. Following, we report on the estimated times from our reference implementation for longer-running or data-intensive pieces of the homework. Your timing may vary based on implementation design; major differences in time (e.g., 10x longer) usually point to a performance bug.\n",
    "\n",
    "* Reading data, tokenizing, and converting to ids: ~20 seconds\n",
    "* Training one epoch: ~18 minutes\n",
    "* Training one epoch using frozen embeddings: ~3 minutes\n",
    "* Evaluating on dev/test set: ~5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Attention plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the necessary parameters from the word2vec code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word-to-index mapping we used for word2vec and use the same type\n",
    "# of tokenizer. We'll need to use this to tokenize in the same way and keep \n",
    "# the same word-to-id mapping\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# 假设你的映射和分词器保存在pickle文件中\n",
    "word_to_index_file = './models/1st_model_word_to_index.pkl'\n",
    "index_to_word_file = 'index_to_word.pkl'\n",
    "\n",
    "# 加载word_to_index映射\n",
    "with open(word_to_index_file, 'rb') as f:\n",
    "    word_to_index = pickle.load(f)\n",
    "\n",
    "# 加载index_to_word映射\n",
    "with open(index_to_word_file, 'rb') as f:\n",
    "    index_to_word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Classifier Model\n",
    "\n",
    "Just like we did for word2vec, let's define a PyTorch `nn.Module` class here that will contain our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, embeddings_fname, freeze_embeddings=True):\n",
    "        '''\n",
    "        Creates the new classifier model. embeddings_fname is a string containing the\n",
    "        filename with the saved pytorch parameters (the state dict) for the Embedding\n",
    "        object that should be used to initialize this class's word Embedding parameters\n",
    "        '''\n",
    "        super(DocumentAttentionClassifier, self).__init__()\n",
    "        \n",
    "        # Save the input arguments to the state\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        # Create the Embedding object that will hold our word embeddings that we\n",
    "        # learned in word2vec. This embedding object should have the same size\n",
    "        # as what we learned before. However, we don't to start from scratch! \n",
    "        # Once created, load the saved (word2vec-based) parameters into the object\n",
    "        # using load_state_dict.\n",
    "        \n",
    "        # Create the Embedding object\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        # Load pre-trained word embeddings\n",
    "        self.embeddings.load_state_dict(torch.load(embeddings_fname))\n",
    "        # 决定是否冻结词嵌入层\n",
    "        if freeze_embeddings:\n",
    "            for param in self.embedding.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Define the attention heads. You have two options:\n",
    "        # \n",
    "        # 1) the worse way to implement this is to define your heads using an Embedding\n",
    "        #    and then access them individually later in forward(). This will be slower\n",
    "        #    but will probably still work \n",
    "        #\n",
    "        # 2) the ideal way is to think of your attention heads as rows in a matrix--\n",
    "        #    just like we do for word2vec. While this is kind of the same as how\n",
    "        #    we represent things like in an Embedding, the key difference is that we\n",
    "        #    can now use **matrix operations** to calculate the different r and a\n",
    "        #    vectors, which will be much faster (and less code). To do this, you'll\n",
    "        #    need to represent the attention heads as a Tensor directly (not a layer)\n",
    "        #    and make sure pytorch runs gradient descent on these parameters.\n",
    "        #\n",
    "        #  It's up to you which to use, but try option 2 first and see what you do \n",
    "        #  in the forward() function\n",
    "        \n",
    "        self.attention_heads = nn.Parameter(torch.randn(num_heads, embedding_size))\n",
    "        \n",
    "        # Define the layer that goes from the concatenated attention heads' outputs\n",
    "        # to the single output value. We'll push this output value through the sigmoid\n",
    "        # to get our prediction\n",
    "\n",
    "        self.linear = nn.Linear(num_heads * embedding_size, 1)  # Adjust the size accordingly\n",
    "    \n",
    "\n",
    "    def forward(self, word_ids):\n",
    "        \n",
    "        # Pro Tip™: when implementing this forward pass, try playing around with pytorch\n",
    "        # tensors in a jupyter notebook by making \"fake\" versions of them. For example:\n",
    "        #\n",
    "        # word_embeds = torch.Tensor([[1,6,2], [9,1,7]])\n",
    "        #\n",
    "        # If you have two word embeddings of length 3, how can you define the attention\n",
    "        # heads to get the 'r' vector? Trying things out in the simple case will let you\n",
    "        # quickly verify the sequence of operations you want to run, e.g., that you can take\n",
    "        # the softmax of the 'r' vector to get the 'a' vector and it has the right shape\n",
    "        # and values\n",
    "        \n",
    "        # Hint 1: If you're representing attention using Option 2, most of this code is just \n",
    "        #         matrix multiplications\n",
    "\n",
    "        # Hint 2: Most of your time is going to be spent figuring out shape errors and what\n",
    "        #         operations you need to do to get the right outputs. This is normal.\n",
    "        \n",
    "        # Hint 3: This is the hardest part of this last part of the homework.\n",
    "\n",
    "        \n",
    "        # Get the word embeddings for the ids\n",
    "        \n",
    "        embeds = self.embeddings(word_ids)\n",
    "\n",
    "        # Calcuate the 'r' vectors which are the dot product of each attention head\n",
    "        # with each word embedding. You should be getting a tensor that has this\n",
    "        # dot product back out---remember this vector is capturing how much the \n",
    "        # head thinks the vector is relevant for the task\n",
    "        \n",
    "        relevance_scores = torch.matmul(embeds, self.attention_heads.t())  # [batch_size, seq_length, num_heads]\n",
    "\n",
    "        # Calcuate the softmax of the 'r' vector, which call 'a'. This will give us\n",
    "        # a probability distribution over the tokens for each head. Be sure to check\n",
    "        # that the softmax is being calculated over the right axis/dimension of the \n",
    "        # data (You should see probability values that sum to 1 for each head's \n",
    "        # ratings across all the tokens)   \n",
    "        \n",
    "        attention_weights = F.softmax(relevance_scores, dim=1)  # [batch_size, seq_length, num_heads]\n",
    "\n",
    "        # Calculate the re-weighting of the word embeddings for each head's attention\n",
    "        # weight and sum the reweighted sequence for each head into a single vector.\n",
    "        # This should give you n_heads vectors that each have embedding_size length.\n",
    "        # Note again that each head should give you a different weighting of the\n",
    "        # input word embeddings\n",
    "        weighted_embeds = torch.einsum('bsn,bse->bne', attention_weights, embeds)  # [batch_size, num_heads, embedding_size]\n",
    "\n",
    "        \n",
    "        # Create a single vector that has all n_heads' attention-weighted vectors\n",
    "        # as one single vector. We need this one-long-vector shape so that we \n",
    "        # can pass all these vectors as input into a layer.\n",
    "        #\n",
    "        # NOTE: if you're doing Option 2 for representing attention, you don't \n",
    "        # actually need to create a new vector (which is very inefficient).\n",
    "        # Instead, you can create a new *view* of the same data that reshapes the\n",
    "        # different heads' vectors so it looks like one long vector. \n",
    "        \n",
    "        concatenated = weighted_embeds.view(weighted_embeds.size(0), -1)  # [batch_size, num_heads * embedding_size]\n",
    "\n",
    "\n",
    "        # Pass the side-by-side attention-weighted vectors through your linear\n",
    "        # layer to get some output activation.\n",
    "        #\n",
    "        # NOTE: if you're feeling adventurous, try adding an extra layer here\n",
    "        # which will allow you different attention-weighted vectors to interact\n",
    "        # in making the model decision\n",
    "        \n",
    "        output = self.linear(concatenated)  # [batch_size, 1]\n",
    "        \n",
    "        # Return the sigmoid of the output activation *and* the attention \n",
    "        # weights for each head. We'll need these later for visualization\n",
    "        probability = torch.sigmoid(output)\n",
    "        \n",
    "        return probability, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the datasets \n",
    "\n",
    "You can keep these as pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你的数据存储在CSV文件中\n",
    "train_data_path = 'sentiment.train.csv'\n",
    "dev_data_path = 'sentiment.dev.csv'\n",
    "test_data_path = 'sentiment.test.csv'\n",
    "\n",
    "# 使用pandas读取数据\n",
    "sent_train_df = pd.read_csv(train_data_path)\n",
    "sent_dev_df = pd.read_csv(dev_data_path)\n",
    "sent_test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each dataset into a list of tuples of the form `([word-ids,...], label)`. Both the word ids and the label should be numpy arrays so they will get converted into Tensors by our data loader. Note that you did something very similar for creating the word2vec training data. This process will require tokenizing the data in the same way as you did for word2vec and using the same word-to-id mapping (both of which you loaded/created above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_ids(text, tokenizer, word_to_index):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = [word_to_index.get(token, word_to_index['<UNK>']) for token in tokens]\n",
    "    return np.array(ids)\n",
    "\n",
    "def prepare_data(df, tokenizer, word_to_index):\n",
    "    data_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        word_ids = tokenize_and_convert_to_ids(row['text'], tokenizer, word_to_index)\n",
    "        label = np.array(row['label'])\n",
    "        data_list.append((word_ids, label))\n",
    "    return data_list\n",
    "\n",
    "def prepare_test_data(df, tokenizer, word_to_index):\n",
    "    test_data = []\n",
    "    for text in df['text']:\n",
    "        word_ids = tokenize_and_convert_to_ids(text, tokenizer, word_to_index)\n",
    "        test_data.append(torch.tensor(word_ids, dtype=torch.long))\n",
    "    return test_data\n",
    "\n",
    "# 使用定义的函数准备测试数据\n",
    "test_data = prepare_test_data(sent_test_df, tokenizer, word_to_index)\n",
    "\n",
    "# 现在，假设你的数据帧已经正确加载\n",
    "# 以下是如何应用上述函数的例子\n",
    "train_list = prepare_data(sent_train_df, tokenizer, word_to_index)\n",
    "dev_list = prepare_data(sent_dev_df, tokenizer, word_to_index)\n",
    "test_list = prepare_test_data(sent_test_df, tokenizer, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this worked you should see XXXX train, XXXX dev, and XXX test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_list), len(dev_list), len(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the code training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll evaluate periodically so before we start training, let's define a function that takes in some evaluation data (e.g., the dev or test sets) and computes the F1 score on that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(model, eval_data):\n",
    "    '''\n",
    "    Scores the model on the evaluation data and returns the F1\n",
    "    '''\n",
    "    model.eval()  # 切换到评估模式\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for word_ids, labels in eval_data:\n",
    "            outputs = model(word_ids).squeeze()\n",
    "            preds = torch.round(torch.sigmoid(outputs))  # 将输出转换为0或1的预测\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    model.train()  # 切换回训练模式\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The training code will look surprisingly similar to your word2vec code. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. **see below:** Initializes Weights & Biases and periodically write our running-sum of the loss \n",
    "5. Train your model \n",
    "\n",
    "For step 4, in addition to writing the loss, you should write the F1 score on the dev set to the `wandb` as well, using the specified number of steps.\n",
    "\n",
    "**NOTE:** In this training, you'll use a batch size of 1, which will make your life _much_ simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word_ids, label = self.data[idx]\n",
    "        return torch.tensor(word_ids, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# TODO: Set your training stuff, hyperparameters, models, etc. here\n",
    "vocab_size = len(word_to_index)  # 假设word_to_index是你的词汇到索引的映射\n",
    "embedding_size = 50  # 或你在word2vec中使用的任何尺寸\n",
    "num_heads = 4  # 注意力头的数量\n",
    "batch_size = 1\n",
    "learning_rate = 5e-5\n",
    "epochs = 1  # 或更多，根据需要\n",
    "max_steps = 10000  # 设置最大步数\n",
    "patience = 1000  # 设置耐心值，即在这么多步之后如果没有提升则停止训练\n",
    "best_f1 = 0.0  # 记录迄今为止最好的F1分数\n",
    "steps_since_improvement = 0  # 记录自上次性能提升以来的步数\n",
    "# 初始化收集损失和F1分数的列表\n",
    "losses = []\n",
    "f1_scores = []\n",
    "\n",
    "embeddings_file_path = './models/1st_model_embeddings_state_dict.pt'\n",
    "model = DocumentAttentionClassifier(vocab_size, embedding_size, num_heads, \"embeddings_file_path\")\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# TODO: initialize weights and biases (wandb) here\n",
    "wandb.init(project=\"attention_classifier\", entity=\"yanzhuo\")\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "# 创建DataLoader实例\n",
    "train_dataset = TextDataset(train_list)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dev_dataset = TextDataset(dev_list)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for step, (word_ids, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        # 早停条件之一：达到最大步数\n",
    "        if step >= max_steps:\n",
    "            print(f\"Stopping early at step {step} due to reaching max steps.\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(word_ids).squeeze()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:  # 每500步报告一次\n",
    "            dev_f1 = run_eval(model, dev_loader)\n",
    "            wandb.log({\"loss\": running_loss / 500, \"f1_score\": dev_f1})\n",
    "            losses.append(running_loss / 500)  # 收集平均损失\n",
    "            f1_scores.append(dev_f1)  # 收集F1分数\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # 早停条件之二：性能没有提升\n",
    "            if dev_f1 > best_f1:\n",
    "                best_f1 = dev_f1\n",
    "                steps_since_improvement = 0\n",
    "                # 保存当前最好模型\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            else:\n",
    "                steps_since_improvement += 500\n",
    "                if steps_since_improvement >= patience:\n",
    "                    print(f\"Stopping early at step {step} due to no improvement in {patience} steps.\")\n",
    "                    break\n",
    "\n",
    "        # 早停条件之一：在for循环内部提前终止循环\n",
    "        if step >= max_steps or steps_since_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 绘制损失\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制F1分数\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(f1_scores, label='F1 Score', color='orange')\n",
    "plt.title('F1 Score')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 训练结束后加载最佳模型\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for word_ids in test_data:\n",
    "        # 由于我们的模型是基于批处理的，我们需要添加一个批次维度\n",
    "        output, _ = model(word_ids.unsqueeze(0))  # 假设模型返回预测和注意力权重\n",
    "        prediction = torch.round(torch.sigmoid(output)).item()  # 将输出转换为类别标签\n",
    "        predictions.append(prediction)\n",
    "\n",
    "# 将预测结果添加到DataFrame中\n",
    "sent_test_df['predicted_label'] = predictions\n",
    "\n",
    "# 保存到CSV文件中\n",
    "sent_test_df.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "# 打印前几个预测结果作为验证\n",
    "print(sent_test_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 初始化收集损失和F1分数的列表\n",
    "losses = []\n",
    "f1_scores = []\n",
    "\n",
    "model1 = DocumentAttentionClassifier(vocab_size, embedding_size, num_heads, \"embeddings_file_path\", False)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "# TODO: initialize weights and biases (wandb) here\n",
    "wandb.init(project=\"attention_classifier2\", entity=\"yanzhuo\")\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    model1.train()\n",
    "    running_loss = 0.0\n",
    "    for step, (word_ids, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        # 早停条件之一：达到最大步数\n",
    "        if step >= max_steps:\n",
    "            print(f\"Stopping early at step {step} due to reaching max steps.\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model1(word_ids).squeeze()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:  \n",
    "            dev_f1 = run_eval(model1, dev_loader)\n",
    "            wandb.log({\"loss\": running_loss / 500, \"f1_score\": dev_f1})\n",
    "            losses.append(running_loss / 500)  # 收集平均损失\n",
    "            f1_scores.append(dev_f1)  # 收集F1分数\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # 早停条件之二：性能没有提升\n",
    "            if dev_f1 > best_f1:\n",
    "                best_f1 = dev_f1\n",
    "                steps_since_improvement = 0\n",
    "                # 保存当前最好模型\n",
    "                torch.save(model1.state_dict(), \"best_model1.pth\")\n",
    "            else:\n",
    "                steps_since_improvement += 500\n",
    "                if steps_since_improvement >= patience:\n",
    "                    print(f\"Stopping early at step {step} due to no improvement in {patience} steps.\")\n",
    "                    break\n",
    "\n",
    "        # 早停条件之一：在for循环内部提前终止循环\n",
    "        if step >= max_steps or steps_since_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 绘制损失\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制F1分数\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(f1_scores, label='F1 Score', color='orange')\n",
    "plt.title('F1 Score')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 训练结束后加载最佳模型\n",
    "model1.load_state_dict(torch.load(\"best_model1.pth\"))\n",
    "model1.eval()  # 切换到评估模式"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting what the model learned\n",
    "\n",
    "In this last bit of the homework you should look at the model's attention weights. We've written a visualization helper function below that will plot the attention weights. You'll need to fill in the `get_label_and_weights` method that uses the model to classify some new text and structures the attention output in a way that's specified. \n",
    "\n",
    "**NOTE:** most of the code for `get_label_and_weights` is code you've already written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_and_weights(text):\n",
    "    '''\n",
    "    Classifies the text (requires tokenizing, etc.) and returns (1) the classification label, \n",
    "    (2) the tokenized words in the model's vocabulary, \n",
    "    and (3) the attention weights over the in-vocab tokens as a numpy array. Note that the\n",
    "    attention weights will be a matrix, depending on how many heads were used in training.\n",
    "    '''\n",
    "    # 分词并转换为ID\n",
    "    word_ids = tokenize_and_convert_to_ids(text, tokenizer, word_to_index)\n",
    "    # 转换为PyTorch tensor并添加一个批次维度\n",
    "    word_ids_tensor = torch.tensor([word_ids], dtype=torch.long)\n",
    "    \n",
    "    # 确保模型处于评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 获取模型的输出，假设模型修改为同时返回分类概率和注意力权重\n",
    "        probs, attention_weights = model(word_ids_tensor)\n",
    "        # 由于我们只处理一个示例，我们移除批次维度\n",
    "        probs = probs.squeeze(0)\n",
    "        attention_weights = attention_weights.squeeze(0)\n",
    "        \n",
    "        # 获取最终的分类标签\n",
    "        label = torch.round(torch.sigmoid(probs)).item()\n",
    "        \n",
    "        # 将单词ID转换回单词\n",
    "        tokens = [index_to_word.get(id, '<UNK>') for id in word_ids]\n",
    "        \n",
    "        # 将注意力权重转换为numpy数组\n",
    "        attention_weights_np = attention_weights.cpu().numpy()\n",
    "        \n",
    "    return label, tokens, attention_weights_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(words, attention_weights):\n",
    "    '''\n",
    "    Makes a heatmap figure that visualizes the attention weights for an item.\n",
    "    Attention weights should be a numpy array that has the shape (num_words, num_heads)\n",
    "    '''\n",
    "    fig, ax = plt.subplots() \n",
    "    # Rescale image size based on the input length\n",
    "    fig.set_size_inches((len(words), 4))    \n",
    "    im = ax.imshow(attention_weights.T)\n",
    "\n",
    "    head_labels = [ 'head-%d' % h for h in range(attention_weights.shape[1])]\n",
    "    ax.set_xticks(np.arange(len(words))) # , labels=words)\n",
    "    ax.set_yticks(np.arange(len(head_labels))) #, labels=head_labels)\n",
    "\n",
    "    # Rotate the word labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add the words and axis labels\n",
    "    ax.set_yticklabels(labels=range(attention_weights.shape[1]), fontsize=16)\n",
    "    ax.set_ylabel('Attention Head', fontsize=16)\n",
    "    ax.set_xticklabels(labels=words, fontsize=16)\n",
    "\n",
    "    # Add a color bar to show probability scaling\n",
    "    cb = fig.colorbar(im, ax=ax, label='Probability', pad = 0.01)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    cb.set_label(label='Probability',size=16)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example messages to try visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Just as I remembered it, one of my favorites from childhood! Great condition, very happy to have this to share with my daughter. Packaging was so nice and was received quickly.'\n",
    "pred, tokens, attn = get_label_and_weights(s)\n",
    "visualize_attention(tokens, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''\n",
    "I'm a big fan of his, and I have to say that this was a BIG letdown. It features: Stilted dialogue, no character development, no suspense, no description of Indian tradition and poor editing.\\n\\nAvoid at all costs.\n",
    "'''\n",
    "pred, tokens, attn = get_label_and_weights(s)\n",
    "visualize_attention(tokens, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 随机选择两个正样本和两个负样本\n",
    "positive_samples = sent_dev_df[sent_dev_df['label'] == 1].sample(2)\n",
    "negative_samples = sent_dev_df[sent_dev_df['label'] == 0].sample(2)\n",
    "\n",
    "# 合并选定的样本\n",
    "selected_samples = pd.concat([positive_samples, negative_samples])\n",
    "\n",
    "# 获取样本文本\n",
    "texts = selected_samples['text'].tolist()\n",
    "\n",
    "for text in texts:\n",
    "    label, tokens, attn = get_label_and_weights(text)\n",
    "    print(f\"Text: {text[:50]}...\")  # 打印部分文本以供参考\n",
    "    print(f\"Predicted Label: {label}\")\n",
    "    visualize_attention(tokens, attn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 示例：手动编写一个模棱两可的样本\n",
    "sample_text = \"Although this product arrived on time and was nicely packaged, I found its quality lacking and performance subpar compared to other brands.\"\n",
    "label, tokens, attn = get_label_and_weights(sample_text)\n",
    "print(f\"Predicted Label: {label}\")\n",
    "visualize_attention(tokens, attn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional TODOs:\n",
    "\n",
    "### How many instances do we need to learn?\n",
    "\n",
    "Since the word2vec vectors capture word meaning, do we need to see a lot of examples to train an effective classifier? Maybe we can get away with fewer (or not?). Try making a plot that shows the performance of training on 1 epoch with varying numbers of training examples. What if we just had 10 examples? 100? 1000? \n",
    "\n",
    "### Make the \"important word\" vectors learn different attentions\n",
    "\n",
    "If your attention vectors are learning to look at similar words, we could try to add some structure to how we learn so these vectors become dissimilar. One idea is to penalize the model according to how similar each head's vector is to the others. You could use cosine similarity or MSE or any weighting function. For example, you could add the cosine similarities of each pair to the `loss` so that the model benefits from learning orthoginal or dissimlar vectors. Would this help? \n",
    "\n",
    "### Add more layers to the network\n",
    "\n",
    "This is the second easiest one, but can be fun. What if you add more layers after you aggregate? Does letting the different attention heads' representations interact give better performance? Find out!\n",
    "\n",
    "### Change the learning rate dynamically\n",
    "\n",
    "We have a fixed learning rate, but what if we wanted to decrease the learning rate as the model starts to converge? In many cases, this can help the model take smaller but more precise steps towards the best possible parameters. PyTorch supports this with _learning rate schedulers_ that tell pytorch how and when to change the learning rate. See if you can get a better performance using a scheduler!\n",
    "\n",
    "### Add support for batch sizes > 1\n",
    "\n",
    "This is non-trivial but will increase training speed _a lot_. The main issue with increasing batch sizes is that our input sequences (the word ids) in a batch will have different lengths. Under the hood, pytorch is turning your code into a series of very fast matrix operations. However, if those matrices suddenly have difference sizes, the math no longer works. As a result developers (like us) have to do a few things:\n",
    "\n",
    "* We need to _pad_ the sequences with empty values so that all sequences have the same length. You could do this by  adding an extra word ID that is the \"empty token\" and make sure its values are set to 0 (so it won't interact with anything)\n",
    "\n",
    "* Set up a collate function in our DataLoader that automatically pads each batch's data based on the longest length in the batch\n",
    "\n",
    "* At inference time, it's also efficient to mask part of the sequence that's the padded part so we ignore the computations for that part in anything downstream. Depending on how you set it up, you may be able to avoid this step.\n",
    "\n",
    "If you want to dig into this, you might see some of the documentation around packed and padded sequences in pytorch. You won't need to use these functions but they can provide more context for what's happening and why.\n",
    "\n",
    "\n",
    "### Add positional information to the word embeddings\n",
    "\n",
    "Right now our model doesn't know much about which order the words are in. What if we helped the model in this? One way that people have done this is to _add_ some positional embedding to the word embedding, where the positional embedding represents which position in the sequence is in. There are many complicated schemes for this, but one potential idea is to _learn the positional embeddings_. You would keep a separate `Embedding` object for positions with the number of positions up to the length of your longest sequence in the data. Then for the word in the first position, you add `position_embeddings(0)` to it. You can definitely speed that up by passing in a sequence of the positions in the current input and, conveniently, pytorch will let you easily add all the position embeddings to the word embeddings easily (no for loop required).\n",
    "\n",
    "Will it help here? I have no idea but I'm curious.\n",
    "\n",
    "\n",
    "**No extra credit is given for these; they're just for folks who want to explore more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
