{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Part 4: Attention-based classification\n",
    "\n",
    "This last part of homework 2 will have you _using_ the vectors we learned from your word2vec implementation to do classification. You should complete the initial word2vec part before before starting on this.\n",
    "\n",
    "Broadly, this last part of the homework consists of a few major steps:\n",
    "1. Load in the data, word vectors, and word-indexing\n",
    "2. Define the attention-based classification network\n",
    "3. Train your model at least one epoch (2+ is recommended though).\n",
    "4. Perform exploratory analyses on attention\n",
    "5. Test the effects of freezing the pre-trained word vectors (see homework PDF for details)\n",
    "\n",
    "After Step 2, you should be able to train your classifier implementation on a small percent of the dataset and verify that it's learning correctly. **Please note that this list is a general sketch and the homework PDF has the full list/description of to-dos and all your deliverables.**\n",
    "\n",
    "\n",
    "### Estimated performance times\n",
    "\n",
    "We designed this homework to be run on a laptop-grade CPU, so no GPU is required. If your primary computing device is a tablet or similar device, this homework can also be _developed_ on that device but then run on a more powerful machine in the Great Lakes cluster (for free). Such cases are the exception though. Following, we report on the estimated times from our reference implementation for longer-running or data-intensive pieces of the homework. Your timing may vary based on implementation design; major differences in time (e.g., 10x longer) usually point to a performance bug.\n",
    "\n",
    "* Reading data, tokenizing, and converting to ids: ~20 seconds\n",
    "* Training one epoch: ~18 minutes\n",
    "* Training one epoch using frozen embeddings: ~3 minutes\n",
    "* Evaluating on dev/test set: ~5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T04:40:07.068468Z",
     "start_time": "2024-03-04T04:40:07.064698Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Attention plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the necessary parameters from the word2vec code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T04:40:09.915268Z",
     "start_time": "2024-03-04T04:40:09.891054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the word-to-index mapping we used for word2vec and use the same type\n",
    "# of tokenizer. We'll need to use this to tokenize in the same way and keep \n",
    "# the same word-to-id mapping\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# 假设你的映射和分词器保存在pickle文件中\n",
    "word_to_index_file = './models/1st_model_word_to_index.pkl'\n",
    "index_to_word_file = './models/1st_model_index_to_word.pkl'\n",
    "\n",
    "# 加载word_to_index映射\n",
    "with open(word_to_index_file, 'rb') as f:\n",
    "    word_to_index = pickle.load(f)\n",
    "\n",
    "# 加载index_to_word映射\n",
    "with open(index_to_word_file, 'rb') as f:\n",
    "    index_to_word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Classifier Model\n",
    "\n",
    "Just like we did for word2vec, let's define a PyTorch `nn.Module` class here that will contain our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T04:40:11.145679Z",
     "start_time": "2024-03-04T04:40:11.137526Z"
    }
   },
   "outputs": [],
   "source": [
    "class DocumentAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, embeddings_fname, freeze_embeddings=True):\n",
    "        '''\n",
    "        Creates the new classifier model. embeddings_fname is a string containing the\n",
    "        filename with the saved pytorch parameters (the state dict) for the Embedding\n",
    "        object that should be used to initialize this class's word Embedding parameters\n",
    "        '''\n",
    "        super(DocumentAttentionClassifier, self).__init__()\n",
    "        \n",
    "        # Save the input arguments to the state\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        # Create the Embedding object that will hold our word embeddings that we\n",
    "        # learned in word2vec. This embedding object should have the same size\n",
    "        # as what we learned before. However, we don't to start from scratch! \n",
    "        # Once created, load the saved (word2vec-based) parameters into the object\n",
    "        # using load_state_dict.\n",
    "        \n",
    "        # Create the Embedding object\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        # Load pre-trained word embeddings\n",
    "        self.embeddings.load_state_dict(torch.load(embeddings_fname))\n",
    "        # 决定是否冻结词嵌入层\n",
    "        if freeze_embeddings:\n",
    "            for param in self.embeddings.parameters():  # 修改为正确的属性名\n",
    "                param.requires_grad = False\n",
    "\n",
    "                \n",
    "        # Define the attention heads. You have two options:\n",
    "        # \n",
    "        # 1) the worse way to implement this is to define your heads using an Embedding\n",
    "        #    and then access them individually later in forward(). This will be slower\n",
    "        #    but will probably still work \n",
    "        #\n",
    "        # 2) the ideal way is to think of your attention heads as rows in a matrix--\n",
    "        #    just like we do for word2vec. While this is kind of the same as how\n",
    "        #    we represent things like in an Embedding, the key difference is that we\n",
    "        #    can now use **matrix operations** to calculate the different r and a\n",
    "        #    vectors, which will be much faster (and less code). To do this, you'll\n",
    "        #    need to represent the attention heads as a Tensor directly (not a layer)\n",
    "        #    and make sure pytorch runs gradient descent on these parameters.\n",
    "        #\n",
    "        #  It's up to you which to use, but try option 2 first and see what you do \n",
    "        #  in the forward() function\n",
    "        \n",
    "        self.attention_heads = nn.Parameter(torch.randn(num_heads, embedding_size))\n",
    "        \n",
    "        # Define the layer that goes from the concatenated attention heads' outputs\n",
    "        # to the single output value. We'll push this output value through the sigmoid\n",
    "        # to get our prediction\n",
    "\n",
    "        self.linear = nn.Linear(num_heads * embedding_size, 1)  # Adjust the size accordingly\n",
    "    \n",
    "\n",
    "    def forward(self, word_ids):\n",
    "        \n",
    "        # Pro Tip™: when implementing this forward pass, try playing around with pytorch\n",
    "        # tensors in a jupyter notebook by making \"fake\" versions of them. For example:\n",
    "        #\n",
    "        # word_embeds = torch.Tensor([[1,6,2], [9,1,7]])\n",
    "        #\n",
    "        # If you have two word embeddings of length 3, how can you define the attention\n",
    "        # heads to get the 'r' vector? Trying things out in the simple case will let you\n",
    "        # quickly verify the sequence of operations you want to run, e.g., that you can take\n",
    "        # the softmax of the 'r' vector to get the 'a' vector and it has the right shape\n",
    "        # and values\n",
    "        \n",
    "        # Hint 1: If you're representing attention using Option 2, most of this code is just \n",
    "        #         matrix multiplications\n",
    "\n",
    "        # Hint 2: Most of your time is going to be spent figuring out shape errors and what\n",
    "        #         operations you need to do to get the right outputs. This is normal.\n",
    "        \n",
    "        # Hint 3: This is the hardest part of this last part of the homework.\n",
    "\n",
    "        \n",
    "        # Get the word embeddings for the ids\n",
    "        \n",
    "        embeds = self.embeddings(word_ids)\n",
    "\n",
    "        # Calcuate the 'r' vectors which are the dot product of each attention head\n",
    "        # with each word embedding. You should be getting a tensor that has this\n",
    "        # dot product back out---remember this vector is capturing how much the \n",
    "        # head thinks the vector is relevant for the task\n",
    "        \n",
    "        relevance_scores = torch.matmul(embeds, self.attention_heads.t())  # [batch_size, seq_length, num_heads]\n",
    "\n",
    "        # Calcuate the softmax of the 'r' vector, which call 'a'. This will give us\n",
    "        # a probability distribution over the tokens for each head. Be sure to check\n",
    "        # that the softmax is being calculated over the right axis/dimension of the \n",
    "        # data (You should see probability values that sum to 1 for each head's \n",
    "        # ratings across all the tokens)   \n",
    "        \n",
    "        attention_weights = F.softmax(relevance_scores, dim=1)  # [batch_size, seq_length, num_heads]\n",
    "\n",
    "        # Calculate the re-weighting of the word embeddings for each head's attention\n",
    "        # weight and sum the reweighted sequence for each head into a single vector.\n",
    "        # This should give you n_heads vectors that each have embedding_size length.\n",
    "        # Note again that each head should give you a different weighting of the\n",
    "        # input word embeddings\n",
    "        weighted_embeds = torch.einsum('bsn,bse->bne', attention_weights, embeds)  # [batch_size, num_heads, embedding_size]\n",
    "\n",
    "        \n",
    "        # Create a single vector that has all n_heads' attention-weighted vectors\n",
    "        # as one single vector. We need this one-long-vector shape so that we \n",
    "        # can pass all these vectors as input into a layer.\n",
    "        #\n",
    "        # NOTE: if you're doing Option 2 for representing attention, you don't \n",
    "        # actually need to create a new vector (which is very inefficient).\n",
    "        # Instead, you can create a new *view* of the same data that reshapes the\n",
    "        # different heads' vectors so it looks like one long vector. \n",
    "        \n",
    "        concatenated = weighted_embeds.view(weighted_embeds.size(0), -1)  # [batch_size, num_heads * embedding_size]\n",
    "\n",
    "\n",
    "        # Pass the side-by-side attention-weighted vectors through your linear\n",
    "        # layer to get some output activation.\n",
    "        #\n",
    "        # NOTE: if you're feeling adventurous, try adding an extra layer here\n",
    "        # which will allow you different attention-weighted vectors to interact\n",
    "        # in making the model decision\n",
    "        \n",
    "        output = self.linear(concatenated)  # [batch_size, 1]\n",
    "        \n",
    "        # Return the sigmoid of the output activation *and* the attention \n",
    "        # weights for each head. We'll need these later for visualization\n",
    "        probability = torch.sigmoid(output)\n",
    "        \n",
    "        return probability, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the datasets \n",
    "\n",
    "You can keep these as pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:01:17.232345Z",
     "start_time": "2024-03-04T05:01:16.828057Z"
    }
   },
   "outputs": [],
   "source": [
    "# 假设你的数据存储在CSV文件中\n",
    "train_data_path = 'sentiment.train.csv'\n",
    "dev_data_path = 'sentiment.dev.csv'\n",
    "test_data_path = 'sentiment.test.csv'\n",
    "\n",
    "# 使用pandas读取数据\n",
    "sent_train_df = pd.read_csv(train_data_path)\n",
    "sent_dev_df = pd.read_csv(dev_data_path)\n",
    "sent_test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each dataset into a list of tuples of the form `([word-ids,...], label)`. Both the word ids and the label should be numpy arrays so they will get converted into Tensors by our data loader. Note that you did something very similar for creating the word2vec training data. This process will require tokenizing the data in the same way as you did for word2vec and using the same word-to-id mapping (both of which you loaded/created above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:01:24.036670Z",
     "start_time": "2024-03-04T05:01:18.095273Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_ids(text, tokenizer, word_to_index):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    ids = [word_to_index.get(token, word_to_index['<UNK>']) for token in tokens]\n",
    "    return np.array(ids)\n",
    "\n",
    "def prepare_data(df, tokenizer, word_to_index):\n",
    "    data_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        word_ids = tokenize_and_convert_to_ids(row['text'], tokenizer, word_to_index)\n",
    "        label = np.array(row['label'])\n",
    "        data_list.append((word_ids, label))\n",
    "    return data_list\n",
    "\n",
    "def prepare_test_data(df, tokenizer, word_to_index):\n",
    "    test_data = []\n",
    "    for text in df['text']:\n",
    "        word_ids = tokenize_and_convert_to_ids(text, tokenizer, word_to_index)\n",
    "        test_data.append(torch.tensor(word_ids, dtype=torch.long))\n",
    "    return test_data\n",
    "\n",
    "# 使用定义的函数准备测试数据\n",
    "test_data = prepare_test_data(sent_test_df, tokenizer, word_to_index)\n",
    "\n",
    "# 现在，假设你的数据帧已经正确加载\n",
    "# 以下是如何应用上述函数的例子\n",
    "train_list = prepare_data(sent_train_df, tokenizer, word_to_index)\n",
    "dev_list = prepare_data(sent_dev_df, tokenizer, word_to_index)\n",
    "test_list = prepare_test_data(sent_test_df, tokenizer, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this worked you should see XXXX train, XXXX dev, and XXX test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:01:25.097697Z",
     "start_time": "2024-03-04T05:01:25.090248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(160000, 20000, 20000)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list), len(dev_list), len(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the code training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll evaluate periodically so before we start training, let's define a function that takes in some evaluation data (e.g., the dev or test sets) and computes the F1 score on that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:04:49.470879Z",
     "start_time": "2024-03-04T05:04:49.462935Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_eval(model, eval_data):\n",
    "    '''\n",
    "    Scores the model on the evaluation data and returns the F1\n",
    "    '''\n",
    "    model.eval()  # 切换到评估模式\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for word_ids, labels in eval_data:\n",
    "            probability, _ = model(word_ids)  # 提取概率输出，忽略注意力权重\n",
    "            outputs = probability.squeeze()  # 现在可以安全地调用.squeeze()方法\n",
    "            preds = torch.round(outputs)  # 将输出转换为0或1的预测\n",
    "            \n",
    "            # 确保preds和labels是可迭代的\n",
    "            preds_list = [preds.item()] if preds.dim() == 0 else preds.tolist()\n",
    "            labels_list = [labels.item()] if labels.dim() == 0 else labels.tolist()\n",
    "\n",
    "            all_preds.extend(preds_list)\n",
    "            all_labels.extend(labels_list)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    model.train()  # 切换回训练模式\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The training code will look surprisingly similar to your word2vec code. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. **see below:** Initializes Weights & Biases and periodically write our running-sum of the loss \n",
    "5. Train your model \n",
    "\n",
    "For step 4, in addition to writing the loss, you should write the F1 score on the dev set to the `wandb` as well, using the specified number of steps.\n",
    "\n",
    "**NOTE:** In this training, you'll use a batch size of 1, which will make your life _much_ simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:46:26.194230Z",
     "start_time": "2024-03-04T05:16:26.544367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:5jhif565) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cd428934bfb4563adbd098f3e5cc03b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">sandy-leaf-17</strong> at: <a href='https://wandb.ai/yanzhuo/attention_classifier/runs/5jhif565' target=\"_blank\">https://wandb.ai/yanzhuo/attention_classifier/runs/5jhif565</a><br/> View job at <a href='https://wandb.ai/yanzhuo/attention_classifier/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NDYwMzMxNA==/version_details/v4' target=\"_blank\">https://wandb.ai/yanzhuo/attention_classifier/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NDYwMzMxNA==/version_details/v4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240304_131324-5jhif565\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:5jhif565). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>G:\\My Drive\\SI_630\\Homeworks\\Homework2\\wandb\\run-20240304_131626-jnkrao83</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/yanzhuo/attention_classifier/runs/jnkrao83' target=\"_blank\">solar-wood-18</a></strong> to <a href='https://wandb.ai/yanzhuo/attention_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/yanzhuo/attention_classifier' target=\"_blank\">https://wandb.ai/yanzhuo/attention_classifier</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/yanzhuo/attention_classifier/runs/jnkrao83' target=\"_blank\">https://wandb.ai/yanzhuo/attention_classifier/runs/jnkrao83</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a6be8b2687b43fa96136f135f448568"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/160000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d449ba116314d24809f3849750f50d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Avg Loss: 0.7050650745630265, F1 Score: 0.3454101498053191\n",
      "Step 200, Avg Loss: 0.7000345730781555, F1 Score: 0.3556165457753382\n",
      "Step 300, Avg Loss: 0.7013837188482285, F1 Score: 0.39878892733564014\n",
      "Step 400, Avg Loss: 0.6967344796657562, F1 Score: 0.411628191034567\n",
      "Step 500, Avg Loss: 0.6961191695928574, F1 Score: 0.4122737035900583\n",
      "Step 600, Avg Loss: 0.6856914323568344, F1 Score: 0.4294323249367241\n",
      "Step 700, Avg Loss: 0.6841693487763405, F1 Score: 0.42830223086742447\n",
      "Step 800, Avg Loss: 0.6950281876325607, F1 Score: 0.4704062338852919\n",
      "Step 900, Avg Loss: 0.6995347064733505, F1 Score: 0.47438330170777987\n",
      "Step 1000, Avg Loss: 0.6704000675678253, F1 Score: 0.477587893434605\n",
      "Step 1100, Avg Loss: 0.6925497597455978, F1 Score: 0.49070716228467814\n",
      "Step 1200, Avg Loss: 0.6771543425321579, F1 Score: 0.46016959273856445\n",
      "Step 1300, Avg Loss: 0.6940661972761154, F1 Score: 0.49810079936504337\n",
      "Step 1400, Avg Loss: 0.6777113610506058, F1 Score: 0.5103432852510343\n",
      "Step 1500, Avg Loss: 0.6692822182178497, F1 Score: 0.4709403244149159\n",
      "Step 1600, Avg Loss: 0.6892200216650963, F1 Score: 0.41679199633819397\n",
      "Step 1700, Avg Loss: 0.6819550603628158, F1 Score: 0.4076126878130217\n",
      "Step 1800, Avg Loss: 0.6832691800594329, F1 Score: 0.3814884302689181\n",
      "Step 1900, Avg Loss: 0.6793816623091697, F1 Score: 0.4153794717485791\n",
      "Step 2000, Avg Loss: 0.6855886387825012, F1 Score: 0.4535637149028078\n",
      "Step 2100, Avg Loss: 0.6886596810817719, F1 Score: 0.48641419974469635\n",
      "Step 2200, Avg Loss: 0.6632455956935882, F1 Score: 0.4827331486611265\n",
      "Step 2300, Avg Loss: 0.6807202082872391, F1 Score: 0.5120538879697472\n",
      "Step 2400, Avg Loss: 0.6815590539574623, F1 Score: 0.5560092704999449\n",
      "Step 2500, Avg Loss: 0.6842624005675316, F1 Score: 0.5526698757416322\n",
      "Step 2600, Avg Loss: 0.6729248243570328, F1 Score: 0.5567991046446559\n",
      "Step 2700, Avg Loss: 0.6827444991469384, F1 Score: 0.5566766247674877\n",
      "Step 2800, Avg Loss: 0.678179898262024, F1 Score: 0.592596515384208\n",
      "Step 2900, Avg Loss: 0.6742965090274811, F1 Score: 0.5813611881834503\n",
      "Step 3000, Avg Loss: 0.6608131858706474, F1 Score: 0.5894089732528042\n",
      "Step 3100, Avg Loss: 0.6746522909402848, F1 Score: 0.5902067464635473\n",
      "Step 3200, Avg Loss: 0.670637139081955, F1 Score: 0.5762081784386617\n",
      "Step 3300, Avg Loss: 0.6620082578063011, F1 Score: 0.5647126768697661\n",
      "Step 3400, Avg Loss: 0.6721972596645355, F1 Score: 0.5580380828862819\n",
      "Step 3500, Avg Loss: 0.6644848021864891, F1 Score: 0.5863644053459651\n",
      "Step 3600, Avg Loss: 0.6649920397996902, F1 Score: 0.6279045750284002\n",
      "Step 3700, Avg Loss: 0.6692762356996537, F1 Score: 0.6318709544847426\n",
      "Step 3800, Avg Loss: 0.6624016925692559, F1 Score: 0.6293180036582179\n",
      "Step 3900, Avg Loss: 0.6645233064889908, F1 Score: 0.6312884074575565\n",
      "Step 4000, Avg Loss: 0.6584592121839523, F1 Score: 0.6010391957528521\n",
      "Step 4100, Avg Loss: 0.6801030397415161, F1 Score: 0.6134528155769443\n",
      "Step 4200, Avg Loss: 0.6774051350355148, F1 Score: 0.6378609402689461\n",
      "Step 4300, Avg Loss: 0.6606150311231613, F1 Score: 0.635320251758608\n",
      "Step 4400, Avg Loss: 0.6585455620288849, F1 Score: 0.6444850065189048\n",
      "Step 4500, Avg Loss: 0.6694438773393631, F1 Score: 0.6495602690118986\n",
      "Step 4600, Avg Loss: 0.6603445854783058, F1 Score: 0.6509463233012721\n",
      "Step 4700, Avg Loss: 0.6656011295318603, F1 Score: 0.6518227009113504\n",
      "Step 4800, Avg Loss: 0.6570416843891144, F1 Score: 0.6493193717277487\n",
      "Step 4900, Avg Loss: 0.6544439911842346, F1 Score: 0.6480747900491206\n",
      "Step 5000, Avg Loss: 0.6521845319867134, F1 Score: 0.6510375415808649\n",
      "Step 5100, Avg Loss: 0.6514688557386399, F1 Score: 0.6235676415255686\n",
      "Step 5200, Avg Loss: 0.6663832458853721, F1 Score: 0.6144065325167687\n",
      "Step 5300, Avg Loss: 0.6492485707998276, F1 Score: 0.6001077392709643\n",
      "Step 5400, Avg Loss: 0.6771268352866173, F1 Score: 0.6358802127064092\n",
      "Step 5500, Avg Loss: 0.6424387404322625, F1 Score: 0.64463264408271\n",
      "Step 5600, Avg Loss: 0.6516490045189858, F1 Score: 0.6746140913953844\n",
      "Step 5700, Avg Loss: 0.6702171930670738, F1 Score: 0.6716848834811301\n",
      "Step 5800, Avg Loss: 0.6522022551298141, F1 Score: 0.6767501277465509\n",
      "Step 5900, Avg Loss: 0.6566334122419357, F1 Score: 0.6718303269740604\n",
      "Step 6000, Avg Loss: 0.6600772279500962, F1 Score: 0.6822538925800071\n",
      "Step 6100, Avg Loss: 0.666730885207653, F1 Score: 0.6846602332127061\n",
      "Step 6200, Avg Loss: 0.6494943264126778, F1 Score: 0.6881109116297626\n",
      "Step 6300, Avg Loss: 0.6487345123291015, F1 Score: 0.6873582909255807\n",
      "Step 6400, Avg Loss: 0.6382720038294792, F1 Score: 0.6971759594496741\n",
      "Step 6500, Avg Loss: 0.6658016300201416, F1 Score: 0.6963424730920956\n",
      "Step 6600, Avg Loss: 0.6605049076676369, F1 Score: 0.692588700395336\n",
      "Step 6700, Avg Loss: 0.6357434371113777, F1 Score: 0.6987550240172532\n",
      "Step 6800, Avg Loss: 0.6531588879227638, F1 Score: 0.7002675747993189\n",
      "Step 6900, Avg Loss: 0.6389627501368522, F1 Score: 0.7013926215489861\n",
      "Step 7000, Avg Loss: 0.6293104445934296, F1 Score: 0.6981752916542028\n",
      "Step 7100, Avg Loss: 0.6461324843764306, F1 Score: 0.6898796301079713\n",
      "Step 7200, Avg Loss: 0.6493669655919075, F1 Score: 0.696885568623492\n",
      "Step 7300, Avg Loss: 0.6515720909833909, F1 Score: 0.706002331002331\n",
      "Step 7400, Avg Loss: 0.6437640774250031, F1 Score: 0.7112589073634205\n",
      "Step 7500, Avg Loss: 0.6452238038182259, F1 Score: 0.7127043195628622\n",
      "Step 7600, Avg Loss: 0.6487022164463997, F1 Score: 0.7124761904761905\n",
      "Step 7700, Avg Loss: 0.6233704948425293, F1 Score: 0.7156585641605426\n",
      "Step 7800, Avg Loss: 0.6470852434635163, F1 Score: 0.7106930500048738\n",
      "Step 7900, Avg Loss: 0.6509463143348694, F1 Score: 0.7120524944514137\n",
      "Step 8000, Avg Loss: 0.6479411813616752, F1 Score: 0.7065108847613342\n",
      "Step 8100, Avg Loss: 0.6211777955293656, F1 Score: 0.7078534031413612\n",
      "Step 8200, Avg Loss: 0.6285864937305451, F1 Score: 0.7159499496620164\n",
      "Step 8300, Avg Loss: 0.6519376474618912, F1 Score: 0.7189388217522659\n",
      "Step 8400, Avg Loss: 0.6280451303720475, F1 Score: 0.7210010779397291\n",
      "Step 8500, Avg Loss: 0.6268244433403015, F1 Score: 0.7173223045109166\n",
      "Step 8600, Avg Loss: 0.6481490379571915, F1 Score: 0.7181385510312004\n",
      "Step 8700, Avg Loss: 0.6252412804961205, F1 Score: 0.7175816203143893\n",
      "Step 8800, Avg Loss: 0.6212140235304833, F1 Score: 0.7138425173585463\n",
      "Step 8900, Avg Loss: 0.6393701687455178, F1 Score: 0.7127283588033252\n",
      "Step 9000, Avg Loss: 0.6432882225513459, F1 Score: 0.7184776292852992\n",
      "Step 9100, Avg Loss: 0.6401759847998619, F1 Score: 0.7119971877667856\n",
      "Step 9200, Avg Loss: 0.6213411056995392, F1 Score: 0.7114696244013108\n",
      "Step 9300, Avg Loss: 0.6403036987781525, F1 Score: 0.7108494874657465\n",
      "Step 9400, Avg Loss: 0.6405463755130768, F1 Score: 0.7064892190240379\n",
      "Step 9500, Avg Loss: 0.6272260549664498, F1 Score: 0.7063150805574263\n",
      "Step 9600, Avg Loss: 0.6262025570869446, F1 Score: 0.7170072739868375\n",
      "Step 9700, Avg Loss: 0.622691171169281, F1 Score: 0.7181353265145555\n",
      "Step 9800, Avg Loss: 0.6301995268464089, F1 Score: 0.7121782480415099\n",
      "Step 9900, Avg Loss: 0.6239648675918579, F1 Score: 0.7064329918352489\n",
      "Step 10000, Avg Loss: 0.6322861832380294, F1 Score: 0.7079325680008274\n",
      "Step 10100, Avg Loss: 0.6027918034791946, F1 Score: 0.7129648445437919\n",
      "Step 10200, Avg Loss: 0.6292527586221695, F1 Score: 0.7133770925558439\n",
      "Step 10300, Avg Loss: 0.623879226744175, F1 Score: 0.7179\n",
      "Step 10400, Avg Loss: 0.6214846801757813, F1 Score: 0.7185819557857\n",
      "Step 10500, Avg Loss: 0.6210657376050949, F1 Score: 0.7114808308360029\n",
      "Step 10600, Avg Loss: 0.6449065256118774, F1 Score: 0.7151786167996341\n",
      "Step 10700, Avg Loss: 0.6214714094996452, F1 Score: 0.71404468531111\n",
      "Step 10800, Avg Loss: 0.6150382485985756, F1 Score: 0.7128202484664158\n",
      "Step 10900, Avg Loss: 0.6183868256211281, F1 Score: 0.7062608787383301\n",
      "Step 11000, Avg Loss: 0.6364946109056473, F1 Score: 0.6927300680869756\n",
      "Step 11100, Avg Loss: 0.6146402448415756, F1 Score: 0.6879341966320236\n",
      "Step 11200, Avg Loss: 0.6389180108904838, F1 Score: 0.6923796423658872\n",
      "Step 11300, Avg Loss: 0.617776897251606, F1 Score: 0.6945295404814005\n",
      "Step 11400, Avg Loss: 0.6462996882200241, F1 Score: 0.6929984024679117\n",
      "Step 11500, Avg Loss: 0.6445743006467819, F1 Score: 0.6945496459739832\n",
      "Step 11600, Avg Loss: 0.6579469725489616, F1 Score: 0.7066964760992228\n",
      "Step 11700, Avg Loss: 0.6283223548531532, F1 Score: 0.7082582423407188\n",
      "Step 11800, Avg Loss: 0.6094313621520996, F1 Score: 0.7162588588277896\n",
      "Step 11900, Avg Loss: 0.6018346014618874, F1 Score: 0.7163391933815926\n",
      "Step 12000, Avg Loss: 0.6254020187258721, F1 Score: 0.711282078305317\n",
      "Step 12100, Avg Loss: 0.6415825566649437, F1 Score: 0.7205634665441739\n",
      "Step 12200, Avg Loss: 0.6281889653205872, F1 Score: 0.7226159141526625\n",
      "Step 12300, Avg Loss: 0.6362927365303039, F1 Score: 0.7228424325839833\n",
      "Step 12400, Avg Loss: 0.6078606206178665, F1 Score: 0.715615521388976\n",
      "Step 12500, Avg Loss: 0.6143250435590744, F1 Score: 0.7260782347041124\n",
      "Step 12600, Avg Loss: 0.6227622818946839, F1 Score: 0.728474188900482\n",
      "Step 12700, Avg Loss: 0.6208440381288528, F1 Score: 0.7200206398348813\n",
      "Step 12800, Avg Loss: 0.6015651887655258, F1 Score: 0.7144060657118787\n",
      "Step 12900, Avg Loss: 0.6085134828090668, F1 Score: 0.7136344359626803\n",
      "Step 13000, Avg Loss: 0.5993042299151421, F1 Score: 0.719247166476032\n",
      "Step 13100, Avg Loss: 0.6209122113883495, F1 Score: 0.7238668844426297\n",
      "Step 13200, Avg Loss: 0.616859434247017, F1 Score: 0.7229410556582461\n",
      "Step 13300, Avg Loss: 0.5878246375918388, F1 Score: 0.7173341742878167\n",
      "Step 13400, Avg Loss: 0.6137035489082336, F1 Score: 0.7119277816355963\n",
      "Step 13500, Avg Loss: 0.6091446572542191, F1 Score: 0.7124252775405636\n",
      "Step 13600, Avg Loss: 0.6391041377186775, F1 Score: 0.7177491454115172\n",
      "Step 13700, Avg Loss: 0.6312531462311745, F1 Score: 0.7182128777923784\n",
      "Step 13800, Avg Loss: 0.5952240294218063, F1 Score: 0.7156050955414013\n",
      "Step 13900, Avg Loss: 0.5941839838027954, F1 Score: 0.7168814705260934\n",
      "Step 14000, Avg Loss: 0.6089041623473167, F1 Score: 0.7128533333333333\n",
      "Step 14100, Avg Loss: 0.626172485947609, F1 Score: 0.7099935386603489\n",
      "Step 14200, Avg Loss: 0.5915629506111145, F1 Score: 0.7136845469557342\n",
      "Step 14300, Avg Loss: 0.6627462854981423, F1 Score: 0.7173236240831619\n",
      "Step 14400, Avg Loss: 0.6053409580886364, F1 Score: 0.7186908019364344\n",
      "Step 14500, Avg Loss: 0.5912248748540878, F1 Score: 0.7096426453942759\n",
      "Step 14600, Avg Loss: 0.6090992891788483, F1 Score: 0.7094860594294343\n",
      "Step 14700, Avg Loss: 0.6110853457450867, F1 Score: 0.7197606047879043\n",
      "Step 14800, Avg Loss: 0.6201876658201217, F1 Score: 0.7225981055480379\n",
      "Step 14900, Avg Loss: 0.617716489136219, F1 Score: 0.7219618869103405\n",
      "Step 15000, Avg Loss: 0.6114673528075218, F1 Score: 0.7271334456871457\n",
      "Step 15100, Avg Loss: 0.6093375930190086, F1 Score: 0.7305425419374338\n",
      "Step 15200, Avg Loss: 0.6104184272885322, F1 Score: 0.7327939266806512\n",
      "Step 15300, Avg Loss: 0.6304830256104469, F1 Score: 0.7342149746445262\n",
      "Step 15400, Avg Loss: 0.6459400469064712, F1 Score: 0.731695060053269\n",
      "Step 15500, Avg Loss: 0.5779495650529861, F1 Score: 0.7326444622792937\n",
      "Step 15600, Avg Loss: 0.6728826585412025, F1 Score: 0.7365287287883299\n",
      "Step 15700, Avg Loss: 0.5973936958611011, F1 Score: 0.7343843544202754\n",
      "Step 15800, Avg Loss: 0.6138746550679207, F1 Score: 0.7375872481560318\n",
      "Step 15900, Avg Loss: 0.611588158607483, F1 Score: 0.7390236715450349\n",
      "Step 16000, Avg Loss: 0.5817401620745659, F1 Score: 0.7380248373743347\n",
      "Step 16100, Avg Loss: 0.5963198336958885, F1 Score: 0.7392862402434833\n",
      "Step 16200, Avg Loss: 0.5838435687124729, F1 Score: 0.7400569576745556\n",
      "Step 16300, Avg Loss: 0.5896538645029068, F1 Score: 0.7389400127946459\n",
      "Step 16400, Avg Loss: 0.6072885051369667, F1 Score: 0.7361871577899453\n",
      "Step 16500, Avg Loss: 0.5771951270103455, F1 Score: 0.7319321347176501\n",
      "Step 16600, Avg Loss: 0.6085527217388154, F1 Score: 0.7276845464840935\n",
      "Step 16700, Avg Loss: 0.5779446032643318, F1 Score: 0.7269626337278412\n",
      "Step 16800, Avg Loss: 0.5807971589267253, F1 Score: 0.7277680682635962\n",
      "Step 16900, Avg Loss: 0.606804390847683, F1 Score: 0.7235896090265023\n",
      "Step 17000, Avg Loss: 0.6025347489118577, F1 Score: 0.7278526597386562\n",
      "Step 17100, Avg Loss: 0.5992481055855751, F1 Score: 0.7268014515292898\n",
      "Step 17200, Avg Loss: 0.6017206671833992, F1 Score: 0.7205579331114281\n",
      "Step 17300, Avg Loss: 0.5998211182653904, F1 Score: 0.7205120067703374\n",
      "Step 17400, Avg Loss: 0.5901442834734917, F1 Score: 0.7244102725037921\n",
      "Step 17500, Avg Loss: 0.5857565575838088, F1 Score: 0.7249086161879895\n",
      "Step 17600, Avg Loss: 0.5893377560377121, F1 Score: 0.7246013071895425\n",
      "Step 17700, Avg Loss: 0.5816094195842743, F1 Score: 0.7296229802513465\n",
      "Step 17800, Avg Loss: 0.5700035914778709, F1 Score: 0.7332928311057109\n",
      "Step 17900, Avg Loss: 0.5829254442453384, F1 Score: 0.7329003450375482\n",
      "Step 18000, Avg Loss: 0.5704605820775032, F1 Score: 0.7295726847328636\n",
      "Step 18100, Avg Loss: 0.6033661198616028, F1 Score: 0.7281738861066653\n",
      "Step 18200, Avg Loss: 0.6268639343976975, F1 Score: 0.7307633236062049\n",
      "Step 18300, Avg Loss: 0.5981010708212853, F1 Score: 0.7361723446893788\n",
      "Step 18400, Avg Loss: 0.5865505476295948, F1 Score: 0.7340990740272226\n",
      "Step 18500, Avg Loss: 0.5873299214243889, F1 Score: 0.7327296774849934\n",
      "Step 18600, Avg Loss: 0.6253755113482475, F1 Score: 0.7315778730353133\n",
      "Step 18700, Avg Loss: 0.5824793544411659, F1 Score: 0.7280273944173498\n",
      "Step 18800, Avg Loss: 0.5980605441331863, F1 Score: 0.7355939045312342\n",
      "Step 18900, Avg Loss: 0.6062940949201584, F1 Score: 0.7326773058685566\n",
      "Step 19000, Avg Loss: 0.5739726570248603, F1 Score: 0.7304472859857238\n",
      "Step 19100, Avg Loss: 0.6058901339769364, F1 Score: 0.7259197149145792\n",
      "Step 19200, Avg Loss: 0.6053128366172313, F1 Score: 0.7263978241539829\n",
      "Step 19300, Avg Loss: 0.5985015101730824, F1 Score: 0.7306086687625625\n",
      "Step 19400, Avg Loss: 0.6057477487623691, F1 Score: 0.7275285289979678\n",
      "Step 19500, Avg Loss: 0.6265071541070938, F1 Score: 0.7254551194359676\n",
      "Step 19600, Avg Loss: 0.5655834211409092, F1 Score: 0.7236133122028526\n",
      "Step 19700, Avg Loss: 0.6172548246383667, F1 Score: 0.7307612956467918\n",
      "Step 19800, Avg Loss: 0.573477054387331, F1 Score: 0.7336565450369993\n",
      "Step 19900, Avg Loss: 0.5898725028336048, F1 Score: 0.7308108108108108\n",
      "Step 20000, Avg Loss: 0.5950185759365558, F1 Score: 0.7299020370082413\n",
      "Step 20100, Avg Loss: 0.5813713592290878, F1 Score: 0.7302638385928608\n",
      "Step 20200, Avg Loss: 0.585210300385952, F1 Score: 0.7307334229854143\n",
      "Step 20300, Avg Loss: 0.5852944763004779, F1 Score: 0.7336745138178096\n",
      "Step 20400, Avg Loss: 0.6159336656332016, F1 Score: 0.7339843550283757\n",
      "Step 20500, Avg Loss: 0.6126926063001156, F1 Score: 0.7345335849441526\n",
      "Step 20600, Avg Loss: 0.5603234508633613, F1 Score: 0.7339646787816739\n",
      "Step 20700, Avg Loss: 0.5859478591382503, F1 Score: 0.7396944653143\n",
      "Step 20800, Avg Loss: 0.5915712651610374, F1 Score: 0.7386243386243386\n",
      "Step 20900, Avg Loss: 0.5700994428992271, F1 Score: 0.7398097145718577\n",
      "Step 21000, Avg Loss: 0.5741055762767792, F1 Score: 0.7352460690218501\n",
      "Step 21100, Avg Loss: 0.5762773330509663, F1 Score: 0.7316544535039736\n",
      "Step 21200, Avg Loss: 0.5938945093750954, F1 Score: 0.7311190810333177\n",
      "Step 21300, Avg Loss: 0.601916133761406, F1 Score: 0.734166837466694\n",
      "Step 21400, Avg Loss: 0.6152142772078514, F1 Score: 0.7371472158657514\n",
      "Step 21500, Avg Loss: 0.5833382443338633, F1 Score: 0.7375266741184839\n",
      "Step 21600, Avg Loss: 0.5867305113375187, F1 Score: 0.7367777438092327\n",
      "Step 21700, Avg Loss: 0.5500705976784229, F1 Score: 0.735300143413235\n",
      "Step 21800, Avg Loss: 0.5737499038875103, F1 Score: 0.7319534012897857\n",
      "Step 21900, Avg Loss: 0.5808806605637074, F1 Score: 0.7274927650618258\n",
      "Step 22000, Avg Loss: 0.5612033519148827, F1 Score: 0.7275217276797472\n",
      "Step 22100, Avg Loss: 0.561088899821043, F1 Score: 0.7222132861508767\n",
      "Step 22200, Avg Loss: 0.5743035988509655, F1 Score: 0.7237698793894759\n",
      "Step 22300, Avg Loss: 0.5750312559306622, F1 Score: 0.7314877806269298\n",
      "Step 22400, Avg Loss: 0.5873103541135788, F1 Score: 0.7367508288701862\n",
      "Step 22500, Avg Loss: 0.5595261852443218, F1 Score: 0.7432877532687893\n",
      "Step 22600, Avg Loss: 0.5744695781171322, F1 Score: 0.7454112511161821\n",
      "Step 22700, Avg Loss: 0.6040962824225425, F1 Score: 0.7507591459006121\n",
      "Step 22800, Avg Loss: 0.5777044074237346, F1 Score: 0.7492356224217422\n",
      "Step 22900, Avg Loss: 0.5371448880434037, F1 Score: 0.7493332040153242\n",
      "Step 23000, Avg Loss: 0.574916200786829, F1 Score: 0.7494892499270357\n",
      "Step 23100, Avg Loss: 0.5379675135016442, F1 Score: 0.7489494771816672\n",
      "Step 23200, Avg Loss: 0.6090597745776176, F1 Score: 0.7456538917423943\n",
      "Step 23300, Avg Loss: 0.5585263387858868, F1 Score: 0.7464483030781374\n",
      "Step 23400, Avg Loss: 0.5827754490077496, F1 Score: 0.7457911626758825\n",
      "Step 23500, Avg Loss: 0.591436511427164, F1 Score: 0.7492181391712275\n",
      "Step 23600, Avg Loss: 0.5674608798325061, F1 Score: 0.7496335385517443\n",
      "Step 23700, Avg Loss: 0.578612198382616, F1 Score: 0.7489736070381232\n",
      "Step 23800, Avg Loss: 0.542355609536171, F1 Score: 0.7498788642310301\n",
      "Step 23900, Avg Loss: 0.5799524779617786, F1 Score: 0.7536011409555503\n",
      "Step 24000, Avg Loss: 0.526545410901308, F1 Score: 0.7516258607498087\n",
      "Step 24100, Avg Loss: 0.6076283042132854, F1 Score: 0.7488718854227977\n",
      "Step 24200, Avg Loss: 0.5863877457380294, F1 Score: 0.7458486626230486\n",
      "Step 24300, Avg Loss: 0.5712876260280609, F1 Score: 0.7457761876366528\n",
      "Step 24400, Avg Loss: 0.5762911388278007, F1 Score: 0.7418784418733896\n",
      "Step 24500, Avg Loss: 0.5922459197044373, F1 Score: 0.7482014388489209\n",
      "Step 24600, Avg Loss: 0.5987914662063122, F1 Score: 0.7463012610465694\n",
      "Step 24700, Avg Loss: 0.5650972715020179, F1 Score: 0.7430384208671131\n",
      "Step 24800, Avg Loss: 0.5972581495344639, F1 Score: 0.7486198738170347\n",
      "Step 24900, Avg Loss: 0.565060615837574, F1 Score: 0.748868333005314\n",
      "Step 25000, Avg Loss: 0.5517864316701889, F1 Score: 0.7501466849207902\n",
      "Step 25100, Avg Loss: 0.585130227804184, F1 Score: 0.7506225889936032\n",
      "Step 25200, Avg Loss: 0.5689406672120094, F1 Score: 0.7456866460556497\n",
      "Step 25300, Avg Loss: 0.6199643832445144, F1 Score: 0.7469879518072289\n",
      "Step 25400, Avg Loss: 0.5360486941039562, F1 Score: 0.7383450181669311\n",
      "Step 25500, Avg Loss: 0.593589526116848, F1 Score: 0.7399491094147582\n",
      "Step 25600, Avg Loss: 0.563551956564188, F1 Score: 0.7409962463224105\n",
      "Step 25700, Avg Loss: 0.5482813435792923, F1 Score: 0.7407859285242234\n",
      "Step 25800, Avg Loss: 0.5918104536831379, F1 Score: 0.7395918367346939\n",
      "Step 25900, Avg Loss: 0.5077315239608288, F1 Score: 0.7387081856019732\n",
      "Step 26000, Avg Loss: 0.5985745577514172, F1 Score: 0.7403517716033647\n",
      "Step 26100, Avg Loss: 0.5921470880508423, F1 Score: 0.744952285283777\n",
      "Step 26200, Avg Loss: 0.5594687721133232, F1 Score: 0.7476831091180867\n",
      "Step 26300, Avg Loss: 0.562330125272274, F1 Score: 0.7497526222046309\n",
      "Step 26400, Avg Loss: 0.5462107463181018, F1 Score: 0.7512983831455169\n",
      "Step 26500, Avg Loss: 0.5421524925529957, F1 Score: 0.7513904611901363\n",
      "Step 26600, Avg Loss: 0.5722903193533421, F1 Score: 0.7510714813537612\n",
      "Step 26700, Avg Loss: 0.6058063891530037, F1 Score: 0.7510469527516381\n",
      "Step 26800, Avg Loss: 0.5470487584173679, F1 Score: 0.7521066039584559\n",
      "Step 26900, Avg Loss: 0.5722004355490208, F1 Score: 0.7529503181001408\n",
      "Step 27000, Avg Loss: 0.5598964422941208, F1 Score: 0.7524810559765338\n",
      "Step 27100, Avg Loss: 0.5755016680061817, F1 Score: 0.751828759389268\n",
      "Step 27200, Avg Loss: 0.5448655280470848, F1 Score: 0.7486790948061011\n",
      "Step 27300, Avg Loss: 0.5800555641949177, F1 Score: 0.7479260369815093\n",
      "Step 27400, Avg Loss: 0.5917762976139784, F1 Score: 0.7484889355112643\n",
      "Step 27500, Avg Loss: 0.5772557820379735, F1 Score: 0.7447982584923809\n",
      "Step 27600, Avg Loss: 0.5422906386107207, F1 Score: 0.7485871467866967\n",
      "Step 27700, Avg Loss: 0.5191569703817368, F1 Score: 0.7489761262611128\n",
      "Step 27800, Avg Loss: 0.6026422016322612, F1 Score: 0.7489769438067672\n",
      "Step 27900, Avg Loss: 0.6187103572487831, F1 Score: 0.7494007191370355\n",
      "Step 28000, Avg Loss: 0.5511884267628193, F1 Score: 0.7500745897563401\n",
      "Step 28100, Avg Loss: 0.5398115342855454, F1 Score: 0.751258016773557\n",
      "Step 28200, Avg Loss: 0.5962425936758519, F1 Score: 0.7515910997089151\n",
      "Step 28300, Avg Loss: 0.5755365279316902, F1 Score: 0.751803181503804\n",
      "Step 28400, Avg Loss: 0.5802700270712375, F1 Score: 0.7467659938591634\n",
      "Step 28500, Avg Loss: 0.5740906800329685, F1 Score: 0.7456135915457349\n",
      "Step 28600, Avg Loss: 0.5455457639694213, F1 Score: 0.7418957734919983\n",
      "Step 28700, Avg Loss: 0.5623799987137318, F1 Score: 0.7457764289327263\n",
      "Step 28800, Avg Loss: 0.5590934891998768, F1 Score: 0.7506852045647082\n",
      "Step 28900, Avg Loss: 0.5426165424287319, F1 Score: 0.7508213041314087\n",
      "Step 29000, Avg Loss: 0.54254380017519, F1 Score: 0.74798873692679\n",
      "Step 29100, Avg Loss: 0.5775954572856427, F1 Score: 0.7519429731201426\n",
      "Step 29200, Avg Loss: 0.5759554441273212, F1 Score: 0.7492347066793797\n",
      "Step 29300, Avg Loss: 0.5590889711678028, F1 Score: 0.7508614232209738\n",
      "Step 29400, Avg Loss: 0.5584754261374474, F1 Score: 0.7561864942389032\n",
      "Step 29500, Avg Loss: 0.5600427103042602, F1 Score: 0.7517045737321455\n",
      "Step 29600, Avg Loss: 0.5267080767452716, F1 Score: 0.7514462397765809\n",
      "Step 29700, Avg Loss: 0.5422335511445999, F1 Score: 0.7530123469033569\n",
      "Step 29800, Avg Loss: 0.5571957428753376, F1 Score: 0.7498743339700412\n",
      "Step 29900, Avg Loss: 0.57916271135211, F1 Score: 0.7448360279492018\n",
      "Step 30000, Avg Loss: 0.5762680667638779, F1 Score: 0.7500627226654624\n",
      "Step 30100, Avg Loss: 0.520080159381032, F1 Score: 0.7495088903440286\n",
      "Step 30200, Avg Loss: 0.5574892294406891, F1 Score: 0.7486744432661718\n",
      "Step 30300, Avg Loss: 0.5357551743090153, F1 Score: 0.7506643268989722\n",
      "Step 30400, Avg Loss: 0.6157930605113506, F1 Score: 0.7497103712285297\n",
      "Step 30500, Avg Loss: 0.5576160012185574, F1 Score: 0.7526838966202783\n",
      "Step 30600, Avg Loss: 0.5383867248892784, F1 Score: 0.7517213850913083\n",
      "Step 30700, Avg Loss: 0.5504346624016762, F1 Score: 0.7546965139786007\n",
      "Step 30800, Avg Loss: 0.6009712341427803, F1 Score: 0.7515490705576654\n",
      "Step 30900, Avg Loss: 0.594745362251997, F1 Score: 0.751797124600639\n",
      "Step 31000, Avg Loss: 0.52877202257514, F1 Score: 0.7524051642490405\n",
      "Step 31100, Avg Loss: 0.5586946368217468, F1 Score: 0.7495451788963008\n",
      "Step 31200, Avg Loss: 0.5408613353967666, F1 Score: 0.7534273792966422\n",
      "Step 31300, Avg Loss: 0.6175340989232063, F1 Score: 0.754754260311188\n",
      "Step 31400, Avg Loss: 0.5523221378028392, F1 Score: 0.7535253227408143\n",
      "Step 31500, Avg Loss: 0.5636299033463001, F1 Score: 0.7507039420756235\n",
      "Step 31600, Avg Loss: 0.5124500201642513, F1 Score: 0.7509025270758123\n",
      "Step 31700, Avg Loss: 0.553232846558094, F1 Score: 0.7529610829103215\n",
      "Step 31800, Avg Loss: 0.502989461272955, F1 Score: 0.7534512833291802\n",
      "Step 31900, Avg Loss: 0.5649863204360008, F1 Score: 0.7509949120951086\n",
      "Step 32000, Avg Loss: 0.547017038166523, F1 Score: 0.7441597200782134\n",
      "Step 32100, Avg Loss: 0.5380945146083832, F1 Score: 0.7440565210664741\n",
      "Step 32200, Avg Loss: 0.5640321846306324, F1 Score: 0.7458999589995899\n",
      "Step 32300, Avg Loss: 0.59252232670784, F1 Score: 0.7487534344153862\n",
      "Step 32400, Avg Loss: 0.5344909739494323, F1 Score: 0.7489058524173028\n",
      "Step 32500, Avg Loss: 0.5094518031179905, F1 Score: 0.7488305877567623\n",
      "Step 32600, Avg Loss: 0.5020975171029568, F1 Score: 0.7437700341226346\n",
      "Step 32700, Avg Loss: 0.5138123047351837, F1 Score: 0.7467771639042358\n",
      "Step 32800, Avg Loss: 0.5491576911509037, F1 Score: 0.7455937516057757\n",
      "Step 32900, Avg Loss: 0.48022970780730245, F1 Score: 0.745726166640998\n",
      "Step 33000, Avg Loss: 0.5335047160834074, F1 Score: 0.7504301184090679\n",
      "Step 33100, Avg Loss: 0.578801476508379, F1 Score: 0.7530096308186196\n",
      "Step 33200, Avg Loss: 0.5909495843946934, F1 Score: 0.7508221603845181\n",
      "Step 33300, Avg Loss: 0.5574489124119282, F1 Score: 0.7517408416590978\n",
      "Step 33400, Avg Loss: 0.5141151948273182, F1 Score: 0.7470323372902169\n",
      "Step 33500, Avg Loss: 0.5111664472520352, F1 Score: 0.7446533064056755\n",
      "Step 33600, Avg Loss: 0.5567533655464649, F1 Score: 0.7444317828654304\n",
      "Step 33700, Avg Loss: 0.5378381870687008, F1 Score: 0.7474633596392334\n",
      "Step 33800, Avg Loss: 0.485729588419199, F1 Score: 0.7493880048959608\n",
      "Step 33900, Avg Loss: 0.5235748025029898, F1 Score: 0.7535228925329722\n",
      "Step 34000, Avg Loss: 0.5358372323215008, F1 Score: 0.7555952558185698\n",
      "Step 34100, Avg Loss: 0.5844746577739716, F1 Score: 0.7513800962268928\n",
      "Step 34200, Avg Loss: 0.5892447946965694, F1 Score: 0.7540407589599438\n",
      "Step 34300, Avg Loss: 0.5297157102078199, F1 Score: 0.7521815889029004\n",
      "Step 34400, Avg Loss: 0.5200690661370754, F1 Score: 0.7517084282460137\n",
      "Step 34500, Avg Loss: 0.5281163024157286, F1 Score: 0.747845711940911\n",
      "Step 34600, Avg Loss: 0.5361202217638492, F1 Score: 0.7459168906346909\n",
      "Step 34700, Avg Loss: 0.5234132799506187, F1 Score: 0.7435294117647059\n",
      "Step 34800, Avg Loss: 0.5337046897411346, F1 Score: 0.7419997901584304\n",
      "Step 34900, Avg Loss: 0.5354910708963871, F1 Score: 0.7452451670053671\n",
      "Step 35000, Avg Loss: 0.5574017407000065, F1 Score: 0.7451103113753716\n",
      "Step 35100, Avg Loss: 0.5378892075270414, F1 Score: 0.7462207496376061\n",
      "Step 35200, Avg Loss: 0.5806363231688738, F1 Score: 0.7509664292980671\n",
      "Step 35300, Avg Loss: 0.5153772734105587, F1 Score: 0.7525502474497525\n",
      "Step 35400, Avg Loss: 0.5560028905421496, F1 Score: 0.7554622268886556\n",
      "Step 35500, Avg Loss: 0.5488504485040903, F1 Score: 0.7552552552552553\n",
      "Step 35600, Avg Loss: 0.5259755645692349, F1 Score: 0.7553884711779448\n",
      "Step 35700, Avg Loss: 0.513391248062253, F1 Score: 0.7549728752260397\n",
      "Step 35800, Avg Loss: 0.5841608612239361, F1 Score: 0.7555867321374887\n",
      "Step 35900, Avg Loss: 0.5256398136168718, F1 Score: 0.7504455420337084\n",
      "Step 36000, Avg Loss: 0.5748923704028129, F1 Score: 0.7549922036114883\n",
      "Step 36100, Avg Loss: 0.5191521966457366, F1 Score: 0.7566811529047405\n",
      "Step 36200, Avg Loss: 0.5480929632484913, F1 Score: 0.7565569266123795\n",
      "Step 36300, Avg Loss: 0.5607231438159943, F1 Score: 0.7565535562127211\n",
      "Step 36400, Avg Loss: 0.5233156036585569, F1 Score: 0.7527970434870652\n",
      "Step 36500, Avg Loss: 0.5434284716099501, F1 Score: 0.7529864345009111\n",
      "Step 36600, Avg Loss: 0.5407082913815975, F1 Score: 0.7486889460154241\n",
      "Step 36700, Avg Loss: 0.5192757725715638, F1 Score: 0.7499487809875025\n",
      "Step 36800, Avg Loss: 0.5260490813851356, F1 Score: 0.7516676001833087\n",
      "Step 36900, Avg Loss: 0.5302404946088791, F1 Score: 0.7518712765415755\n",
      "Step 37000, Avg Loss: 0.5801012027263641, F1 Score: 0.7527023598071555\n",
      "Step 37100, Avg Loss: 0.5336831030249596, F1 Score: 0.7547817310118597\n",
      "Step 37200, Avg Loss: 0.5034110163152218, F1 Score: 0.756811098823766\n",
      "Step 37300, Avg Loss: 0.5637558943033218, F1 Score: 0.760246104991565\n",
      "Step 37400, Avg Loss: 0.5166947000846267, F1 Score: 0.7611888630631521\n",
      "Step 37500, Avg Loss: 0.5617178378999234, F1 Score: 0.7608566329565735\n",
      "Step 37600, Avg Loss: 0.5535302715003491, F1 Score: 0.7610610709690191\n",
      "Step 37700, Avg Loss: 0.4935218143463135, F1 Score: 0.7594168283823456\n",
      "Step 37800, Avg Loss: 0.5427236995100975, F1 Score: 0.756735022114998\n",
      "Step 37900, Avg Loss: 0.5421929800510407, F1 Score: 0.7568002413394339\n",
      "Step 38000, Avg Loss: 0.5440158703923226, F1 Score: 0.7571636473126914\n",
      "Step 38100, Avg Loss: 0.5583796297758817, F1 Score: 0.7597092212706632\n",
      "Step 38200, Avg Loss: 0.473615215010941, F1 Score: 0.7601332869150047\n",
      "Step 38300, Avg Loss: 0.5638980001956224, F1 Score: 0.7554185823270854\n",
      "Step 38400, Avg Loss: 0.5323422057181597, F1 Score: 0.7523731754618761\n",
      "Step 38500, Avg Loss: 0.5268531772494316, F1 Score: 0.7570445527148526\n",
      "Step 38600, Avg Loss: 0.5755907329171897, F1 Score: 0.7586310417292105\n",
      "Step 38700, Avg Loss: 0.5714032796025276, F1 Score: 0.7621212870063757\n",
      "Step 38800, Avg Loss: 0.5292331062257289, F1 Score: 0.7621779338311656\n",
      "Step 38900, Avg Loss: 0.5084942071139813, F1 Score: 0.7620979713013359\n",
      "Step 39000, Avg Loss: 0.5327634093165398, F1 Score: 0.761456060305495\n",
      "Step 39100, Avg Loss: 0.5079298431426287, F1 Score: 0.7580362068100898\n",
      "Step 39200, Avg Loss: 0.5453277429938317, F1 Score: 0.7590963614554178\n",
      "Step 39300, Avg Loss: 0.5415851749479771, F1 Score: 0.7568166927070208\n",
      "Step 39400, Avg Loss: 0.5353812326490879, F1 Score: 0.758831181827279\n",
      "Step 39500, Avg Loss: 0.5412588208913803, F1 Score: 0.7582566732016287\n",
      "Step 39600, Avg Loss: 0.5242029710114002, F1 Score: 0.754121790618141\n",
      "Step 39700, Avg Loss: 0.5161737699061633, F1 Score: 0.7526550715714946\n",
      "Step 39800, Avg Loss: 0.5782482359558344, F1 Score: 0.7536794766966476\n",
      "Step 39900, Avg Loss: 0.5270760814845562, F1 Score: 0.7532587026529673\n",
      "Step 40000, Avg Loss: 0.5402297928184271, F1 Score: 0.7580409724669049\n",
      "Step 40100, Avg Loss: 0.5993009549379349, F1 Score: 0.7544745296007342\n",
      "Step 40200, Avg Loss: 0.5024192930012942, F1 Score: 0.750335293510781\n",
      "Step 40300, Avg Loss: 0.5722016610205174, F1 Score: 0.745876315651673\n",
      "Step 40400, Avg Loss: 0.4852064096927643, F1 Score: 0.7459753292912398\n",
      "Step 40500, Avg Loss: 0.5122004889696836, F1 Score: 0.7476091476091477\n",
      "Step 40600, Avg Loss: 0.546435996517539, F1 Score: 0.7460815047021944\n",
      "Step 40700, Avg Loss: 0.5654767252504825, F1 Score: 0.7485471149854711\n",
      "Step 40800, Avg Loss: 0.5376086724549531, F1 Score: 0.7511065362840967\n",
      "Step 40900, Avg Loss: 0.5394459456205368, F1 Score: 0.7488333506170279\n",
      "Step 41000, Avg Loss: 0.5310109307616949, F1 Score: 0.7475273295158772\n",
      "Step 41100, Avg Loss: 0.5063556826114655, F1 Score: 0.750348531006351\n",
      "Step 41200, Avg Loss: 0.5206566826999187, F1 Score: 0.7503226139472462\n",
      "Step 41300, Avg Loss: 0.49314734928309917, F1 Score: 0.7507349528082934\n",
      "Step 41400, Avg Loss: 0.5236602085828781, F1 Score: 0.7511978978824256\n",
      "Step 41500, Avg Loss: 0.5519658114016056, F1 Score: 0.7505169561621174\n",
      "Step 41600, Avg Loss: 0.5252830791473388, F1 Score: 0.7514427040395714\n",
      "Step 41700, Avg Loss: 0.5407535474002362, F1 Score: 0.7531003382187148\n",
      "Step 41800, Avg Loss: 0.5671122948825359, F1 Score: 0.7482979055142664\n",
      "Step 41900, Avg Loss: 0.5189167478680611, F1 Score: 0.7488712439669937\n",
      "Step 42000, Avg Loss: 0.5429397459328175, F1 Score: 0.7512910555670316\n",
      "Step 42100, Avg Loss: 0.5186635284870863, F1 Score: 0.752174584384168\n",
      "Step 42200, Avg Loss: 0.5246186146885157, F1 Score: 0.7537791442480144\n",
      "Step 42300, Avg Loss: 0.48322154983878135, F1 Score: 0.7535413672757134\n",
      "Step 42400, Avg Loss: 0.48918309561908246, F1 Score: 0.7508786437874716\n",
      "Step 42500, Avg Loss: 0.5113526675850153, F1 Score: 0.7504915657663251\n",
      "Step 42600, Avg Loss: 0.49806553304195406, F1 Score: 0.75006475006475\n",
      "Step 42700, Avg Loss: 0.5395471777021885, F1 Score: 0.7492972410203019\n",
      "Step 42800, Avg Loss: 0.522794697880745, F1 Score: 0.7476302697041111\n",
      "Step 42900, Avg Loss: 0.5029115300625563, F1 Score: 0.7468892739014018\n",
      "Step 43000, Avg Loss: 0.47584384702146054, F1 Score: 0.7481961727491373\n",
      "Step 43100, Avg Loss: 0.5329093983024359, F1 Score: 0.7471789219545478\n",
      "Step 43200, Avg Loss: 0.5526365199312567, F1 Score: 0.7502333782802614\n",
      "Step 43300, Avg Loss: 0.5547173775732517, F1 Score: 0.7458843393837062\n",
      "Step 43400, Avg Loss: 0.4794903142750263, F1 Score: 0.7448159119763013\n",
      "Step 43500, Avg Loss: 0.5308159580081702, F1 Score: 0.7475880872483222\n",
      "Step 43600, Avg Loss: 0.5135094812512397, F1 Score: 0.7461757569363857\n",
      "Step 43700, Avg Loss: 0.526307252831757, F1 Score: 0.7461189143520963\n",
      "Step 43800, Avg Loss: 0.5674024584889412, F1 Score: 0.747730254526371\n",
      "Step 43900, Avg Loss: 0.5506906246393919, F1 Score: 0.7510731833462633\n",
      "Step 44000, Avg Loss: 0.5425932279974223, F1 Score: 0.7503112033195021\n",
      "Step 44100, Avg Loss: 0.5200432112812996, F1 Score: 0.7493861344757328\n",
      "Step 44200, Avg Loss: 0.5032228443026543, F1 Score: 0.7494118262142521\n",
      "Step 44300, Avg Loss: 0.5353416806459427, F1 Score: 0.7501566088953853\n",
      "Step 44400, Avg Loss: 0.5157523500919342, F1 Score: 0.7522964186190525\n",
      "Step 44500, Avg Loss: 0.4673123267292976, F1 Score: 0.7550644567219152\n",
      "Step 44600, Avg Loss: 0.5147679308801889, F1 Score: 0.7561398145317436\n",
      "Step 44700, Avg Loss: 0.5579266438633204, F1 Score: 0.758879262299235\n",
      "Step 44800, Avg Loss: 0.476109551563859, F1 Score: 0.7600947723950194\n",
      "Step 44900, Avg Loss: 0.47015545025467875, F1 Score: 0.759307972480777\n",
      "Step 45000, Avg Loss: 0.48720732241868975, F1 Score: 0.7593436645396536\n",
      "Step 45100, Avg Loss: 0.5178213687986135, F1 Score: 0.7600605143721634\n",
      "Step 45200, Avg Loss: 0.5424873492121697, F1 Score: 0.7581798812966063\n",
      "Step 45300, Avg Loss: 0.5108723469078541, F1 Score: 0.75465104327269\n",
      "Step 45400, Avg Loss: 0.42838712349534036, F1 Score: 0.751506963209312\n",
      "Step 45500, Avg Loss: 0.4974444592744112, F1 Score: 0.7501698280817265\n",
      "Step 45600, Avg Loss: 0.5544453752040863, F1 Score: 0.7477382705659583\n",
      "Step 45700, Avg Loss: 0.5262837746739387, F1 Score: 0.7506906437320824\n",
      "Step 45800, Avg Loss: 0.5251618777215481, F1 Score: 0.7515731447293151\n",
      "Step 45900, Avg Loss: 0.4819181147962809, F1 Score: 0.7528875537369866\n",
      "Step 46000, Avg Loss: 0.5097773826122284, F1 Score: 0.7520141379489579\n",
      "Step 46100, Avg Loss: 0.562339401319623, F1 Score: 0.7530934506859953\n",
      "Step 46200, Avg Loss: 0.542097584605217, F1 Score: 0.7537371333988517\n",
      "Step 46300, Avg Loss: 0.5378306099772453, F1 Score: 0.753994931995656\n",
      "Step 46400, Avg Loss: 0.5054972571134567, F1 Score: 0.7566405650237985\n",
      "Step 46500, Avg Loss: 0.4610833355039358, F1 Score: 0.7565101496821817\n",
      "Step 46600, Avg Loss: 0.561726132594049, F1 Score: 0.7591894919051013\n",
      "Step 46700, Avg Loss: 0.4994109145924449, F1 Score: 0.7607660350592765\n",
      "Step 46800, Avg Loss: 0.5465809193253517, F1 Score: 0.7615007827096905\n",
      "Step 46900, Avg Loss: 0.478868330642581, F1 Score: 0.7573897923698476\n",
      "Step 47000, Avg Loss: 0.5379256463795901, F1 Score: 0.7567428981642909\n",
      "Step 47100, Avg Loss: 0.5341079711169004, F1 Score: 0.7547325527415278\n",
      "Step 47200, Avg Loss: 0.5253808230906725, F1 Score: 0.7568426447975397\n",
      "Step 47300, Avg Loss: 0.5733938736468553, F1 Score: 0.7550809862787579\n",
      "Step 47400, Avg Loss: 0.4840201014280319, F1 Score: 0.7533748701973001\n",
      "Step 47500, Avg Loss: 0.510939245223999, F1 Score: 0.7573600942092059\n",
      "Step 47600, Avg Loss: 0.5296651370823383, F1 Score: 0.7607691917398143\n",
      "Step 47700, Avg Loss: 0.510103981345892, F1 Score: 0.7616779815584153\n",
      "Step 47800, Avg Loss: 0.4468135917186737, F1 Score: 0.7596148948092303\n",
      "Step 47900, Avg Loss: 0.5482831288874149, F1 Score: 0.7607063121574995\n",
      "Step 48000, Avg Loss: 0.49501636773347857, F1 Score: 0.7612922827628115\n",
      "Step 48100, Avg Loss: 0.4529418251663446, F1 Score: 0.7578828828828829\n",
      "Step 48200, Avg Loss: 0.45266241922974587, F1 Score: 0.7572805578342904\n",
      "Step 48300, Avg Loss: 0.44074808061122894, F1 Score: 0.7604145498882341\n",
      "Step 48400, Avg Loss: 0.5524354500323534, F1 Score: 0.7596938775510204\n",
      "Step 48500, Avg Loss: 0.5096025893837214, F1 Score: 0.7629696786541755\n",
      "Step 48600, Avg Loss: 0.5277050862461329, F1 Score: 0.7657644185235033\n",
      "Step 48700, Avg Loss: 0.5444533412158489, F1 Score: 0.7663208062354168\n",
      "Step 48800, Avg Loss: 0.539296320155263, F1 Score: 0.7636418450406994\n",
      "Step 48900, Avg Loss: 0.4700122591853142, F1 Score: 0.7659701790255822\n",
      "Step 49000, Avg Loss: 0.5131751073151827, F1 Score: 0.76649245636525\n",
      "Step 49100, Avg Loss: 0.5050711961090565, F1 Score: 0.7666534496431404\n",
      "Step 49200, Avg Loss: 0.47893705472350123, F1 Score: 0.7661101420383752\n",
      "Step 49300, Avg Loss: 0.5047433852404356, F1 Score: 0.7666136092319936\n",
      "Step 49400, Avg Loss: 0.48857589237391946, F1 Score: 0.766457524150981\n",
      "Step 49500, Avg Loss: 0.47836568266153334, F1 Score: 0.7649506240914331\n",
      "Step 49600, Avg Loss: 0.551763833463192, F1 Score: 0.7650152782647899\n",
      "Step 49700, Avg Loss: 0.4676052211970091, F1 Score: 0.7666865909543734\n",
      "Step 49800, Avg Loss: 0.4998586754500866, F1 Score: 0.7653555899845069\n",
      "Step 49900, Avg Loss: 0.5799253802746535, F1 Score: 0.7668942317205897\n",
      "Step 50000, Avg Loss: 0.5146189370006323, F1 Score: 0.7650656773287877\n",
      "Step 50100, Avg Loss: 0.5014372509717941, F1 Score: 0.7667639926758054\n",
      "Step 50200, Avg Loss: 0.4916894174739718, F1 Score: 0.7664801149026794\n",
      "Step 50300, Avg Loss: 0.4749669224768877, F1 Score: 0.7665557156341597\n",
      "Step 50400, Avg Loss: 0.5486560177803039, F1 Score: 0.7632030697768353\n",
      "Step 50500, Avg Loss: 0.49928829319775103, F1 Score: 0.7616582940072055\n",
      "Step 50600, Avg Loss: 0.49834643945097923, F1 Score: 0.7589559877175026\n",
      "Step 50700, Avg Loss: 0.5576628276705742, F1 Score: 0.7596463433331629\n",
      "Step 50800, Avg Loss: 0.512269892692566, F1 Score: 0.7644005442725395\n",
      "Step 50900, Avg Loss: 0.4866046305000782, F1 Score: 0.7636730575176589\n",
      "Step 51000, Avg Loss: 0.5285011020302772, F1 Score: 0.7669388765157139\n",
      "Step 51100, Avg Loss: 0.4728165780007839, F1 Score: 0.7669864504005538\n",
      "Step 51200, Avg Loss: 0.5225731278955936, F1 Score: 0.7682216456997406\n",
      "Step 51300, Avg Loss: 0.5895005284249782, F1 Score: 0.7675039556962026\n",
      "Step 51400, Avg Loss: 0.4624010626971722, F1 Score: 0.7669478908188586\n",
      "Step 51500, Avg Loss: 0.5280956422165036, F1 Score: 0.7672004744958482\n",
      "Step 51600, Avg Loss: 0.592047276198864, F1 Score: 0.7684603299293009\n",
      "Step 51700, Avg Loss: 0.5543670532107353, F1 Score: 0.7683554858011673\n",
      "Step 51800, Avg Loss: 0.5091484022140503, F1 Score: 0.7686728092530877\n",
      "Step 51900, Avg Loss: 0.4853172045573592, F1 Score: 0.7689520078354555\n",
      "Step 52000, Avg Loss: 0.5254201038181782, F1 Score: 0.768533569774888\n",
      "Step 52100, Avg Loss: 0.5264833751320839, F1 Score: 0.7683281565843317\n",
      "Step 52200, Avg Loss: 0.5424074517935514, F1 Score: 0.7688676926102561\n",
      "Step 52300, Avg Loss: 0.4862080019712448, F1 Score: 0.7680371614943665\n",
      "Step 52400, Avg Loss: 0.5078365501016379, F1 Score: 0.7675825810931455\n",
      "Step 52500, Avg Loss: 0.49725399069488047, F1 Score: 0.76747077841333\n",
      "Step 52600, Avg Loss: 0.4987224007025361, F1 Score: 0.767363184079602\n",
      "Step 52700, Avg Loss: 0.5216966383904219, F1 Score: 0.7689650066630472\n",
      "Step 52800, Avg Loss: 0.44407003067433837, F1 Score: 0.7691096126396614\n",
      "Step 52900, Avg Loss: 0.5455464965850115, F1 Score: 0.7696309383588535\n",
      "Step 53000, Avg Loss: 0.49527295894920825, F1 Score: 0.7696747370995354\n",
      "Step 53100, Avg Loss: 0.5186595343798399, F1 Score: 0.7697530256813933\n",
      "Step 53200, Avg Loss: 0.49877245269715786, F1 Score: 0.7697601258356273\n",
      "Step 53300, Avg Loss: 0.4717977187037468, F1 Score: 0.7690790771051075\n",
      "Step 53400, Avg Loss: 0.5852437914162875, F1 Score: 0.7677110357214129\n",
      "Step 53500, Avg Loss: 0.5060905299335718, F1 Score: 0.7654432865126114\n",
      "Step 53600, Avg Loss: 0.5241812287271023, F1 Score: 0.7641085232152781\n",
      "Step 53700, Avg Loss: 0.5268062244728208, F1 Score: 0.7632742238707383\n",
      "Step 53800, Avg Loss: 0.5289930243045091, F1 Score: 0.7629191723575578\n",
      "Step 53900, Avg Loss: 0.5571666998416185, F1 Score: 0.7638776337115073\n",
      "Step 54000, Avg Loss: 0.5650255123525858, F1 Score: 0.7639149627073926\n",
      "Step 54100, Avg Loss: 0.4818005393445492, F1 Score: 0.7641222919619356\n",
      "Step 54200, Avg Loss: 0.5010356463491916, F1 Score: 0.7676666499724546\n",
      "Step 54300, Avg Loss: 0.4742067927122116, F1 Score: 0.7682304691407422\n",
      "Step 54400, Avg Loss: 0.48250980146229266, F1 Score: 0.7681173887003394\n",
      "Step 54500, Avg Loss: 0.48016487784683703, F1 Score: 0.7647562881193608\n",
      "Step 54600, Avg Loss: 0.5246066772192717, F1 Score: 0.7648305084745762\n",
      "Step 54700, Avg Loss: 0.5528467018157244, F1 Score: 0.7676990043528293\n",
      "Step 54800, Avg Loss: 0.5332876326143742, F1 Score: 0.7678052689572273\n",
      "Step 54900, Avg Loss: 0.4762873411923647, F1 Score: 0.7674488567990373\n",
      "Step 55000, Avg Loss: 0.5408213344216347, F1 Score: 0.7678821879382889\n",
      "Step 55100, Avg Loss: 0.5576083857938647, F1 Score: 0.7658910043316208\n",
      "Step 55200, Avg Loss: 0.478733657002449, F1 Score: 0.767719298245614\n",
      "Step 55300, Avg Loss: 0.5250440227612853, F1 Score: 0.7672058675776148\n",
      "Step 55400, Avg Loss: 0.4545389721542597, F1 Score: 0.7674605567279671\n",
      "Step 55500, Avg Loss: 0.5750842788815498, F1 Score: 0.7683542405917337\n",
      "Step 55600, Avg Loss: 0.5648007161729037, F1 Score: 0.7701752648777107\n",
      "Step 55700, Avg Loss: 0.4913214568607509, F1 Score: 0.7666314677930306\n",
      "Step 55800, Avg Loss: 0.5241639541089534, F1 Score: 0.7656155245603395\n",
      "Step 55900, Avg Loss: 0.5143852573260665, F1 Score: 0.765072133637054\n",
      "Step 56000, Avg Loss: 0.505997392013669, F1 Score: 0.7647807709935326\n",
      "Step 56100, Avg Loss: 0.46812019728124143, F1 Score: 0.7651776649746193\n",
      "Step 56200, Avg Loss: 0.492643274217844, F1 Score: 0.7617678763923823\n",
      "Step 56300, Avg Loss: 0.4890154889971018, F1 Score: 0.7620661326761142\n",
      "Step 56400, Avg Loss: 0.4966346937417984, F1 Score: 0.7628316770822662\n",
      "Step 56500, Avg Loss: 0.5205522733181716, F1 Score: 0.7628316770822662\n",
      "Step 56600, Avg Loss: 0.472597943469882, F1 Score: 0.7644208700974142\n",
      "Step 56700, Avg Loss: 0.4894244176894426, F1 Score: 0.7653004215551831\n",
      "Step 56800, Avg Loss: 0.49542697504162786, F1 Score: 0.7677810636058343\n",
      "Step 56900, Avg Loss: 0.5371034266054631, F1 Score: 0.7705829774555567\n",
      "Step 57000, Avg Loss: 0.513528048992157, F1 Score: 0.7707838479809976\n",
      "Step 57100, Avg Loss: 0.5365680912137032, F1 Score: 0.7708961494735801\n",
      "Step 57200, Avg Loss: 0.5130964984372258, F1 Score: 0.7711725981289907\n",
      "Step 57300, Avg Loss: 0.5111098910868168, F1 Score: 0.7709624634303566\n",
      "Step 57400, Avg Loss: 0.5191323026642203, F1 Score: 0.7699212127256407\n",
      "Step 57500, Avg Loss: 0.500032129958272, F1 Score: 0.766225900878167\n",
      "Step 57600, Avg Loss: 0.5394275784492493, F1 Score: 0.7658732172765569\n",
      "Step 57700, Avg Loss: 0.4816676130518317, F1 Score: 0.7660287324229657\n",
      "Step 57800, Avg Loss: 0.45270287584513424, F1 Score: 0.7659206510681587\n",
      "Step 57900, Avg Loss: 0.5102728389948606, F1 Score: 0.7659358505887129\n",
      "Step 58000, Avg Loss: 0.5032037910073995, F1 Score: 0.7684142005908567\n",
      "Step 58100, Avg Loss: 0.5357134690880776, F1 Score: 0.768451664661051\n",
      "Step 58200, Avg Loss: 0.4259333004802465, F1 Score: 0.7674078555942117\n",
      "Step 58300, Avg Loss: 0.49168478392064574, F1 Score: 0.7674395161290323\n",
      "Step 58400, Avg Loss: 0.4274511769413948, F1 Score: 0.7692769877828961\n",
      "Step 58500, Avg Loss: 0.4709010449051857, F1 Score: 0.770451952175423\n",
      "Step 58600, Avg Loss: 0.5331090771406889, F1 Score: 0.7719747733543555\n",
      "Step 58700, Avg Loss: 0.48886208929121494, F1 Score: 0.7724976705409249\n",
      "Step 58800, Avg Loss: 0.5758917108550667, F1 Score: 0.7729134165366615\n",
      "Step 58900, Avg Loss: 0.499637982621789, F1 Score: 0.7720678632898943\n",
      "Step 59000, Avg Loss: 0.45701446518301964, F1 Score: 0.771957463568334\n",
      "Step 59100, Avg Loss: 0.48956865806132555, F1 Score: 0.7729411764705882\n",
      "Step 59200, Avg Loss: 0.5346351010724902, F1 Score: 0.7729600546261522\n",
      "Step 59300, Avg Loss: 0.46282134391367435, F1 Score: 0.7729840280483055\n",
      "Step 59400, Avg Loss: 0.4771902575343847, F1 Score: 0.773945987954148\n",
      "Step 59500, Avg Loss: 0.529839968085289, F1 Score: 0.7732682926829268\n",
      "Step 59600, Avg Loss: 0.5126879417896271, F1 Score: 0.7735628829880362\n",
      "Step 59700, Avg Loss: 0.4700287961214781, F1 Score: 0.7736737368344416\n",
      "Step 59800, Avg Loss: 0.5515970878303051, F1 Score: 0.7734930372967183\n",
      "Step 59900, Avg Loss: 0.48882031083106997, F1 Score: 0.7736521102707977\n",
      "Step 60000, Avg Loss: 0.49158202987164257, F1 Score: 0.7733724914341654\n",
      "Step 60100, Avg Loss: 0.5065658069401979, F1 Score: 0.7732484076433122\n",
      "Step 60200, Avg Loss: 0.4916053545475006, F1 Score: 0.7734887822082884\n",
      "Step 60300, Avg Loss: 0.532571703121066, F1 Score: 0.7737097485663873\n",
      "Step 60400, Avg Loss: 0.50112593062222, F1 Score: 0.7731340348116825\n",
      "Step 60500, Avg Loss: 0.4991502846777439, F1 Score: 0.7722164463587307\n",
      "Step 60600, Avg Loss: 0.526927861943841, F1 Score: 0.7726105845181674\n",
      "Step 60700, Avg Loss: 0.5218475031852722, F1 Score: 0.771948980098268\n",
      "Step 60800, Avg Loss: 0.47758784782141445, F1 Score: 0.7717223905807541\n",
      "Step 60900, Avg Loss: 0.5126675891503691, F1 Score: 0.771776397515528\n",
      "Step 61000, Avg Loss: 0.5157359675876796, F1 Score: 0.7711817864097059\n",
      "Step 61100, Avg Loss: 0.46264945454895495, F1 Score: 0.7716261254539124\n",
      "Step 61200, Avg Loss: 0.4636624163761735, F1 Score: 0.7716300784742227\n",
      "Step 61300, Avg Loss: 0.4880444350093603, F1 Score: 0.7698177619358402\n",
      "Step 61400, Avg Loss: 0.49354498863220214, F1 Score: 0.7695012305992265\n",
      "Step 61500, Avg Loss: 0.48247399121522905, F1 Score: 0.7696014456379882\n",
      "Step 61600, Avg Loss: 0.5014612737298012, F1 Score: 0.7697635898208102\n",
      "Step 61700, Avg Loss: 0.512591441962868, F1 Score: 0.7700953336678374\n",
      "Step 61800, Avg Loss: 0.5631201508827508, F1 Score: 0.7697729446709963\n",
      "Step 61900, Avg Loss: 0.43373946238309147, F1 Score: 0.769804099310067\n",
      "Step 62000, Avg Loss: 0.5143808236718178, F1 Score: 0.7714828137490009\n",
      "Step 62100, Avg Loss: 0.5384890226274729, F1 Score: 0.7698963678438475\n",
      "Step 62200, Avg Loss: 0.4763751481100917, F1 Score: 0.7698883636729358\n",
      "Step 62300, Avg Loss: 0.5489537569135428, F1 Score: 0.7699279923460396\n",
      "Step 62400, Avg Loss: 0.5022409987449646, F1 Score: 0.7699592001208885\n",
      "Step 62500, Avg Loss: 0.42395324558019637, F1 Score: 0.7713856928464232\n",
      "Step 62600, Avg Loss: 0.5441913307830691, F1 Score: 0.7711422845691382\n",
      "Step 62700, Avg Loss: 0.47909779999405144, F1 Score: 0.7720995163783217\n",
      "Step 62800, Avg Loss: 0.5839566626958549, F1 Score: 0.7727069128552232\n",
      "Step 62900, Avg Loss: 0.5305370128154755, F1 Score: 0.7733452416393768\n",
      "Step 63000, Avg Loss: 0.5992942652106286, F1 Score: 0.770447219983884\n",
      "Step 63100, Avg Loss: 0.5017489705979824, F1 Score: 0.7702546004537434\n",
      "Step 63200, Avg Loss: 0.5247775047272444, F1 Score: 0.7713484833316648\n",
      "Step 63300, Avg Loss: 0.420809882581234, F1 Score: 0.7731426301152166\n",
      "Step 63400, Avg Loss: 0.48213749680668117, F1 Score: 0.7740757194333383\n",
      "Step 63500, Avg Loss: 0.4765759291499853, F1 Score: 0.7748814006944784\n",
      "Step 63600, Avg Loss: 0.5173134861141443, F1 Score: 0.7751444520615023\n",
      "Step 63700, Avg Loss: 0.6101818381994962, F1 Score: 0.7742604573065337\n",
      "Step 63800, Avg Loss: 0.4929016716405749, F1 Score: 0.7737877061766308\n",
      "Step 63900, Avg Loss: 0.5377399701625108, F1 Score: 0.7718278815809247\n",
      "Step 64000, Avg Loss: 0.4558073228225112, F1 Score: 0.7725047457288441\n",
      "Step 64100, Avg Loss: 0.4964657781273127, F1 Score: 0.774330809954975\n",
      "Step 64200, Avg Loss: 0.5622452789545059, F1 Score: 0.7732589974612972\n",
      "Step 64300, Avg Loss: 0.5320359955355525, F1 Score: 0.7745447347585115\n",
      "Step 64400, Avg Loss: 0.505406742580235, F1 Score: 0.774668775954123\n",
      "Step 64500, Avg Loss: 0.5222373925894499, F1 Score: 0.7751290243303023\n",
      "Step 64600, Avg Loss: 0.513101447224617, F1 Score: 0.7750875289708565\n",
      "Step 64700, Avg Loss: 0.5151480061933398, F1 Score: 0.7747062888735314\n",
      "Step 64800, Avg Loss: 0.48616064723581076, F1 Score: 0.7750332791007247\n",
      "Step 64900, Avg Loss: 0.5327945207431912, F1 Score: 0.7741903486584337\n",
      "Step 65000, Avg Loss: 0.5025027782842517, F1 Score: 0.7747160493827161\n",
      "Step 65100, Avg Loss: 0.5391501303389669, F1 Score: 0.7751257520465529\n",
      "Step 65200, Avg Loss: 0.4809041061997414, F1 Score: 0.7748301680963952\n",
      "Step 65300, Avg Loss: 0.4790825265459716, F1 Score: 0.7747094467070627\n",
      "Step 65400, Avg Loss: 0.4913182534649968, F1 Score: 0.775997641045803\n",
      "Step 65500, Avg Loss: 0.5189518801867962, F1 Score: 0.774639482844356\n",
      "Step 65600, Avg Loss: 0.517611365802586, F1 Score: 0.772722717707185\n",
      "Step 65700, Avg Loss: 0.5346727981418371, F1 Score: 0.7727113117632305\n",
      "Step 65800, Avg Loss: 0.48209795661270616, F1 Score: 0.7734668335419274\n",
      "Step 65900, Avg Loss: 0.5325951062887907, F1 Score: 0.7752236887636561\n",
      "Step 66000, Avg Loss: 0.5250000961124897, F1 Score: 0.7753103516494386\n",
      "Step 66100, Avg Loss: 0.4400756945088506, F1 Score: 0.7767865895500441\n",
      "Step 66200, Avg Loss: 0.43581640735268595, F1 Score: 0.7763190244382161\n",
      "Step 66300, Avg Loss: 0.49318761222064494, F1 Score: 0.7764026806241745\n",
      "Step 66400, Avg Loss: 0.46968716409057376, F1 Score: 0.77711283024376\n",
      "Step 66500, Avg Loss: 0.5960129265487194, F1 Score: 0.7763410197158169\n",
      "Step 66600, Avg Loss: 0.45062414050102234, F1 Score: 0.7759444417081219\n",
      "Step 66700, Avg Loss: 0.5359309564717114, F1 Score: 0.7753504730767325\n",
      "Step 66800, Avg Loss: 0.5110598424822093, F1 Score: 0.7755243714979918\n",
      "Step 66900, Avg Loss: 0.5437553932890296, F1 Score: 0.7758126667325363\n",
      "Step 67000, Avg Loss: 0.4735114758089185, F1 Score: 0.7754372019077902\n",
      "Step 67100, Avg Loss: 0.5263842077925801, F1 Score: 0.7745244497560004\n",
      "Step 67200, Avg Loss: 0.5342340534180403, F1 Score: 0.7756378437406929\n",
      "Step 67300, Avg Loss: 0.5177089155465364, F1 Score: 0.7744319600499375\n",
      "Step 67400, Avg Loss: 0.4897785907238722, F1 Score: 0.7736501729583396\n",
      "Step 67500, Avg Loss: 0.5061380230635405, F1 Score: 0.7723818630164562\n",
      "Step 67600, Avg Loss: 0.49543091773986814, F1 Score: 0.7720447445329034\n",
      "Step 67700, Avg Loss: 0.44216544480994346, F1 Score: 0.7707867452734296\n",
      "Step 67800, Avg Loss: 0.45513861052691934, F1 Score: 0.7720606763090259\n",
      "Step 67900, Avg Loss: 0.503363364264369, F1 Score: 0.7720591949088338\n",
      "Step 68000, Avg Loss: 0.531919043790549, F1 Score: 0.7705068186444127\n",
      "Step 68100, Avg Loss: 0.4801013720035553, F1 Score: 0.7699607162899852\n",
      "Step 68200, Avg Loss: 0.5431546700745821, F1 Score: 0.77005456678056\n",
      "Step 68300, Avg Loss: 0.4410883073881269, F1 Score: 0.7720789074355083\n",
      "Step 68400, Avg Loss: 0.4718070402741432, F1 Score: 0.7729653505237711\n",
      "Step 68500, Avg Loss: 0.4491929342597723, F1 Score: 0.7711455422910846\n",
      "Step 68600, Avg Loss: 0.4506581810489297, F1 Score: 0.771689497716895\n",
      "Step 68700, Avg Loss: 0.5335113898664713, F1 Score: 0.7718052738336714\n",
      "Step 68800, Avg Loss: 0.5205637099593878, F1 Score: 0.773765214767126\n",
      "Step 68900, Avg Loss: 0.572156857252121, F1 Score: 0.773552551946742\n",
      "Step 69000, Avg Loss: 0.4989215166121721, F1 Score: 0.7734260799435455\n",
      "Step 69100, Avg Loss: 0.5283361370489001, F1 Score: 0.7720670391061453\n",
      "Step 69200, Avg Loss: 0.48130840837955474, F1 Score: 0.7691756272401433\n",
      "Step 69300, Avg Loss: 0.47692144226282834, F1 Score: 0.7666855640880276\n",
      "Step 69400, Avg Loss: 0.4674758017808199, F1 Score: 0.765625\n",
      "Step 69500, Avg Loss: 0.4803386840596795, F1 Score: 0.7651236327821264\n",
      "Step 69600, Avg Loss: 0.49135155204683545, F1 Score: 0.7646234494212902\n",
      "Step 69700, Avg Loss: 0.5255882092565298, F1 Score: 0.7667784369357751\n",
      "Step 69800, Avg Loss: 0.49106442745774986, F1 Score: 0.7693490517683239\n",
      "Step 69900, Avg Loss: 0.4273411362618208, F1 Score: 0.770688952303302\n",
      "Step 70000, Avg Loss: 0.4758450470119715, F1 Score: 0.7742423478392416\n",
      "Step 70100, Avg Loss: 0.5100113585591316, F1 Score: 0.7722360342990512\n",
      "Step 70200, Avg Loss: 0.4851129450649023, F1 Score: 0.7724459774779344\n",
      "Step 70300, Avg Loss: 0.4933282268792391, F1 Score: 0.7705654215145948\n",
      "Step 70400, Avg Loss: 0.5529248689860106, F1 Score: 0.7720689129440463\n",
      "Step 70500, Avg Loss: 0.47993059806525706, F1 Score: 0.7745641439080924\n",
      "Step 70600, Avg Loss: 0.5101558522321283, F1 Score: 0.7735934893595511\n",
      "Step 70700, Avg Loss: 0.41879689775407314, F1 Score: 0.7744243462488033\n",
      "Step 70800, Avg Loss: 0.549117701947689, F1 Score: 0.7748943874471937\n",
      "Step 70900, Avg Loss: 0.5471721906214952, F1 Score: 0.7737926853909881\n",
      "Step 71000, Avg Loss: 0.45077295016497376, F1 Score: 0.7742780829511666\n",
      "Step 71100, Avg Loss: 0.4703451709449291, F1 Score: 0.7750740053183482\n",
      "Step 71200, Avg Loss: 0.4394040437787771, F1 Score: 0.7751961375980688\n",
      "Step 71300, Avg Loss: 0.4827827087044716, F1 Score: 0.7758395989974938\n",
      "Step 71400, Avg Loss: 0.4675053069740534, F1 Score: 0.7760865211295814\n",
      "Step 71500, Avg Loss: 0.44123709155246615, F1 Score: 0.776223428087818\n",
      "Step 71600, Avg Loss: 0.4999735188111663, F1 Score: 0.7763401109057301\n",
      "Step 71700, Avg Loss: 0.453151941485703, F1 Score: 0.776131357629155\n",
      "Step 71800, Avg Loss: 0.4556555846706033, F1 Score: 0.7756063339346563\n",
      "Step 71900, Avg Loss: 0.527895667552948, F1 Score: 0.7731007201541739\n",
      "Step 72000, Avg Loss: 0.4624796071276069, F1 Score: 0.7736087903184972\n",
      "Step 72100, Avg Loss: 0.4935045319423079, F1 Score: 0.769728065674705\n",
      "Step 72200, Avg Loss: 0.4355907970666885, F1 Score: 0.770962296004502\n",
      "Step 72300, Avg Loss: 0.5336340292543172, F1 Score: 0.7707746659157237\n",
      "Step 72400, Avg Loss: 0.5136749194562436, F1 Score: 0.77167924913283\n",
      "Step 72500, Avg Loss: 0.4982702725753188, F1 Score: 0.7697368421052632\n",
      "Step 72600, Avg Loss: 0.44105518743395805, F1 Score: 0.7688980432543769\n",
      "Step 72700, Avg Loss: 0.4327620904892683, F1 Score: 0.7675921617289696\n",
      "Step 72800, Avg Loss: 0.4939312084019184, F1 Score: 0.7667132722282637\n",
      "Step 72900, Avg Loss: 0.4459155187383294, F1 Score: 0.7687236015246729\n",
      "Step 73000, Avg Loss: 0.43225995138287543, F1 Score: 0.7668651307274139\n",
      "Step 73100, Avg Loss: 0.5042752623185516, F1 Score: 0.764754953076121\n",
      "Step 73200, Avg Loss: 0.541192690692842, F1 Score: 0.765437236643604\n",
      "Step 73300, Avg Loss: 0.5195802762545645, F1 Score: 0.7672012415933782\n",
      "Step 73400, Avg Loss: 0.4842859930172563, F1 Score: 0.7682228527211762\n",
      "Step 73500, Avg Loss: 0.4775645628944039, F1 Score: 0.765528353864546\n",
      "Step 73600, Avg Loss: 0.43462368786334993, F1 Score: 0.7669445451250453\n",
      "Step 73700, Avg Loss: 0.5116043367236852, F1 Score: 0.7698082163607384\n",
      "Step 73800, Avg Loss: 0.5386160073801876, F1 Score: 0.7714490234175273\n",
      "Step 73900, Avg Loss: 0.4168445528671145, F1 Score: 0.7699183233163815\n",
      "Step 74000, Avg Loss: 0.4761008371785283, F1 Score: 0.7720697234575474\n",
      "Step 74100, Avg Loss: 0.44574723318219184, F1 Score: 0.7715704849198628\n",
      "Step 74200, Avg Loss: 0.4555296381190419, F1 Score: 0.7718258024943774\n",
      "Step 74300, Avg Loss: 0.4748778484016657, F1 Score: 0.7718975302960577\n",
      "Step 74400, Avg Loss: 0.4210032794997096, F1 Score: 0.7690326568455753\n",
      "Step 74500, Avg Loss: 0.5361615314334631, F1 Score: 0.7670551670551671\n",
      "Step 74600, Avg Loss: 0.5253548340499401, F1 Score: 0.7689218412109978\n",
      "Step 74700, Avg Loss: 0.510136573985219, F1 Score: 0.7694048660048352\n",
      "Step 74800, Avg Loss: 0.4680924245715141, F1 Score: 0.7674238112485124\n",
      "Step 74900, Avg Loss: 0.621444389782846, F1 Score: 0.7691990940910026\n",
      "Step 75000, Avg Loss: 0.47399944946169853, F1 Score: 0.768754833720031\n",
      "Step 75100, Avg Loss: 0.45562409702688456, F1 Score: 0.7692941358183597\n",
      "Step 75200, Avg Loss: 0.5172727839276194, F1 Score: 0.769476175774416\n",
      "Step 75300, Avg Loss: 0.4785663293674588, F1 Score: 0.7712264150943396\n",
      "Step 75400, Avg Loss: 0.5034300607815385, F1 Score: 0.7728084043041461\n",
      "Step 75500, Avg Loss: 0.457154232673347, F1 Score: 0.772972972972973\n",
      "Step 75600, Avg Loss: 0.48810607500374315, F1 Score: 0.7726645896113182\n",
      "Step 75700, Avg Loss: 0.4356864437647164, F1 Score: 0.7735060167244544\n",
      "Step 75800, Avg Loss: 0.46048883471637964, F1 Score: 0.7737070063694268\n",
      "Step 75900, Avg Loss: 0.5930598266236484, F1 Score: 0.7723278152462701\n",
      "Step 76000, Avg Loss: 0.5124907886236906, F1 Score: 0.7748260273276781\n",
      "Step 76100, Avg Loss: 0.48289712170138954, F1 Score: 0.7750938800365371\n",
      "Step 76200, Avg Loss: 0.38506727751344444, F1 Score: 0.7769776927337732\n",
      "Step 76300, Avg Loss: 0.5055473981425166, F1 Score: 0.7774710104914412\n",
      "Step 76400, Avg Loss: 0.4414955012127757, F1 Score: 0.7784569138276554\n",
      "Step 76500, Avg Loss: 0.47829744707793, F1 Score: 0.7782493633594647\n",
      "Step 76600, Avg Loss: 0.5266636704653501, F1 Score: 0.7784980552508228\n",
      "Step 76700, Avg Loss: 0.47063842341303824, F1 Score: 0.7796543209876543\n",
      "Step 76800, Avg Loss: 0.4691811959818006, F1 Score: 0.7803869443213706\n",
      "Step 76900, Avg Loss: 0.5110479943454266, F1 Score: 0.7805884375460485\n",
      "Step 77000, Avg Loss: 0.508882712405175, F1 Score: 0.7808541973490427\n",
      "Step 77100, Avg Loss: 0.45056317914277316, F1 Score: 0.7811890838206628\n",
      "Step 77200, Avg Loss: 0.4565975390933454, F1 Score: 0.7809877025180558\n",
      "Step 77300, Avg Loss: 0.4648363595083356, F1 Score: 0.781124300990876\n",
      "Step 77400, Avg Loss: 0.5454057116061449, F1 Score: 0.7801264322402213\n",
      "Step 77500, Avg Loss: 0.5226719501614571, F1 Score: 0.7794708889660992\n",
      "Step 77600, Avg Loss: 0.509643204510212, F1 Score: 0.7808616200815445\n",
      "Step 77700, Avg Loss: 0.43964059378951786, F1 Score: 0.7810889096175561\n",
      "Step 77800, Avg Loss: 0.5783190184086561, F1 Score: 0.7806680461129175\n",
      "Step 77900, Avg Loss: 0.5374741191789508, F1 Score: 0.779874213836478\n",
      "Step 78000, Avg Loss: 0.4156192742008716, F1 Score: 0.7792143212332173\n",
      "Step 78100, Avg Loss: 0.4794092942215502, F1 Score: 0.7797317436661699\n",
      "Step 78200, Avg Loss: 0.43966824175789954, F1 Score: 0.7809533179202125\n",
      "Step 78300, Avg Loss: 0.47133203931152823, F1 Score: 0.7801915670978572\n",
      "Step 78400, Avg Loss: 0.5074131108820439, F1 Score: 0.781292984869326\n",
      "Step 78500, Avg Loss: 0.45236126877367494, F1 Score: 0.7802371541501977\n",
      "Step 78600, Avg Loss: 0.4612368805706501, F1 Score: 0.7802921515226541\n",
      "Step 78700, Avg Loss: 0.47509851031005385, F1 Score: 0.7799285288862418\n",
      "Step 78800, Avg Loss: 0.551170173920691, F1 Score: 0.7799642218246869\n",
      "Step 78900, Avg Loss: 0.49564363675191997, F1 Score: 0.7801707026597856\n",
      "Step 79000, Avg Loss: 0.492673862837255, F1 Score: 0.7810125333070167\n",
      "Step 79100, Avg Loss: 0.5725959299132228, F1 Score: 0.7817691742220044\n",
      "Step 79200, Avg Loss: 0.48514132108539343, F1 Score: 0.7814191516583013\n",
      "Step 79300, Avg Loss: 0.45318314250558617, F1 Score: 0.7805555555555556\n",
      "Step 79400, Avg Loss: 0.4524713022261858, F1 Score: 0.780277074333383\n",
      "Step 79500, Avg Loss: 0.5086379478685558, F1 Score: 0.7805794521902502\n",
      "Step 79600, Avg Loss: 0.507616750523448, F1 Score: 0.7798942854293408\n",
      "Step 79700, Avg Loss: 0.4776650996506214, F1 Score: 0.7795747229709494\n",
      "Step 79800, Avg Loss: 0.49939176343381403, F1 Score: 0.7798339501850555\n",
      "Step 79900, Avg Loss: 0.48639214769005773, F1 Score: 0.7802909525707453\n",
      "Step 80000, Avg Loss: 0.5281348774768412, F1 Score: 0.7807391261270187\n",
      "Step 80100, Avg Loss: 0.4518848714604974, F1 Score: 0.7805917628983496\n",
      "Step 80200, Avg Loss: 0.4953698533214629, F1 Score: 0.7804345669213215\n",
      "Step 80300, Avg Loss: 0.5228097644634545, F1 Score: 0.7795507978590366\n",
      "Step 80400, Avg Loss: 0.46676842287182807, F1 Score: 0.7778226213298355\n",
      "Step 80500, Avg Loss: 0.5072055694274604, F1 Score: 0.7795795795795796\n",
      "Step 80600, Avg Loss: 0.4546383575350046, F1 Score: 0.7801984146767037\n",
      "Step 80700, Avg Loss: 0.4820705225504935, F1 Score: 0.7809674391027225\n",
      "Step 80800, Avg Loss: 0.48506663225591184, F1 Score: 0.7805748265609515\n",
      "Step 80900, Avg Loss: 0.485934988707304, F1 Score: 0.7808151093439364\n",
      "Step 81000, Avg Loss: 0.5091755509749055, F1 Score: 0.7805800129333931\n",
      "Step 81100, Avg Loss: 0.46106834545731545, F1 Score: 0.7801764969835967\n",
      "Step 81200, Avg Loss: 0.5015141709148884, F1 Score: 0.7795051237190702\n",
      "Step 81300, Avg Loss: 0.5340923774987459, F1 Score: 0.7807178672491685\n",
      "Step 81400, Avg Loss: 0.4992511922866106, F1 Score: 0.7805676855895196\n",
      "Step 81500, Avg Loss: 0.4922800775617361, F1 Score: 0.78041027169324\n",
      "Step 81600, Avg Loss: 0.43843648698180915, F1 Score: 0.7803682209325595\n",
      "Step 81700, Avg Loss: 0.4731166998669505, F1 Score: 0.7789600481830957\n",
      "Step 81800, Avg Loss: 0.4572792279347777, F1 Score: 0.7788447036348097\n",
      "Step 81900, Avg Loss: 0.42771751508116723, F1 Score: 0.7786996530396741\n",
      "Step 82000, Avg Loss: 0.4805271802470088, F1 Score: 0.7789048700479435\n",
      "Step 82100, Avg Loss: 0.5129264143109321, F1 Score: 0.778924060188214\n",
      "Step 82200, Avg Loss: 0.5106190951913595, F1 Score: 0.778434642008994\n",
      "Step 82300, Avg Loss: 0.47435596471652386, F1 Score: 0.7787806968887095\n",
      "Step 82400, Avg Loss: 0.5159488072991372, F1 Score: 0.7792625339093741\n",
      "Step 82500, Avg Loss: 0.43777014760300514, F1 Score: 0.7792129466753782\n",
      "Step 82600, Avg Loss: 0.45314377479255197, F1 Score: 0.7787958115183246\n",
      "Step 82700, Avg Loss: 0.4389467829838395, F1 Score: 0.7798307545941615\n",
      "Step 82800, Avg Loss: 0.5001830848678946, F1 Score: 0.780256051210242\n",
      "Step 82900, Avg Loss: 0.43963937206193804, F1 Score: 0.7807305663382104\n",
      "Step 83000, Avg Loss: 0.5154056955128908, F1 Score: 0.7802269205777977\n",
      "Step 83100, Avg Loss: 0.42442623812705277, F1 Score: 0.7788369395269078\n",
      "Step 83200, Avg Loss: 0.46696961894631384, F1 Score: 0.7799158147925436\n",
      "Step 83300, Avg Loss: 0.4317522741481662, F1 Score: 0.7821002031815254\n",
      "Step 83400, Avg Loss: 0.4574029664788395, F1 Score: 0.7819712729073799\n",
      "Step 83500, Avg Loss: 0.4977167224138975, F1 Score: 0.7818820155268753\n",
      "Step 83600, Avg Loss: 0.4629786830767989, F1 Score: 0.7836562100501524\n",
      "Step 83700, Avg Loss: 0.4826134745031595, F1 Score: 0.7826044031987363\n",
      "Step 83800, Avg Loss: 0.5018595327809453, F1 Score: 0.7813541614845032\n",
      "Step 83900, Avg Loss: 0.45591146025806667, F1 Score: 0.7814925373134328\n",
      "Step 84000, Avg Loss: 0.4528046928904951, F1 Score: 0.7815063385533184\n",
      "Step 84100, Avg Loss: 0.4837445179000497, F1 Score: 0.7807368842611133\n",
      "Step 84200, Avg Loss: 0.4671942699328065, F1 Score: 0.7810845900329572\n",
      "Step 84300, Avg Loss: 0.4431204930320382, F1 Score: 0.7813884050758895\n",
      "Step 84400, Avg Loss: 0.43740547585301104, F1 Score: 0.779488727858293\n",
      "Step 84500, Avg Loss: 0.4644652426615357, F1 Score: 0.7808246597277823\n",
      "Step 84600, Avg Loss: 0.45821885369718074, F1 Score: 0.7808506389776357\n",
      "Step 84700, Avg Loss: 0.48438899479806424, F1 Score: 0.7809333666084353\n",
      "Step 84800, Avg Loss: 0.4196206570416689, F1 Score: 0.7811116086978156\n",
      "Step 84900, Avg Loss: 0.5365290191769599, F1 Score: 0.7824450168416881\n",
      "Step 85000, Avg Loss: 0.46664870459586383, F1 Score: 0.7819653408808779\n",
      "Step 85100, Avg Loss: 0.4139232615754008, F1 Score: 0.7812313209802749\n",
      "Step 85200, Avg Loss: 0.49763303209096194, F1 Score: 0.7837518463810931\n",
      "Step 85300, Avg Loss: 0.5495077123120428, F1 Score: 0.7830557065888982\n",
      "Step 85400, Avg Loss: 0.4142837947234511, F1 Score: 0.7822552664188351\n",
      "Step 85500, Avg Loss: 0.43689048260450364, F1 Score: 0.7823061630218688\n",
      "Step 85600, Avg Loss: 0.48594942525029183, F1 Score: 0.7812515632034416\n",
      "Step 85700, Avg Loss: 0.4825834351032972, F1 Score: 0.7802645476034804\n",
      "Step 85800, Avg Loss: 0.4542734293267131, F1 Score: 0.7793047696038804\n",
      "Step 85900, Avg Loss: 0.43544831961393354, F1 Score: 0.780541847114513\n",
      "Step 86000, Avg Loss: 0.46677856300026177, F1 Score: 0.7798253142828293\n",
      "Step 86100, Avg Loss: 0.5229269098490477, F1 Score: 0.778503773489338\n",
      "Step 86200, Avg Loss: 0.45968679275363683, F1 Score: 0.7776084527075079\n",
      "Step 86300, Avg Loss: 0.5552807944267988, F1 Score: 0.7785207700101318\n",
      "Step 86400, Avg Loss: 0.47778483863919974, F1 Score: 0.7808700894202752\n",
      "Step 86500, Avg Loss: 0.5415499087423086, F1 Score: 0.7801970194493559\n",
      "Step 86600, Avg Loss: 0.3948304896429181, F1 Score: 0.7805271380335634\n",
      "Step 86700, Avg Loss: 0.5429202305153012, F1 Score: 0.7806204673650282\n",
      "Step 86800, Avg Loss: 0.42456466406583787, F1 Score: 0.7809005741915986\n",
      "Step 86900, Avg Loss: 0.46210431318730116, F1 Score: 0.7805983680870353\n",
      "Step 87000, Avg Loss: 0.5148887510597706, F1 Score: 0.7801031240521686\n",
      "Step 87100, Avg Loss: 0.4896497334167361, F1 Score: 0.7801942140400566\n",
      "Step 87200, Avg Loss: 0.46073787823319434, F1 Score: 0.7810734463276836\n",
      "Step 87300, Avg Loss: 0.4265599067136645, F1 Score: 0.7806849176684514\n",
      "Step 87400, Avg Loss: 0.4964403208345175, F1 Score: 0.7801468726259813\n",
      "Step 87500, Avg Loss: 0.4490885815024376, F1 Score: 0.7806627601535664\n",
      "Step 87600, Avg Loss: 0.4328929692879319, F1 Score: 0.7808599868627154\n",
      "Step 87700, Avg Loss: 0.4601082007214427, F1 Score: 0.7799169115411896\n",
      "Step 87800, Avg Loss: 0.5003912197425961, F1 Score: 0.7813948800645031\n",
      "Step 87900, Avg Loss: 0.40017563857138155, F1 Score: 0.7813887767460638\n",
      "Step 88000, Avg Loss: 0.46794822899624705, F1 Score: 0.7812957632681917\n",
      "Step 88100, Avg Loss: 0.46210604390129445, F1 Score: 0.7811729171910395\n",
      "Step 88200, Avg Loss: 0.4963692020252347, F1 Score: 0.7817478265239459\n",
      "Step 88300, Avg Loss: 0.45996897434815764, F1 Score: 0.7810985460420032\n",
      "Step 88400, Avg Loss: 0.5879375225678086, F1 Score: 0.7795939086294417\n",
      "Step 88500, Avg Loss: 0.4701264686137438, F1 Score: 0.7788339348345448\n",
      "Step 88600, Avg Loss: 0.4579533776640892, F1 Score: 0.7800800445817924\n",
      "Step 88700, Avg Loss: 0.5816994277760387, F1 Score: 0.7818785197103781\n",
      "Step 88800, Avg Loss: 0.4532465042732656, F1 Score: 0.78100050530571\n",
      "Step 88900, Avg Loss: 0.48110645622014997, F1 Score: 0.781491002570694\n",
      "Step 89000, Avg Loss: 0.5270195861905813, F1 Score: 0.7819571557879915\n",
      "Step 89100, Avg Loss: 0.5275332046300173, F1 Score: 0.782197160406807\n",
      "Step 89200, Avg Loss: 0.45971609368920324, F1 Score: 0.7799878222041811\n",
      "Step 89300, Avg Loss: 0.5554203345067799, F1 Score: 0.7783778275932342\n",
      "Step 89400, Avg Loss: 0.4768813157454133, F1 Score: 0.7781179832618902\n",
      "Step 89500, Avg Loss: 0.48190402749925854, F1 Score: 0.780532319391635\n",
      "Step 89600, Avg Loss: 0.5196970434486866, F1 Score: 0.7829138674420937\n",
      "Step 89700, Avg Loss: 0.46416370403021573, F1 Score: 0.7830523735681056\n",
      "Step 89800, Avg Loss: 0.5326348787173629, F1 Score: 0.7813163481953291\n",
      "Step 89900, Avg Loss: 0.5189562841504812, F1 Score: 0.7818411097099621\n",
      "Step 90000, Avg Loss: 0.5084868951141834, F1 Score: 0.7836921850079744\n",
      "Step 90100, Avg Loss: 0.45898910287767647, F1 Score: 0.7840501792114696\n",
      "Step 90200, Avg Loss: 0.45722965221852063, F1 Score: 0.7839426237673075\n",
      "Step 90300, Avg Loss: 0.4480517713725567, F1 Score: 0.7862320627249865\n",
      "Step 90400, Avg Loss: 0.4456635958701372, F1 Score: 0.7857847976307996\n",
      "Step 90500, Avg Loss: 0.46832210348919034, F1 Score: 0.7856401127429165\n",
      "Step 90600, Avg Loss: 0.48704690093174574, F1 Score: 0.7840654135713218\n",
      "Step 90700, Avg Loss: 0.48690927870571615, F1 Score: 0.784356761610216\n",
      "Step 90800, Avg Loss: 0.5014492125064135, F1 Score: 0.7837081459270365\n",
      "Step 90900, Avg Loss: 0.5894162942841649, F1 Score: 0.7835\n",
      "Step 91000, Avg Loss: 0.4633171246573329, F1 Score: 0.7847042421828321\n",
      "Step 91100, Avg Loss: 0.501502916701138, F1 Score: 0.7846575342465754\n",
      "Step 91200, Avg Loss: 0.5789606140367687, F1 Score: 0.7849173142060172\n",
      "Step 91300, Avg Loss: 0.5027678265422583, F1 Score: 0.7837269815852682\n",
      "Step 91400, Avg Loss: 0.49250905096530917, F1 Score: 0.782717910072846\n",
      "Step 91500, Avg Loss: 0.4985076916590333, F1 Score: 0.7816556391736956\n",
      "Step 91600, Avg Loss: 0.4980209966748953, F1 Score: 0.7825867889353555\n",
      "Step 91700, Avg Loss: 0.48908017050474883, F1 Score: 0.7816986855409505\n",
      "Step 91800, Avg Loss: 0.46719123305752874, F1 Score: 0.7818384288317474\n",
      "Step 91900, Avg Loss: 0.4953568052686751, F1 Score: 0.780522517027549\n",
      "Step 92000, Avg Loss: 0.4624814496748149, F1 Score: 0.7801028460872664\n",
      "Step 92100, Avg Loss: 0.4841095765028149, F1 Score: 0.7796143250688705\n",
      "Step 92200, Avg Loss: 0.4367726554349065, F1 Score: 0.7785915492957747\n",
      "Step 92300, Avg Loss: 0.47032007448375224, F1 Score: 0.7777378446372645\n",
      "Step 92400, Avg Loss: 0.5120097348839044, F1 Score: 0.7799724391364262\n",
      "Step 92500, Avg Loss: 0.3796750581637025, F1 Score: 0.7810753125317613\n",
      "Step 92600, Avg Loss: 0.50718281339854, F1 Score: 0.7812721923506137\n",
      "Step 92700, Avg Loss: 0.5022389462962746, F1 Score: 0.7818880129371336\n",
      "Step 92800, Avg Loss: 0.4460553971119225, F1 Score: 0.7843196155001502\n",
      "Step 92900, Avg Loss: 0.4528037881478667, F1 Score: 0.783908507223114\n",
      "Step 93000, Avg Loss: 0.4444779470562935, F1 Score: 0.7834875301689461\n",
      "Step 93100, Avg Loss: 0.5148666968382895, F1 Score: 0.7825735776667172\n",
      "Step 93200, Avg Loss: 0.4931897652894259, F1 Score: 0.7832864038616251\n",
      "Step 93300, Avg Loss: 0.4129142192006111, F1 Score: 0.7842999649105218\n",
      "Step 93400, Avg Loss: 0.46842144979164, F1 Score: 0.7841838526770034\n",
      "Step 93500, Avg Loss: 0.4435807317495346, F1 Score: 0.7845630193212534\n",
      "Step 93600, Avg Loss: 0.5254582441598177, F1 Score: 0.7838842560032151\n",
      "Step 93700, Avg Loss: 0.42600397057831285, F1 Score: 0.7849252761533463\n",
      "Step 93800, Avg Loss: 0.529057480301708, F1 Score: 0.7849542934212498\n",
      "Step 93900, Avg Loss: 0.36703890914097426, F1 Score: 0.784703824043989\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 58\u001B[0m\n\u001B[0;32m     56\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(probability, labels)\n\u001B[0;32m     57\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 58\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# 每100步报告一次，并计算F1分数\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    383\u001B[0m             )\n\u001B[1;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adamw.py:187\u001B[0m, in \u001B[0;36mAdamW.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    174\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    176\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    177\u001B[0m         group,\n\u001B[0;32m    178\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    184\u001B[0m         state_steps,\n\u001B[0;32m    185\u001B[0m     )\n\u001B[1;32m--> 187\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adamw.py:339\u001B[0m, in \u001B[0;36madamw\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    336\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    337\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[1;32m--> 339\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adamw.py:415\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001B[0m\n\u001B[0;32m    412\u001B[0m step_t \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;66;03m# Perform stepweight decay\u001B[39;00m\n\u001B[1;32m--> 415\u001B[0m \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m    418\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mlerp_(grad, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word_ids, label = self.data[idx]\n",
    "        return torch.tensor(word_ids, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# TODO: Set your training stuff, hyperparameters, models, etc. here\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(word_to_index)  # 假设word_to_index是你的词汇到索引的映射\n",
    "embedding_size = 50  # 或你在word2vec中使用的任何尺寸\n",
    "num_heads = 4  # 注意力头的数量\n",
    "batch_size = 1\n",
    "learning_rate = 5e-5\n",
    "epochs = 8  # 或更多，根据需要\n",
    "max_steps = 1000000000  # 设置最大步数\n",
    "patience = 100000  # 设置耐心值，即在这么多步之后如果没有提升则停止训练\n",
    "best_f1 = 0.0  # 记录迄今为止最好的F1分数\n",
    "steps_since_improvement = 0  # 记录自上次性能提升以来的步数\n",
    "# 初始化收集损失和F1分数的列表\n",
    "losses = []\n",
    "f1_scores = []\n",
    "\n",
    "embeddings_file_path = './models/1st_model_embeddings_state_dict.pt'\n",
    "model = DocumentAttentionClassifier(vocab_size=vocab_size, embedding_size=embedding_size, num_heads=num_heads, embeddings_fname=embeddings_file_path)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# TODO: initialize weights and biases (wandb) here\n",
    "wandb.init(project=\"attention_classifier\", entity=\"yanzhuo\")\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "# 创建DataLoader实例\n",
    "train_dataset = TextDataset(train_list)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dev_dataset = TextDataset(dev_list)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 以下是修正后的训练循环\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for step, (word_ids, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        word_ids, labels = word_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        probability, _ = model(word_ids)\n",
    "        probability = probability.squeeze(-1)  # 尝试只压缩最后一个维度\n",
    "        loss = loss_function(probability, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 每100步报告一次，并计算F1分数\n",
    "        if (step + 1) % 100 == 0:\n",
    "            avg_loss = running_loss / 100\n",
    "            dev_f1 = run_eval(model, dev_loader)\n",
    "            print(f\"Step {step + 1}, Avg Loss: {avg_loss}, F1 Score: {dev_f1}\")\n",
    "            wandb.log({\"loss\": avg_loss, \"f1_score\": dev_f1})\n",
    "            losses.append(avg_loss)\n",
    "            f1_scores.append(dev_f1)\n",
    "            running_loss = 0.0\n",
    "\n",
    "            if dev_f1 > best_f1:\n",
    "                best_f1 = dev_f1\n",
    "                steps_since_improvement = 0\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            else:\n",
    "                steps_since_improvement += 100\n",
    "                if steps_since_improvement >= patience:\n",
    "                    print(\"Early stopping triggered due to no improvement.\")\n",
    "                    break\n",
    "\n",
    "        if step >= max_steps or steps_since_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 绘制损失\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制F1分数\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(f1_scores, label='F1 Score', color='orange')\n",
    "plt.title('F1 Score')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 训练结束后加载最佳模型\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for word_ids in test_data:\n",
    "        # 由于我们的模型是基于批处理的，我们需要添加一个批次维度\n",
    "        output, _ = model(word_ids.unsqueeze(0))  # 假设模型返回预测和注意力权重\n",
    "        prediction = torch.round(torch.sigmoid(output)).item()  # 将输出转换为类别标签\n",
    "        predictions.append(prediction)\n",
    "\n",
    "# 将预测结果添加到DataFrame中\n",
    "sent_test_df['predicted_label'] = predictions\n",
    "\n",
    "test_output=pd.DataFrame({'inst_id':sent_test_df['inst_id'],'predicted_label':sent_test_df['predicted_label']})\n",
    "# 保存到CSV文件中\n",
    "test_output.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "# 打印前几个预测结果作为验证\n",
    "print(sent_test_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.587020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 初始化收集损失和F1分数的列表\n",
    "losses = []\n",
    "f1_scores = []\n",
    "\n",
    "model1 = DocumentAttentionClassifier(vocab_size, embedding_size, num_heads, embeddings_file_path, False)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "# TODO: initialize weights and biases (wandb) here\n",
    "wandb.init(project=\"attention_classifier2\", entity=\"yanzhuo\")\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    model1.train()\n",
    "    running_loss = 0.0\n",
    "    for step, (word_ids, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        # 早停条件之一：达到最大步数\n",
    "        if step >= max_steps:\n",
    "            print(f\"Stopping early at step {step} due to reaching max steps.\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        probability, attention_weights = model1(word_ids)\n",
    "        outputs = probability.view(-1)  # 将outputs重塑为一维张量\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:  \n",
    "            dev_f1 = run_eval(model1, dev_loader)\n",
    "            wandb.log({\"loss\": running_loss / 500, \"f1_score\": dev_f1})\n",
    "            losses.append(running_loss / 500)  # 收集平均损失\n",
    "            f1_scores.append(dev_f1)  # 收集F1分数\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # 早停条件之二：性能没有提升\n",
    "            if dev_f1 > best_f1:\n",
    "                best_f1 = dev_f1\n",
    "                steps_since_improvement = 0\n",
    "                # 保存当前最好模型\n",
    "                torch.save(model1.state_dict(), \"best_model1.pth\")\n",
    "            else:\n",
    "                steps_since_improvement += 500\n",
    "                if steps_since_improvement >= patience:\n",
    "                    print(f\"Stopping early at step {step} due to no improvement in {patience} steps.\")\n",
    "                    break\n",
    "\n",
    "        # 早停条件之一：在for循环内部提前终止循环\n",
    "        if step >= max_steps or steps_since_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 绘制损失\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制F1分数\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(f1_scores, label='F1 Score', color='orange')\n",
    "plt.title('F1 Score')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 训练结束后加载最佳模型\n",
    "# model1.load_state_dict(torch.load(\"best_model1.pth\"))\n",
    "model1.eval()  # 切换到评估模式"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.601739Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting what the model learned\n",
    "\n",
    "In this last bit of the homework you should look at the model's attention weights. We've written a visualization helper function below that will plot the attention weights. You'll need to fill in the `get_label_and_weights` method that uses the model to classify some new text and structures the attention output in a way that's specified. \n",
    "\n",
    "**NOTE:** most of the code for `get_label_and_weights` is code you've already written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.602674Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_label_and_weights(text):\n",
    "    '''\n",
    "    Classifies the text (requires tokenizing, etc.) and returns (1) the classification label, \n",
    "    (2) the tokenized words in the model's vocabulary, \n",
    "    and (3) the attention weights over the in-vocab tokens as a numpy array. Note that the\n",
    "    attention weights will be a matrix, depending on how many heads were used in training.\n",
    "    '''\n",
    "    # 分词并转换为ID\n",
    "    word_ids = tokenize_and_convert_to_ids(text, tokenizer, word_to_index)\n",
    "    # 转换为PyTorch tensor并添加一个批次维度\n",
    "    word_ids_tensor = torch.tensor([word_ids], dtype=torch.long)\n",
    "    \n",
    "    # 确保模型处于评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 获取模型的输出，假设模型修改为同时返回分类概率和注意力权重\n",
    "        probs, attention_weights = model(word_ids_tensor)\n",
    "        # 由于我们只处理一个示例，我们移除批次维度\n",
    "        probs = probs.squeeze(0)\n",
    "        attention_weights = attention_weights.squeeze(0)\n",
    "        \n",
    "        # 获取最终的分类标签\n",
    "        label = torch.round(torch.sigmoid(probs)).item()\n",
    "        \n",
    "        # 将单词ID转换回单词\n",
    "        tokens = [index_to_word.get(id, '<UNK>') for id in word_ids]\n",
    "        \n",
    "        # 将注意力权重转换为numpy数组\n",
    "        attention_weights_np = attention_weights.cpu().numpy()\n",
    "        \n",
    "    return label, tokens, attention_weights_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.603752Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_attention(words, attention_weights):\n",
    "    '''\n",
    "    Makes a heatmap figure that visualizes the attention weights for an item.\n",
    "    Attention weights should be a numpy array that has the shape (num_words, num_heads)\n",
    "    '''\n",
    "    fig, ax = plt.subplots() \n",
    "    # Rescale image size based on the input length\n",
    "    fig.set_size_inches((len(words), 4))    \n",
    "    im = ax.imshow(attention_weights.T)\n",
    "\n",
    "    head_labels = [ 'head-%d' % h for h in range(attention_weights.shape[1])]\n",
    "    ax.set_xticks(np.arange(len(words))) # , labels=words)\n",
    "    ax.set_yticks(np.arange(len(head_labels))) #, labels=head_labels)\n",
    "\n",
    "    # Rotate the word labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add the words and axis labels\n",
    "    ax.set_yticklabels(labels=range(attention_weights.shape[1]), fontsize=16)\n",
    "    ax.set_ylabel('Attention Head', fontsize=16)\n",
    "    ax.set_xticklabels(labels=words, fontsize=16)\n",
    "\n",
    "    # Add a color bar to show probability scaling\n",
    "    cb = fig.colorbar(im, ax=ax, label='Probability', pad = 0.01)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    cb.set_label(label='Probability',size=16)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example messages to try visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.604796Z"
    }
   },
   "outputs": [],
   "source": [
    "s = 'Just as I remembered it, one of my favorites from childhood! Great condition, very happy to have this to share with my daughter. Packaging was so nice and was received quickly.'\n",
    "pred, tokens, attn = get_label_and_weights(s)\n",
    "visualize_attention(tokens, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.604796Z"
    }
   },
   "outputs": [],
   "source": [
    "s = '''\n",
    "I'm a big fan of his, and I have to say that this was a BIG letdown. It features: Stilted dialogue, no character development, no suspense, no description of Indian tradition and poor editing.\\n\\nAvoid at all costs.\n",
    "'''\n",
    "pred, tokens, attn = get_label_and_weights(s)\n",
    "visualize_attention(tokens, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 随机选择两个正样本和两个负样本\n",
    "positive_samples = sent_dev_df[sent_dev_df['label'] == 1].sample(2)\n",
    "negative_samples = sent_dev_df[sent_dev_df['label'] == 0].sample(2)\n",
    "\n",
    "# 合并选定的样本\n",
    "selected_samples = pd.concat([positive_samples, negative_samples])\n",
    "\n",
    "# 获取样本文本\n",
    "texts = selected_samples['text'].tolist()\n",
    "\n",
    "for text in texts:\n",
    "    label, tokens, attn = get_label_and_weights(text)\n",
    "    print(f\"Text: {text[:50]}...\")  # 打印部分文本以供参考\n",
    "    print(f\"Predicted Label: {label}\")\n",
    "    visualize_attention(tokens, attn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.605815Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 示例：手动编写一个模棱两可的样本\n",
    "sample_text = \"Although this product arrived on time and was nicely packaged, I found its quality lacking and performance subpar compared to other brands.\"\n",
    "label, tokens, attn = get_label_and_weights(sample_text)\n",
    "print(f\"Predicted Label: {label}\")\n",
    "visualize_attention(tokens, attn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.605815Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional TODOs:\n",
    "\n",
    "### How many instances do we need to learn?\n",
    "\n",
    "Since the word2vec vectors capture word meaning, do we need to see a lot of examples to train an effective classifier? Maybe we can get away with fewer (or not?). Try making a plot that shows the performance of training on 1 epoch with varying numbers of training examples. What if we just had 10 examples? 100? 1000? \n",
    "\n",
    "### Make the \"important word\" vectors learn different attentions\n",
    "\n",
    "If your attention vectors are learning to look at similar words, we could try to add some structure to how we learn so these vectors become dissimilar. One idea is to penalize the model according to how similar each head's vector is to the others. You could use cosine similarity or MSE or any weighting function. For example, you could add the cosine similarities of each pair to the `loss` so that the model benefits from learning orthoginal or dissimlar vectors. Would this help? \n",
    "\n",
    "### Add more layers to the network\n",
    "\n",
    "This is the second easiest one, but can be fun. What if you add more layers after you aggregate? Does letting the different attention heads' representations interact give better performance? Find out!\n",
    "\n",
    "### Change the learning rate dynamically\n",
    "\n",
    "We have a fixed learning rate, but what if we wanted to decrease the learning rate as the model starts to converge? In many cases, this can help the model take smaller but more precise steps towards the best possible parameters. PyTorch supports this with _learning rate schedulers_ that tell pytorch how and when to change the learning rate. See if you can get a better performance using a scheduler!\n",
    "\n",
    "### Add support for batch sizes > 1\n",
    "\n",
    "This is non-trivial but will increase training speed _a lot_. The main issue with increasing batch sizes is that our input sequences (the word ids) in a batch will have different lengths. Under the hood, pytorch is turning your code into a series of very fast matrix operations. However, if those matrices suddenly have difference sizes, the math no longer works. As a result developers (like us) have to do a few things:\n",
    "\n",
    "* We need to _pad_ the sequences with empty values so that all sequences have the same length. You could do this by  adding an extra word ID that is the \"empty token\" and make sure its values are set to 0 (so it won't interact with anything)\n",
    "\n",
    "* Set up a collate function in our DataLoader that automatically pads each batch's data based on the longest length in the batch\n",
    "\n",
    "* At inference time, it's also efficient to mask part of the sequence that's the padded part so we ignore the computations for that part in anything downstream. Depending on how you set it up, you may be able to avoid this step.\n",
    "\n",
    "If you want to dig into this, you might see some of the documentation around packed and padded sequences in pytorch. You won't need to use these functions but they can provide more context for what's happening and why.\n",
    "\n",
    "\n",
    "### Add positional information to the word embeddings\n",
    "\n",
    "Right now our model doesn't know much about which order the words are in. What if we helped the model in this? One way that people have done this is to _add_ some positional embedding to the word embedding, where the positional embedding represents which position in the sequence is in. There are many complicated schemes for this, but one potential idea is to _learn the positional embeddings_. You would keep a separate `Embedding` object for positions with the number of positions up to the length of your longest sequence in the data. Then for the word in the first position, you add `position_embeddings(0)` to it. You can definitely speed that up by passing in a sequence of the positions in the current input and, conveniently, pytorch will let you easily add all the position embeddings to the word embeddings easily (no for loop required).\n",
    "\n",
    "Will it help here? I have no idea but I'm curious.\n",
    "\n",
    "\n",
    "**No extra credit is given for these; they're just for folks who want to explore more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T04:40:04.606834Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
